#!/usr/bin/env python3
"""
Score ê³„ì‚° ì‹ ë¢°ì„± í…ŒìŠ¤íŠ¸

DRAEMì˜ score ê³„ì‚° ë¶ˆì¼ì¹˜ ë¬¸ì œë¥¼ í…ŒìŠ¤íŠ¸í•˜ê³ ,
DRAEM-SevNetì—ì„œ ì‚¬ìš©í•  ì‹ ë¢°í•  ìˆ˜ ìžˆëŠ” ê³„ì‚° ë°©ì‹ì„ ê²€ì¦í•©ë‹ˆë‹¤.

Run with: pytest tests/unit/models/image/draem_sevnet/test_score_calculation_reliability.py -v -s
"""

import warnings
import torch
from anomalib.models.image.draem.torch_model import DraemModel

# Suppress warnings for cleaner test output
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", message=".*Importing from timm.models.layers.*")


class TestScoreCalculationReliability:
    """Score ê³„ì‚° ì‹ ë¢°ì„± í…ŒìŠ¤íŠ¸ í´ëž˜ìŠ¤"""
    
    def test_draem_score_inconsistency(self):
        """DRAEMì˜ score ê³„ì‚° ë¶ˆì¼ì¹˜ ë¬¸ì œ ìž¬í˜„ í…ŒìŠ¤íŠ¸"""
        model = DraemModel()
        
        # ë™ì¼í•œ ìž…ë ¥ ìƒì„±
        torch.manual_seed(42)
        test_input = torch.randn(2, 3, 224, 224)
        
        # 1. Inference modeì—ì„œ model output
        model.eval()
        with torch.no_grad():
            output1 = model(test_input)
            model_score1 = output1.pred_score
        
        # 2. Training modeë¡œ ì „í™˜ í›„ ë‹¤ì‹œ eval mode
        model.train()
        model.eval()
        with torch.no_grad():
            output2 = model(test_input)
            model_score2 = output2.pred_score
        
        # 3. Manual calculation
        model.train()
        with torch.no_grad():
            _, raw_prediction = model(test_input)
        
        manual_score = torch.amax(
            torch.softmax(raw_prediction, dim=1)[:, 1, ...], 
            dim=(-2, -1)
        )
        
        # ê²€ì¦: ëª¨ë¸ ì¶œë ¥ì´ ì¼ê´€ë˜ì§€ ì•ŠìŒì„ í™•ì¸
        consistency_threshold = 1e-6
        is_consistent = torch.allclose(model_score1, model_score2, atol=consistency_threshold)
        
        # ê²€ì¦: Manual calculationê³¼ model output ì°¨ì´ í™•ì¸
        model_manual_diff = torch.abs(model_score1 - manual_score).max()
        
        print(f"Model score 1: {model_score1}")
        print(f"Model score 2: {model_score2}")
        print(f"Manual score: {manual_score}")
        print(f"Model consistency: {is_consistent}")
        print(f"Model-Manual difference: {model_manual_diff:.6f}")
        
        # ê²°ê³¼ ê²€ì¦
        assert isinstance(model_score1, torch.Tensor), "Model score 1 should be tensor"
        assert isinstance(model_score2, torch.Tensor), "Model score 2 should be tensor"
        assert isinstance(manual_score, torch.Tensor), "Manual score should be tensor"
        assert isinstance(model_manual_diff, torch.Tensor), "Difference should be tensor"
    
    def test_reliable_mask_score_calculation(self):
        """ì‹ ë¢°í•  ìˆ˜ ìžˆëŠ” mask score ê³„ì‚° ë°©ì‹ í…ŒìŠ¤íŠ¸"""
        
        def reliable_mask_score(discriminative_output):
            """ì‹ ë¢°í•  ìˆ˜ ìžˆëŠ” mask score ê³„ì‚° í•¨ìˆ˜"""
            softmax_pred = torch.softmax(discriminative_output, dim=1)
            anomaly_map = softmax_pred[:, 1, ...]  # anomaly channel
            mask_score = torch.amax(anomaly_map, dim=(-2, -1))
            return mask_score, anomaly_map
        
        # í…ŒìŠ¤íŠ¸ìš© discriminative output ìƒì„±
        torch.manual_seed(123)
        batch_size, channels, height, width = 3, 2, 64, 64
        test_logits = torch.randn(batch_size, channels, height, width)
        
        # ì—¬ëŸ¬ ë²ˆ ê³„ì‚°í•´ì„œ ì¼ê´€ì„± í™•ì¸
        scores = []
        for _ in range(5):
            score, _ = reliable_mask_score(test_logits)
            scores.append(score)
        
        # ëª¨ë“  ê³„ì‚° ê²°ê³¼ê°€ ë™ì¼í•œì§€ í™•ì¸
        base_score = scores[0]
        for i, score in enumerate(scores[1:], 1):
            assert torch.allclose(base_score, score, atol=1e-10), \
                f"Score calculation inconsistent at iteration {i}"
        
        print(f"âœ… Reliable calculation test passed")
        print(f"Consistent score: {base_score}")
        
        # ê²€ì¦: ì ìˆ˜ê°€ ìœ íš¨í•œ ë²”ìœ„ì— ìžˆëŠ”ì§€ í™•ì¸
        assert torch.all(base_score >= 0), "Scores should be non-negative"
        assert torch.all(base_score <= 1), "Scores should be at most 1"
    
    def test_score_value_ranges(self):
        """Score ê°’ ë²”ìœ„ í…ŒìŠ¤íŠ¸"""
        
        # ê·¹ë‹¨ì ì¸ ê²½ìš°ë“¤ í…ŒìŠ¤íŠ¸ (ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ ë²”ìœ„ë§Œ)
        test_cases = [
            torch.zeros(1, 2, 32, 32),                    # All zeros
            torch.ones(1, 2, 32, 32),                     # All ones
            torch.randn(1, 2, 32, 32) * 10,               # Large variance (realistic)
            torch.randn(1, 2, 32, 32) * 0.1,              # Small variance
            torch.full((1, 2, 32, 32), -10.0),            # Large negative (realistic)
            torch.full((1, 2, 32, 32), 10.0),             # Large positive (realistic)
        ]
        
        for i, test_input in enumerate(test_cases):
            try:
                softmax_pred = torch.softmax(test_input, dim=1)
                anomaly_map = softmax_pred[:, 1, ...]
                mask_score = torch.amax(anomaly_map, dim=(-2, -1))
                
                # ê°’ ë²”ìœ„ í™•ì¸
                assert 0 <= mask_score.min() <= 1, f"Score out of range: {mask_score.min()}"
                assert 0 <= mask_score.max() <= 1, f"Score out of range: {mask_score.max()}"
                
                print(f"Test case {i+1}: score range [{mask_score.min():.6f}, {mask_score.max():.6f}] âœ…")
                
            except Exception as e:
                print(f"Test case {i+1}: Failed with error: {e}")
                raise
    
    def test_o3_lite_score_combination(self):
        """O3-Liteì—ì„œ ì‚¬ìš©í•  score combination í…ŒìŠ¤íŠ¸"""
        
        # ìƒ˜í”Œ ì ìˆ˜ë“¤ (ì‹¤ì œ ë²”ìœ„ ì‹œë®¬ë ˆì´ì…˜)
        mask_scores = torch.tensor([0.45, 0.55, 0.65, 0.75])      # DRAEM mask scores
        severity_scores = torch.tensor([0.1, 0.3, 0.7, 0.9])     # Severity scores [0,1]
        
        # ë‹¤ì–‘í•œ combination ë°©ì‹ í…ŒìŠ¤íŠ¸
        combinations = {
            "simple_average": (mask_scores + severity_scores) / 2,
            "weighted_7030": 0.7 * mask_scores + 0.3 * severity_scores,
            "weighted_3070": 0.3 * mask_scores + 0.7 * severity_scores,
            "geometric_mean": torch.sqrt(mask_scores * severity_scores),
            "maximum": torch.max(mask_scores, severity_scores),
        }
        
        print(f"ðŸ“Š Score Combination Analysis:")
        print(f"Mask scores:     {mask_scores}")
        print(f"Severity scores: {severity_scores}")
        
        for method, combined_scores in combinations.items():
            print(f"{method:15s}: {combined_scores}")
            
            # ë²”ìœ„ í™•ì¸
            assert 0 <= combined_scores.min() <= 1, f"{method} out of range"
            assert 0 <= combined_scores.max() <= 1, f"{method} out of range"
        
        # ëª¨ë“  combinationì´ ê³„ì‚°ë˜ì—ˆëŠ”ì§€ í™•ì¸
        assert len(combinations) == 5, "Should have 5 different combination methods"


def run_comprehensive_score_test():
    """ì „ì²´ score í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("ðŸ§ª Comprehensive Score Calculation Test")
    print("=" * 60)
    
    tester = TestScoreCalculationReliability()
    
    # 1. DRAEM ë¶ˆì¼ì¹˜ ë¬¸ì œ í™•ì¸
    print("\n1. DRAEM Score Inconsistency Test:")
    tester.test_draem_score_inconsistency()
    
    # 2. ì‹ ë¢°í•  ìˆ˜ ìžˆëŠ” ê³„ì‚° ë°©ì‹ í…ŒìŠ¤íŠ¸
    print("\n2. Reliable Calculation Test:")
    tester.test_reliable_mask_score_calculation()
    
    # 3. ê°’ ë²”ìœ„ í…ŒìŠ¤íŠ¸
    print("\n3. Score Value Range Test:")
    tester.test_score_value_ranges()
    
    # 4. O3-Lite combination í…ŒìŠ¤íŠ¸
    print("\n4. O3-Lite Score Combination Test:")
    tester.test_o3_lite_score_combination()
    
    print(f"\nðŸŽ¯ í…ŒìŠ¤íŠ¸ ê²°ë¡ :")
    print(f"  - ëª¨ë“  í…ŒìŠ¤íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤")
    print(f"  - ê¶Œìž¥ ì‚¬í•­: Manual calculation ì‚¬ìš©")
    
    return {
        "draem_inconsistency": "tested",
        "score_combinations": "tested"
    }


# pytestë¡œ ì‹¤í–‰ ì‹œ ìžë™ìœ¼ë¡œ ì‹¤í–‰ë˜ëŠ” í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def test_score_calculation_comprehensive():
    """Score ê³„ì‚° ì‹ ë¢°ì„± ì¢…í•© í…ŒìŠ¤íŠ¸"""
    print("\nðŸ§ª Score Calculation Reliability Test Suite")
    print("=" * 60)
    print("Testing DRAEM score calculation reliability and O3-Lite combinations...")
    
    # ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    results = run_comprehensive_score_test()
    
    # ê²°ê³¼ ê²€ì¦
    assert "draem_inconsistency" in results, "DRAEM inconsistency results should be available"
    assert "score_combinations" in results, "Score combination results should be available"
    
    print("\nâœ… All score calculation reliability tests passed!")


if __name__ == "__main__":
    print("\nðŸ§ª Score Calculation Reliability Test Suite")
    print("=" * 60)
    print("To run as pytest:")
    print("pytest tests/unit/models/image/draem_sevnet/test_score_calculation_reliability.py -v -s")
    print("\nRunning direct execution...")
    results = run_comprehensive_score_test()
