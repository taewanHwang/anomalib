"""Test suite for DRAEM-SevNet Lightning Model Update.

DRAEM-SevNetìœ¼ë¡œ ì—…ë°ì´íŠ¸ëœ Lightning Modelì˜ ëª¨ë“  ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.

Run with: pytest tests/unit/models/image/draem_sevnet/test_draem_sevnet_lightning_model.py -v -s
Author: Taewan Hwang
"""

import torch
from unittest.mock import MagicMock
from anomalib.models.image.draem_sevnet import DraemSevNet
from anomalib.models.image.draem_sevnet.loss import DraemSevNetLoss

# ìƒì„¸ ì¶œë ¥ì„ ìœ„í•œ helper function
def verbose_print(message: str, level: str = "INFO"):
    """ìƒì„¸ ì¶œë ¥ì„ ìœ„í•œ í•¨ìˆ˜"""
    symbols = {"INFO": "â„¹ï¸", "SUCCESS": "âœ…", "WARNING": "âš ï¸", "ERROR": "âŒ"}
    print(f"\n{symbols.get(level, 'â„¹ï¸')} {message}")


class TestDraemSevNetLightningModel:
    """DRAEM-SevNet Lightning Model í…ŒìŠ¤íŠ¸"""
    
    def test_model_initialization(self):
        """ëª¨ë¸ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸"""
        verbose_print("Testing DRAEM-SevNet Lightning model initialization...")
        
        # Default initialization
        model = DraemSevNet()
        verbose_print(f"Default - severity_head_mode: {model.severity_head_mode}")
        verbose_print(f"Default - score_combination: {model.score_combination}")
        verbose_print(f"Default - has model: {hasattr(model, 'model')}")
        verbose_print(f"Default - has loss: {hasattr(model, 'loss')}")
        verbose_print(f"Default - has augmenter: {hasattr(model, 'augmenter')}")
        
        assert model.severity_head_mode == "single_scale"
        assert model.score_combination == "simple_average"
        assert hasattr(model, 'model')
        assert hasattr(model, 'loss')
        assert hasattr(model, 'augmenter')
        
        # Custom initialization 
        custom_model = DraemSevNet(
            severity_head_mode="multi_scale",
            score_combination="weighted_average",
            severity_weight_for_combination=0.3,
            severity_weight=0.8,
            severity_loss_type="smooth_l1"
        )
        verbose_print(f"Custom - severity_head_mode: {custom_model.severity_head_mode}")
        verbose_print(f"Custom - score_combination: {custom_model.score_combination}")
        
        assert custom_model.severity_head_mode == "multi_scale"
        assert custom_model.score_combination == "weighted_average"
        
        verbose_print("Lightning model initialization test passed!", "SUCCESS")
        
    def test_training_step(self):
        """Training step í…ŒìŠ¤íŠ¸"""
        model = DraemSevNet()
        model.train()
        
        # Create mock batch
        batch_size = 4
        batch = MagicMock()
        batch.image = torch.randn(batch_size, 3, 224, 224)
        batch.gt_label = torch.randint(0, 2, (batch_size,))
        
        # Training step
        output = model.training_step(batch)
        
        # Check output structure
        assert isinstance(output, dict)
        assert "loss" in output
        assert isinstance(output["loss"], torch.Tensor)
        assert output["loss"].dim() == 0  # Scalar loss
        
    def test_validation_step(self):
        """Validation step í…ŒìŠ¤íŠ¸"""
        model = DraemSevNet()
        model.eval()
        
        batch_size = 4
        batch = MagicMock()
        batch.image = torch.randn(batch_size, 3, 224, 224)
        batch.gt_label = torch.randint(0, 2, (batch_size,))
        
        # Mock batch.update method to return the batch itself
        def mock_update(**kwargs):
            for key, value in kwargs.items():
                setattr(batch, key, value)
            return batch
        batch.update = mock_update
        
        # Validation step
        with torch.no_grad():
            output = model.validation_step(batch)
        
        # Check that output is the updated batch (Lightning pattern)
        assert output == batch
        assert hasattr(output, 'pred_score')
        assert hasattr(output, 'anomaly_map')
        assert hasattr(output, 'pred_label')
        
        # Check shapes
        assert output.pred_score.shape == (batch_size,)
        assert output.anomaly_map.shape == (batch_size, 224, 224)
        assert output.pred_label.shape == (batch_size,)
        
    def test_on_validation_epoch_end(self):
        """Validation epoch end í…ŒìŠ¤íŠ¸"""
        model = DraemSevNet()
        
        # DraemSevNet uses anomalib's Evaluator which handles validation metrics automatically
        # The on_validation_epoch_end is handled by parent AnomalibModule
        # This test just ensures it doesn't crash
        
        # Mock trainer to avoid errors
        trainer_mock = MagicMock()
        model.trainer = trainer_mock
        
        # Test that on_validation_epoch_end can be called without error
        try:
            model.on_validation_epoch_end()
            success = True
        except Exception:
            success = False
        
        assert success, "on_validation_epoch_end should not crash"
        
    def test_single_class_validation(self):
        """ë‹¨ì¼ í´ë˜ìŠ¤ validation í…ŒìŠ¤íŠ¸"""
        model = DraemSevNet()
        
        # Test validation step with single class scenario
        batch_size = 5
        batch = MagicMock()
        batch.image = torch.randn(batch_size, 3, 224, 224)
        batch.gt_label = torch.ones(batch_size, dtype=torch.long)  # All same class
        
        # Mock batch.update method
        def mock_update(**kwargs):
            for key, value in kwargs.items():
                setattr(batch, key, value)
            return batch
        batch.update = mock_update
        
        # Test that validation step works even with single class
        model.eval()
        with torch.no_grad():
            output = model.validation_step(batch)
        
        assert output == batch
        assert hasattr(output, 'pred_score')
        assert output.pred_score.shape == (batch_size,)
            
    def test_different_score_combinations(self):
        """ë‹¤ì–‘í•œ score combination ë°©ì‹ í…ŒìŠ¤íŠ¸"""
        combinations = ["simple_average", "weighted_average", "maximum"]
        
        for combination in combinations:
            model = DraemSevNet(
                score_combination=combination,
                severity_weight_for_combination=0.3
            )
            model.eval()
            
            batch = MagicMock()
            batch.image = torch.randn(2, 3, 224, 224)
            batch.gt_label = torch.randint(0, 2, (2,))
            
            # Mock batch.update method
            def mock_update(**kwargs):
                for key, value in kwargs.items():
                    setattr(batch, key, value)
                return batch
            batch.update = mock_update
            
            # Should work without errors
            with torch.no_grad():
                output = model.validation_step(batch)
                assert hasattr(output, 'pred_score')
                
    def test_severity_head_modes(self):
        """ë‹¤ì–‘í•œ severity head ëª¨ë“œ í…ŒìŠ¤íŠ¸"""
        modes = ["single_scale", "multi_scale"]
        
        for mode in modes:
            model = DraemSevNet(severity_head_mode=mode)
            
            batch = MagicMock()
            batch.image = torch.randn(2, 3, 224, 224)
            batch.gt_label = torch.randint(0, 2, (2,))
            
            # Mock batch.update method for validation
            def mock_update(**kwargs):
                for key, value in kwargs.items():
                    setattr(batch, key, value)
                return batch
            batch.update = mock_update
            
            # Training mode
            model.train()
            train_output = model.training_step(batch)
            assert "loss" in train_output
            
            # Validation mode
            model.eval()
            with torch.no_grad():
                val_output = model.validation_step(batch)
                assert hasattr(val_output, 'pred_score')
                
    def test_loss_function_integration(self):
        """Loss function í†µí•© í…ŒìŠ¤íŠ¸"""
        model = DraemSevNet(
            severity_weight=0.8,
            severity_loss_type="smooth_l1"
        )
        
        # Check loss type
        
        assert isinstance(model.loss, DraemSevNetLoss)
        assert model.loss.severity_weight == 0.8
        assert model.loss.severity_loss_type == "smooth_l1"
        
    def test_optimizer_configuration(self):
        """Optimizer ì„¤ì • í…ŒìŠ¤íŠ¸"""
        optimizers = ["adam", "adamw", "sgd"]
        
        for opt_name in optimizers:
            model = DraemSevNet(
                optimizer=opt_name,
                learning_rate=2e-4
            )
            
            optimizer = model.configure_optimizers()
            assert hasattr(optimizer, 'param_groups')
            assert optimizer.param_groups[0]['lr'] == 2e-4
            
    def test_synthetic_generator_integration(self):
        """Synthetic generator í†µí•© í…ŒìŠ¤íŠ¸"""
        model = DraemSevNet(
            patch_ratio_range=(1.5, 3.0),
            patch_count=2,
            anomaly_probability=0.8
        )
        
        # Check augmenter configuration
        assert model.augmenter.severity_max == 1.0  # Fixed for DRAEM-SevNet
        assert model.augmenter.patch_count == 2
        assert model.augmenter.probability == 0.8
        
    def test_backward_compatibility(self):
        """ê¸°ì¡´ ì½”ë“œì™€ì˜ í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸"""
        model = DraemSevNet()
        
        # Test that DraemSevNet is compatible with anomalib's evaluation system
        # The evaluator is configured to log val_image_AUROC for compatibility
        assert hasattr(model, 'evaluator')
        assert model.evaluator is not None
        
        # Test validation step output format compatibility
        batch = MagicMock()
        batch.image = torch.randn(4, 3, 224, 224)
        batch.gt_label = torch.randint(0, 2, (4,))
        
        # Mock batch.update method
        def mock_update(**kwargs):
            for key, value in kwargs.items():
                setattr(batch, key, value)
            return batch
        batch.update = mock_update
        
        model.eval()
        with torch.no_grad():
            output = model.validation_step(batch)
        
        # Should have pred_score field expected by anomalib evaluator
        assert hasattr(output, 'pred_score')
        assert output.pred_score.shape == (4,)


# pytestë¡œ ì‹¤í–‰ ì‹œ ìë™ìœ¼ë¡œ ì‹¤í–‰ë˜ëŠ” í†µí•© í…ŒìŠ¤íŠ¸
def test_lightning_model_integration_summary():
    """ì „ì²´ DRAEM-SevNet Lightning Model í…ŒìŠ¤íŠ¸ ìš”ì•½"""
    verbose_print("ğŸ§ª DRAEM-SevNet Lightning Model Test Suite Integration Summary", "INFO")
    verbose_print("=" * 70)
    
    # í…ŒìŠ¤íŠ¸ êµ¬ì„± ìš”ì†Œ í™•ì¸
    test_components = [
        "Lightning model initialization (default & custom parameters)",
        "Training step functionality (loss calculation, logging)",
        "Validation step functionality (score collection)",
        "Validation epoch end (multi-AUROC calculation)",
        "Single class validation edge case handling",
        "Different score combination methods",
        "Severity head mode configurations",
        "Loss function integration (DraemSevNetLoss)",
        "Optimizer configuration",
        "Synthetic generator integration",
        "Backward compatibility validation"
    ]
    
    verbose_print("Test components covered:")
    for i, component in enumerate(test_components, 1):
        verbose_print(f"  {i:2d}. {component}")
    
    verbose_print(f"\nğŸ¯ Total {len(test_components)} test categories covered!", "SUCCESS")
    verbose_print("\nRun individual tests with: pytest tests/unit/models/image/draem_sevnet/test_draem_sevnet_lightning_model.py::TestDraemSevNetLightningModel::test_<method_name> -v -s")


if __name__ == "__main__":
    # ì§ì ‘ ì‹¤í–‰ ì‹œì—ëŠ” pytest ì‹¤í–‰ì„ ê¶Œì¥
    print("\nğŸ§ª DRAEM-SevNet Lightning Model Test Suite")
    print("=" * 60)
    print("To run tests with verbose output:")
    print("pytest tests/unit/models/image/draem_sevnet/test_draem_sevnet_lightning_model.py -v -s")
    print("\nTo run specific test class:")
    print("pytest tests/unit/models/image/draem_sevnet/test_draem_sevnet_lightning_model.py::TestDraemSevNetLightningModel -v -s")
    print("\nTo run specific test method:")
    print("pytest tests/unit/models/image/draem_sevnet/test_draem_sevnet_lightning_model.py::TestDraemSevNetLightningModel::test_model_initialization -v -s")
    print("\n" + "=" * 60)
