"""Test suite for Spatial-Aware DRAEM-SevNet training with random data.

ëœë¤ ë°ì´í„°ë¡œ Spatial-Aware ê¸°ëŠ¥ë“¤ì´ ì‹¤ì œ í•™ìŠµ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ 
ì œëŒ€ë¡œ ì‘ë™í•˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤:
- ëœë¤ ë°ì´í„° ìƒì„± ë° í•™ìŠµ
- Loss ê³„ì‚° ë° backward pass
- í•™ìŠµ ìˆ˜ë ´ì„± í™•ì¸
- ë‹¤ì–‘í•œ ì•„í‚¤í…ì²˜ ì¡°í•©ì—ì„œì˜ í•™ìŠµ ì•ˆì •ì„±

Run with: 
pytest tests/unit/models/image/draem_sevnet/test_spatial_aware_training.py -v -s

Author: Taewan Hwang
"""

import pytest
import torch
import torch.nn as nn
import torch.optim as optim
import time
import numpy as np
from typing import Dict, List, Tuple
from anomalib.models.image.draem_sevnet.torch_model import DraemSevNetModel, DraemSevNetOutput
from anomalib.models.image.draem_sevnet.loss import DraemSevNetLoss

# ìƒì„¸ ì¶œë ¥ì„ ìœ„í•œ helper function
def verbose_print(message: str, level: str = "INFO"):
    """pytest -v ì‹¤í–‰ ì‹œ ìƒì„¸ ì¶œë ¥ì„ ìœ„í•œ í•¨ìˆ˜"""
    symbols = {"INFO": "â„¹ï¸", "SUCCESS": "âœ…", "WARNING": "âš ï¸", "ERROR": "âŒ", "TRAIN": "ğŸ‹ï¸"}
    print(f"\n{symbols.get(level, 'â„¹ï¸')} {message}")

# GPU ì„¤ì • helper
def get_device() -> str:
    """ì‚¬ìš© ê°€ëŠ¥í•œ GPU ë””ë°”ì´ìŠ¤ ë°˜í™˜"""
    return "cuda" if torch.cuda.is_available() else "cpu"

def setup_test_environment() -> str:
    """í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì •"""
    device = get_device()
    if device == "cuda":
        torch.cuda.empty_cache()  # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
    return device


class MockDataGenerator:
    """í•™ìŠµ í…ŒìŠ¤íŠ¸ìš© ëœë¤ ë°ì´í„° ìƒì„±ê¸°"""
    
    def __init__(self, image_size: Tuple[int, int] = (224, 224), num_classes: int = 2):
        self.image_size = image_size
        self.num_classes = num_classes
        
    def generate_batch(self, batch_size: int, device: str = "cpu") -> Dict[str, torch.Tensor]:
        """í•™ìŠµìš© ëœë¤ ë°°ì¹˜ ìƒì„±"""
        height, width = self.image_size
        
        # ì…ë ¥ ì´ë¯¸ì§€ (ì •ê·œí™”ëœ RGB)
        images = torch.randn(batch_size, 3, height, width, device=device)
        images = torch.clamp(images * 0.5 + 0.5, 0, 1)  # [0, 1] ë²”ìœ„ë¡œ ì •ê·œí™”
        
        # ì¬êµ¬ì„± íƒ€ê²Ÿ (ì•½ê°„ì˜ ë…¸ì´ì¦ˆ ì¶”ê°€)
        reconstruction_targets = images + torch.randn_like(images) * 0.1
        reconstruction_targets = torch.clamp(reconstruction_targets, 0, 1)
        
        # ë§ˆìŠ¤í¬ íƒ€ê²Ÿ (ì´ì§„ ë§ˆìŠ¤í¬)
        mask_targets = torch.randint(0, self.num_classes, (batch_size, height, width), device=device)
        
        # ì‹¬ê°ë„ íƒ€ê²Ÿ (ì—°ì†ê°’)
        severity_targets = torch.rand(batch_size, device=device)
        
        return {
            'images': images,
            'reconstruction_targets': reconstruction_targets,
            'mask_targets': mask_targets,
            'severity_targets': severity_targets
        }
    
    def generate_dataset(self, num_batches: int, batch_size: int, device: str = "cpu") -> List[Dict[str, torch.Tensor]]:
        """ì „ì²´ ë°ì´í„°ì…‹ ìƒì„±"""
        dataset = []
        for _ in range(num_batches):
            batch = self.generate_batch(batch_size, device)
            dataset.append(batch)
        return dataset


class TrainingMetrics:
    """í•™ìŠµ ë©”íŠ¸ë¦­ ì¶”ì ê¸°"""
    
    def __init__(self):
        self.reset()
    
    def reset(self):
        self.losses = {
            'total': [],
            'reconstruction': [],
            'mask': [],
            'severity': []
        }
        self.metrics = {
            'mask_accuracy': [],
            'severity_mae': []
        }
    
    def update(self, losses: Dict[str, float], metrics: Dict[str, float]):
        for key, value in losses.items():
            # 'total_loss' -> 'total' ë§¤í•‘
            mapped_key = 'total' if key == 'total_loss' else key
            if mapped_key in self.losses:
                self.losses[mapped_key].append(value)
        
        for key, value in metrics.items():
            if key in self.metrics:
                self.metrics[key].append(value)
    
    def get_recent_average(self, key: str, last_n: int = 5) -> float:
        """ìµœê·¼ Nê°œ ê°’ì˜ í‰ê· """
        if key in self.losses:
            values = self.losses[key][-last_n:]
        elif key in self.metrics:
            values = self.metrics[key][-last_n:]
        else:
            return 0.0
        
        return np.mean(values) if values else 0.0
    
    def is_converging(self, key: str = 'total', window: int = 10, threshold: float = 0.01) -> bool:
        """ìˆ˜ë ´ ì—¬ë¶€ íŒë‹¨"""
        if key not in self.losses or len(self.losses[key]) < window:
            return False
        
        recent_values = self.losses[key][-window:]
        return (max(recent_values) - min(recent_values)) < threshold


class TestSpatialAwareTraining:
    """Spatial-Aware ê¸°ëŠ¥ì˜ í•™ìŠµ í…ŒìŠ¤íŠ¸"""
    
    def test_basic_training_loop(self):
        """ê¸°ë³¸ í•™ìŠµ ë£¨í”„ í…ŒìŠ¤íŠ¸"""
        verbose_print("Testing basic training loop with random data...")
        
        # í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì •
        device = setup_test_environment()
        verbose_print(f"Using device: {device}")
        
        # GPU ë™ê¸°í™”ë¡œ ì •í™•í•œ ì„±ëŠ¥ ì¸¡ì •
        if device == "cuda":
            torch.cuda.synchronize()
        
        # ëª¨ë¸ ë° ë°ì´í„° ì„¤ì •
        model = DraemSevNetModel(
            severity_head_pooling_type="spatial_aware",
            severity_head_spatial_size=4,
            severity_head_use_spatial_attention=True
        ).to(device)
        model.train()
        
        data_generator = MockDataGenerator(image_size=(128, 128))  # ì‘ì€ ì´ë¯¸ì§€ë¡œ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
        loss_fn = DraemSevNetLoss()
        optimizer = optim.Adam(model.parameters(), lr=1e-3)  # ë†’ì€ í•™ìŠµë¥ ë¡œ ë¹ ë¥¸ ìˆ˜ë ´
        metrics_tracker = TrainingMetrics()
        
        # í•™ìŠµ íŒŒë¼ë¯¸í„°
        num_epochs = 3
        batches_per_epoch = 2
        batch_size = 8
        
        verbose_print(f"Setup: {num_epochs} epochs, {batches_per_epoch} batches/epoch, batch_size={batch_size}")
        
        for epoch in range(num_epochs):
            epoch_losses = []
            
            for batch_idx in range(batches_per_epoch):
                # ë°ì´í„° ìƒì„±
                batch_data = data_generator.generate_batch(batch_size, device)
                
                # Forward pass
                optimizer.zero_grad()
                reconstruction, mask_logits, severity_score = model(batch_data['images'])
                
                # Loss ê³„ì‚°
                total_loss = loss_fn(
                    input_image=batch_data['images'],
                    reconstruction=reconstruction,
                    anomaly_mask=batch_data['mask_targets'].unsqueeze(1),
                    prediction=mask_logits,
                    severity_gt=batch_data['severity_targets'],
                    severity_pred=severity_score
                )
                
                # Backward pass
                total_loss.backward()
                optimizer.step()
                
                # ë©”íŠ¸ë¦­ ê³„ì‚°
                with torch.no_grad():
                    mask_pred = torch.argmax(mask_logits, dim=1)
                    mask_accuracy = (mask_pred == batch_data['mask_targets']).float().mean().item()
                    severity_mae = torch.abs(severity_score - batch_data['severity_targets']).mean().item()
                
                # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
                losses = {'total_loss': total_loss.item()}
                metrics = {'mask_accuracy': mask_accuracy, 'severity_mae': severity_mae}
                metrics_tracker.update(losses, metrics)
                
                epoch_losses.append(total_loss.item())
            
            # Epoch ê²°ê³¼ ì¶œë ¥
            avg_loss = np.mean(epoch_losses)
            avg_accuracy = metrics_tracker.get_recent_average('mask_accuracy', batches_per_epoch)
            avg_mae = metrics_tracker.get_recent_average('severity_mae', batches_per_epoch)
            
            verbose_print(f"Epoch {epoch+1:2d}: Loss={avg_loss:.4f}, Acc={avg_accuracy:.3f}, MAE={avg_mae:.3f}", "TRAIN")
        
        # í•™ìŠµ ì„±ê³µ ê²€ì¦
        final_loss = metrics_tracker.get_recent_average('total_loss', 3)
        initial_loss = np.mean(metrics_tracker.losses['total'][:3])
        
        assert final_loss < initial_loss, f"Loss should decrease: initial={initial_loss:.4f}, final={final_loss:.4f}"
        assert final_loss < 10.0, f"Final loss too high: {final_loss:.4f}"
        
        verbose_print(f"Training successful: {initial_loss:.4f} â†’ {final_loss:.4f}", "SUCCESS")
    
    def test_gap_vs_spatial_aware_training(self):
        """GAP vs Spatial-Aware í•™ìŠµ ë¹„êµ í…ŒìŠ¤íŠ¸"""
        verbose_print("Testing GAP vs Spatial-Aware training comparison...")
        
        # í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì •
        device = setup_test_environment()
        
        # GPU ë™ê¸°í™”ë¡œ ì •í™•í•œ ì„±ëŠ¥ ì¸¡ì •
        if device == "cuda":
            torch.cuda.synchronize()
        
        models = {
            "GAP": DraemSevNetModel(severity_head_pooling_type="gap").to(device),
            "Spatial-Aware": DraemSevNetModel(
                severity_head_pooling_type="spatial_aware",
                severity_head_spatial_size=4,
                severity_head_use_spatial_attention=True
            ).to(device)
        }
        
        data_generator = MockDataGenerator(image_size=(128, 128))
        loss_fn = DraemSevNetLoss()
        
        # í•™ìŠµ íŒŒë¼ë¯¸í„°
        num_epochs = 2
        batches_per_epoch = 2
        batch_size = 8
        lr = 1e-3
        
        training_results = {}
        
        for model_name, model in models.items():
            verbose_print(f"Training {model_name}...")
            
            model.train()
            optimizer = optim.Adam(model.parameters(), lr=lr)
            metrics_tracker = TrainingMetrics()
            
            # ë™ì¼í•œ ë°ì´í„°ë¡œ í•™ìŠµ
            torch.manual_seed(42)  # ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ê³ ì •
            
            for epoch in range(num_epochs):
                for batch_idx in range(batches_per_epoch):
                    # ë°ì´í„° ìƒì„±
                    batch_data = data_generator.generate_batch(batch_size, device)
                    
                    # Forward pass
                    optimizer.zero_grad()
                    reconstruction, mask_logits, severity_score = model(batch_data['images'])
                    
                    # Loss ê³„ì‚°
                    total_loss = loss_fn(
                        input_image=batch_data['images'],
                        reconstruction=reconstruction,
                        anomaly_mask=batch_data['mask_targets'].unsqueeze(1),
                        prediction=mask_logits,
                        severity_gt=batch_data['severity_targets'],
                        severity_pred=severity_score
                    )
                
                # Backward pass
                    total_loss.backward()
                    optimizer.step()
                    
                    # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
                    losses = {'total_loss': total_loss.item()}
                    metrics_tracker.update(losses, {})
            
            # ê²°ê³¼ ì €ì¥
            final_loss = metrics_tracker.get_recent_average('total_loss', 2)
            training_results[model_name] = {
                'final_loss': final_loss,
                'loss_history': metrics_tracker.losses['total'],
                'converged': metrics_tracker.is_converging('total', window=3, threshold=0.1)
            }
            
            verbose_print(f"{model_name}: final_loss={final_loss:.4f}, converged={training_results[model_name]['converged']}")
        
        # ë‘ ëª¨ë¸ ëª¨ë‘ í•™ìŠµì´ ì„±ê³µí•´ì•¼ í•¨
        for model_name, results in training_results.items():
            assert results['final_loss'] < 10.0, f"{model_name} final loss too high: {results['final_loss']:.4f}"
            assert len(results['loss_history']) > 0, f"{model_name} no loss history recorded"
        
        verbose_print("GAP vs Spatial-Aware comparison passed!", "SUCCESS")
    
    def test_multi_scale_spatial_aware_training(self):
        """Multi-scale Spatial-Aware í•™ìŠµ í…ŒìŠ¤íŠ¸"""
        verbose_print("Testing Multi-scale Spatial-Aware training...")
        
        # í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì •
        device = setup_test_environment()
        
        # GPU ë™ê¸°í™”ë¡œ ì •í™•í•œ ì„±ëŠ¥ ì¸¡ì •
        if device == "cuda":
            torch.cuda.synchronize()
        
        model = DraemSevNetModel(
            severity_head_mode="multi_scale",
            severity_head_pooling_type="spatial_aware",
            severity_head_spatial_size=4,
            severity_head_use_spatial_attention=True
        ).to(device)
        model.train()
        
        data_generator = MockDataGenerator(image_size=(128, 128))
        loss_fn = DraemSevNetLoss()
        optimizer = optim.Adam(model.parameters(), lr=1e-3)
        
        # í•™ìŠµ íŒŒë¼ë¯¸í„°
        num_epochs = 2
        batches_per_epoch = 2
        batch_size = 4
        
        losses = []
        
        for epoch in range(num_epochs):
            epoch_losses = []
            
            for batch_idx in range(batches_per_epoch):
                # ë°ì´í„° ìƒì„±
                batch_data = data_generator.generate_batch(batch_size, device)
                
                # Forward pass
                optimizer.zero_grad()
                reconstruction, mask_logits, severity_score = model(batch_data['images'])
                
                # ê¸°ë³¸ ê²€ì¦
                assert reconstruction.shape == (batch_size, 3, 128, 128)
                assert mask_logits.shape == (batch_size, 2, 128, 128)
                assert severity_score.shape == (batch_size,)
                # í›ˆë ¨ ëª¨ë“œì—ì„œëŠ” raw severity ê°’ì´ ì‹¤ìˆ˜ ë²”ìœ„ [-âˆ, âˆ]ë¥¼ ê°€ì§
                assert torch.all(torch.isfinite(severity_score))
                
                # Loss ê³„ì‚°
                total_loss = loss_fn(
                    input_image=batch_data['images'],
                    reconstruction=reconstruction,
                    anomaly_mask=batch_data['mask_targets'].unsqueeze(1),
                    prediction=mask_logits,
                    severity_gt=batch_data['severity_targets'],
                    severity_pred=severity_score
                )
                
                # Backward pass
                total_loss.backward()
                optimizer.step()
                
                epoch_losses.append(total_loss.item())
            
            avg_loss = np.mean(epoch_losses)
            losses.extend(epoch_losses)
            verbose_print(f"Multi-scale Epoch {epoch+1}: Loss={avg_loss:.4f}", "TRAIN")
        
        # í•™ìŠµ ì„±ê³µ ê²€ì¦
        assert len(losses) > 0, "No losses recorded"
        final_loss = np.mean(losses[-2:])
        initial_loss = np.mean(losses[:2])
        
        assert final_loss < 15.0, f"Multi-scale final loss too high: {final_loss:.4f}"
        verbose_print(f"Multi-scale training: {initial_loss:.4f} â†’ {final_loss:.4f}", "SUCCESS")
    
    def test_different_spatial_sizes_training(self):
        """ë‹¤ì–‘í•œ spatial_sizeì—ì„œì˜ í•™ìŠµ í…ŒìŠ¤íŠ¸"""
        verbose_print("Testing training with different spatial sizes...")
        
        # í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì •
        device = setup_test_environment()
        
        # GPU ë™ê¸°í™”ë¡œ ì •í™•í•œ ì„±ëŠ¥ ì¸¡ì •
        if device == "cuda":
            torch.cuda.synchronize()
        
        spatial_sizes = [2, 4]
        data_generator = MockDataGenerator(image_size=(128, 128))
        loss_fn = DraemSevNetLoss()
        
        # í•™ìŠµ íŒŒë¼ë¯¸í„°
        num_epochs = 2
        batches_per_epoch = 2
        batch_size = 8
        lr = 1e-3
        
        results = {}
        
        for spatial_size in spatial_sizes:
            verbose_print(f"Training with spatial_size={spatial_size}...")
            
            model = DraemSevNetModel(
                severity_head_pooling_type="spatial_aware",
                severity_head_spatial_size=spatial_size,
                severity_head_use_spatial_attention=True
            ).to(device)
            model.train()
            
            optimizer = optim.Adam(model.parameters(), lr=lr)
            losses = []
            
            # ë™ì¼í•œ ë°ì´í„°ë¡œ í•™ìŠµ
            torch.manual_seed(42)
            
            for epoch in range(num_epochs):
                for batch_idx in range(batches_per_epoch):
                    # ë°ì´í„° ìƒì„±
                    batch_data = data_generator.generate_batch(batch_size, device)
                    
                    # Forward pass
                    optimizer.zero_grad()
                    reconstruction, mask_logits, severity_score = model(batch_data['images'])
                    
                    # Loss ê³„ì‚°
                    total_loss = loss_fn(
                        input_image=batch_data['images'],
                        reconstruction=reconstruction,
                        anomaly_mask=batch_data['mask_targets'].unsqueeze(1),
                        prediction=mask_logits,
                        severity_gt=batch_data['severity_targets'],
                        severity_pred=severity_score
                    )
                
                # Backward pass
                    total_loss.backward()
                    optimizer.step()
                    
                    losses.append(total_loss.item())
            
            # ê²°ê³¼ ì €ì¥
            final_loss = np.mean(losses[-2:])
            results[spatial_size] = {
                'final_loss': final_loss,
                'loss_history': losses,
                'params': sum(p.numel() for p in model.parameters())
            }
            
            verbose_print(f"Spatial size {spatial_size}: final_loss={final_loss:.4f}, params={results[spatial_size]['params']:,}")
        
        # ëª¨ë“  spatial_sizeì—ì„œ í•™ìŠµì´ ì„±ê³µí•´ì•¼ í•¨
        for spatial_size, result in results.items():
            assert result['final_loss'] < 10.0, f"Spatial size {spatial_size} final loss too high: {result['final_loss']:.4f}"
        
        # íŒŒë¼ë¯¸í„° ìˆ˜ëŠ” spatial_sizeê°€ í´ìˆ˜ë¡ ë§ì•„ì•¼ í•¨
        assert results[4]['params'] > results[2]['params'], "Parameter scaling incorrect"
        
        verbose_print("Different spatial sizes passed!", "SUCCESS")
    
    def test_gradient_flow_stability(self):
        """Gradient íë¦„ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸"""
        verbose_print("Testing gradient flow stability...")
        
        # í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì •
        device = setup_test_environment()
        
        # GPU ë™ê¸°í™”ë¡œ ì •í™•í•œ ì„±ëŠ¥ ì¸¡ì •
        if device == "cuda":
            torch.cuda.synchronize()
        
        model = DraemSevNetModel(
            severity_head_pooling_type="spatial_aware",
            severity_head_spatial_size=4,
            severity_head_use_spatial_attention=True
        ).to(device)
        model.train()
        
        data_generator = MockDataGenerator(image_size=(128, 128))
        loss_fn = DraemSevNetLoss()
        optimizer = optim.Adam(model.parameters(), lr=1e-4)  # ë” ë‚®ì€ í•™ìŠµë¥  ì‚¬ìš©
        
        batch_size = 8
        num_iterations = 5
        
        gradient_norms = {
            'total': [],
            'reconstructive': [],
            'discriminative': [],
            'severity_head': []
        }
        
        for iteration in range(num_iterations):
            # ë°ì´í„° ìƒì„±
            batch_data = data_generator.generate_batch(batch_size, device)
            
            # Forward pass
            model.zero_grad()
            reconstruction, mask_logits, severity_score = model(batch_data['images'])
            
            # Loss ê³„ì‚°
            total_loss = loss_fn(
                input_image=batch_data['images'],
                reconstruction=reconstruction,
                anomaly_mask=batch_data['mask_targets'].unsqueeze(1),
                prediction=mask_logits,
                severity_gt=batch_data['severity_targets'],
                severity_pred=severity_score
            )
                
                # Backward pass
            total_loss.backward()
            
            # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ìœ¼ë¡œ ì•ˆì •ì„± í™•ë³´
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=100.0)
            optimizer.step()
            
            # Gradient norm ê³„ì‚°
            total_norm = 0.0
            reconstructive_norm = 0.0
            discriminative_norm = 0.0
            severity_norm = 0.0
            
            for name, param in model.named_parameters():
                if param.grad is not None:
                    param_norm = param.grad.data.norm(2).item()
                    total_norm += param_norm ** 2
                    
                    if 'reconstructive' in name:
                        reconstructive_norm += param_norm ** 2
                    elif 'discriminative' in name:
                        discriminative_norm += param_norm ** 2
                    elif 'severity_head' in name:
                        severity_norm += param_norm ** 2
            
            gradient_norms['total'].append(total_norm ** 0.5)
            gradient_norms['reconstructive'].append(reconstructive_norm ** 0.5)
            gradient_norms['discriminative'].append(discriminative_norm ** 0.5)
            gradient_norms['severity_head'].append(severity_norm ** 0.5)
        
        # Gradient ì•ˆì •ì„± ê²€ì¦
        for component, norms in gradient_norms.items():
            avg_norm = np.mean(norms)
            std_norm = np.std(norms)
            max_norm = max(norms)
            
            verbose_print(f"{component}: avg={avg_norm:.4f}, std={std_norm:.4f}, max={max_norm:.4f}")
            
            # Gradient exploding ì²´í¬ (í´ë¦¬í•‘ í›„ ë” ë‚®ì€ ì„ê³„ê°’)
            assert max_norm < 200.0, f"{component} gradient exploding: max_norm={max_norm:.4f}"
            
            # Gradient vanishing ì²´í¬ (severity_headëŠ” ë” ì‘ì„ ìˆ˜ ìˆìŒ)
            min_threshold = 1e-6 if component == 'severity_head' else 1e-5
            assert avg_norm > min_threshold, f"{component} gradient vanishing: avg_norm={avg_norm:.6f}"
        
        verbose_print("Gradient flow stability passed!", "SUCCESS")
    
    def test_inference_after_training(self):
        """í•™ìŠµ í›„ ì¶”ë¡  ëª¨ë“œ í…ŒìŠ¤íŠ¸"""
        verbose_print("Testing inference mode after training...")
        
        # í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì •
        device = setup_test_environment()
        
        # GPU ë™ê¸°í™”ë¡œ ì •í™•í•œ ì„±ëŠ¥ ì¸¡ì •
        if device == "cuda":
            torch.cuda.synchronize()
        
        model = DraemSevNetModel(
            severity_head_pooling_type="spatial_aware",
            severity_head_spatial_size=4,
            severity_head_use_spatial_attention=True
        ).to(device)
        
        data_generator = MockDataGenerator(image_size=(128, 128))
        loss_fn = DraemSevNetLoss()
        optimizer = optim.Adam(model.parameters(), lr=1e-3)
        
        # ê°„ë‹¨í•œ í•™ìŠµ
        model.train()
        num_training_steps = 5
        batch_size = 4
        
        for step in range(num_training_steps):
            batch_data = data_generator.generate_batch(batch_size, device)
            
            optimizer.zero_grad()
            reconstruction, mask_logits, severity_score = model(batch_data['images'])
            
            total_loss = loss_fn(
                input_image=batch_data['images'],
                reconstruction=reconstruction,
                anomaly_mask=batch_data['mask_targets'].unsqueeze(1),
                prediction=mask_logits,
                severity_gt=batch_data['severity_targets'],
                severity_pred=severity_score
            )
            

            total_loss.backward()
            optimizer.step()
        
        # ì¶”ë¡  ëª¨ë“œ í…ŒìŠ¤íŠ¸
        model.eval()
        test_batch_size = 6
        test_batch = data_generator.generate_batch(test_batch_size, device)
        
        with torch.no_grad():
            output = model(test_batch['images'])
        
        # ì¶”ë¡  ì¶œë ¥ ê²€ì¦
        assert isinstance(output, DraemSevNetOutput)
        assert output.final_score.shape == (test_batch_size,)
        assert output.normalized_severity_score.shape == (test_batch_size,)
        assert output.raw_severity_score.shape == (test_batch_size,)
        assert output.mask_score.shape == (test_batch_size,)
        assert output.anomaly_map.shape == (test_batch_size, 128, 128)
        
        # ê°’ ë²”ìœ„ ê²€ì¦
        assert torch.all((output.final_score >= 0) & (output.final_score <= 1))
        assert torch.all((output.normalized_severity_score >= 0) & (output.normalized_severity_score <= 1))
        assert torch.all(output.raw_severity_score >= 0)  # ì¶”ë¡  ëª¨ë“œì—ì„œëŠ” clampë¡œ 0 ì´ìƒ
        assert torch.all((output.mask_score >= 0) & (output.mask_score <= 1))
        assert torch.all((output.anomaly_map >= 0) & (output.anomaly_map <= 1))
        
        verbose_print(f"Inference output ranges:")
        verbose_print(f"  final_score: [{output.final_score.min():.3f}, {output.final_score.max():.3f}]")
        verbose_print(f"  normalized_severity_score: [{output.normalized_severity_score.min():.3f}, {output.normalized_severity_score.max():.3f}]")
        verbose_print(f"  raw_severity_score: [{output.raw_severity_score.min():.3f}, {output.raw_severity_score.max():.3f}]")
        verbose_print(f"  mask_score: [{output.mask_score.min():.3f}, {output.mask_score.max():.3f}]")
        
        verbose_print("Inference after training test passed!", "SUCCESS")


class TestSpatialAwareTrainingStability:
    """Spatial-Aware í•™ìŠµ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸"""
    
    def test_training_reproducibility(self):
        """í•™ìŠµ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸ (ì¬í˜„ì„± ê²€ì¦ ì œê±°)"""
        verbose_print("Testing training stability...")
        
        # í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì •
        device = setup_test_environment()
        
        def train_model(seed: int) -> List[float]:
            """ë‹¨ì¼ ëª¨ë¸ í•™ìŠµ"""
            torch.manual_seed(seed)
            np.random.seed(seed)
            
            model = DraemSevNetModel(
                severity_head_pooling_type="spatial_aware",
                severity_head_spatial_size=4,
                severity_head_use_spatial_attention=True
            ).to(device)
            model.train()
            
            data_generator = MockDataGenerator(image_size=(128, 128))
            loss_fn = DraemSevNetLoss()
            optimizer = optim.Adam(model.parameters(), lr=1e-3)
            
            losses = []
            for step in range(5):
                batch_data = data_generator.generate_batch(4, device)
                
                optimizer.zero_grad()
                reconstruction, mask_logits, severity_score = model(batch_data['images'])
                
                total_loss = loss_fn(
                    input_image=batch_data['images'],
                    reconstruction=reconstruction,
                    anomaly_mask=batch_data['mask_targets'].unsqueeze(1),
                    prediction=mask_logits,
                    severity_gt=batch_data['severity_targets'],
                    severity_pred=severity_score
                )
                total_loss.backward()
                optimizer.step()
                
                losses.append(total_loss.item())
            
            return losses
        
        # ë‹¨ì¼ ëª¨ë¸ë¡œ í•™ìŠµ ì•ˆì •ì„± í™•ì¸
        seed = 42
        losses = train_model(seed)
        
        # ê¸°ë³¸ì ì¸ í•™ìŠµ ì•ˆì •ì„± ê²€ì¦
        avg_loss = np.mean(losses)
        std_loss = np.std(losses)
        
        # ì†ì‹¤ì´ í•©ë¦¬ì ì¸ ë²”ìœ„ì— ìˆëŠ”ì§€ í™•ì¸
        assert 0.1 < avg_loss < 10.0, f"Average loss out of range: {avg_loss:.4f}"
        assert std_loss < avg_loss, f"Loss too unstable: std={std_loss:.4f}, avg={avg_loss:.4f}"
        assert all(loss > 0 for loss in losses), "Negative losses detected"
        
        verbose_print(f"Training stability: avg_loss={avg_loss:.4f}Â±{std_loss:.4f}")
        verbose_print("Training stability test passed!", "SUCCESS")
    
    def test_memory_efficiency(self):
        """ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í…ŒìŠ¤íŠ¸"""
        verbose_print("Testing memory efficiency...")
        
        # í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì •
        device = setup_test_environment()
        
        models = {
            "GAP": DraemSevNetModel(severity_head_pooling_type="gap").to(device),
            "Spatial-Aware-2": DraemSevNetModel(
                severity_head_pooling_type="spatial_aware",
                severity_head_spatial_size=2
            ).to(device),
            "Spatial-Aware-4": DraemSevNetModel(
                severity_head_pooling_type="spatial_aware",
                severity_head_spatial_size=4
            ).to(device),
            "Spatial-Aware-8": DraemSevNetModel(
                severity_head_pooling_type="spatial_aware",
                severity_head_spatial_size=8
            ).to(device)
        }
        
        data_generator = MockDataGenerator(image_size=(128, 128))
        loss_fn = DraemSevNetLoss()
        
        batch_size = 8
        memory_test_results = {}
        
        for model_name, model in models.items():
            model.train()
            optimizer = optim.Adam(model.parameters(), lr=1e-3)
            
            try:
                # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ í° ë°°ì¹˜
                batch_data = data_generator.generate_batch(batch_size, device)
                
                optimizer.zero_grad()
                reconstruction, mask_logits, severity_score = model(batch_data['images'])
                
                total_loss = loss_fn(
                    input_image=batch_data['images'],
                    reconstruction=reconstruction,
                    anomaly_mask=batch_data['mask_targets'].unsqueeze(1),
                    prediction=mask_logits,
                    severity_gt=batch_data['severity_targets'],
                    severity_pred=severity_score
                )
                total_loss.backward()
                optimizer.step()
                
                # íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
                total_params = sum(p.numel() for p in model.parameters())
                severity_params = sum(p.numel() for p in model.severity_head.parameters())
                
                memory_test_results[model_name] = {
                    'success': True,
                    'total_params': total_params,
                    'severity_params': severity_params,
                    'final_loss': total_loss.item()
                }
                
                verbose_print(f"âœ… {model_name}: {total_params:,} params, loss={total_loss.item():.4f}")
                
            except Exception as e:
                memory_test_results[model_name] = {
                    'success': False,
                    'error': str(e)
                }
                verbose_print(f"âŒ {model_name}: {e}", "ERROR")
        
        # ëª¨ë“  ëª¨ë¸ì´ ì„±ê³µí•´ì•¼ í•¨
        for model_name, result in memory_test_results.items():
            assert result['success'], f"{model_name} failed: {result.get('error', 'Unknown error')}"
        
        verbose_print("Memory efficiency test passed!", "SUCCESS")
    
    def test_learning_convergence(self):
        """í•™ìŠµ ìˆ˜ë ´ì„± í…ŒìŠ¤íŠ¸"""
        verbose_print("Testing learning convergence...")
        
        # í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì •
        device = setup_test_environment()
        
        model = DraemSevNetModel(
            severity_head_pooling_type="spatial_aware",
            severity_head_spatial_size=4,
            severity_head_use_spatial_attention=True
        ).to(device)
        model.train()
        
        data_generator = MockDataGenerator(image_size=(128, 128))
        loss_fn = DraemSevNetLoss()
        optimizer = optim.Adam(model.parameters(), lr=1e-3)
        
        # ë” ê¸´ í•™ìŠµìœ¼ë¡œ ìˆ˜ë ´ì„± í™•ì¸
        num_epochs = 10
        batches_per_epoch = 3
        batch_size = 4
        
        losses = []
        
        for epoch in range(num_epochs):
            epoch_losses = []
            
            for batch_idx in range(batches_per_epoch):
                batch_data = data_generator.generate_batch(batch_size, device)
                
                optimizer.zero_grad()
                reconstruction, mask_logits, severity_score = model(batch_data['images'])
                
                total_loss = loss_fn(
                    input_image=batch_data['images'],
                    reconstruction=reconstruction,
                    anomaly_mask=batch_data['mask_targets'].unsqueeze(1),
                    prediction=mask_logits,
                    severity_gt=batch_data['severity_targets'],
                    severity_pred=severity_score
                )
                total_loss.backward()
                optimizer.step()
                
                epoch_losses.append(total_loss.item())
            
            avg_loss = np.mean(epoch_losses)
            losses.extend(epoch_losses)
            
            if epoch % 2 == 0:
                verbose_print(f"Epoch {epoch+1:2d}: avg_loss={avg_loss:.4f}", "TRAIN")
        
        # ìˆ˜ë ´ì„± ë¶„ì„
        early_losses = losses[:5]
        late_losses = losses[-5:]
        
        early_avg = np.mean(early_losses)
        late_avg = np.mean(late_losses)
        loss_reduction = (early_avg - late_avg) / early_avg
        
        verbose_print(f"Loss reduction: {early_avg:.4f} â†’ {late_avg:.4f} ({loss_reduction*100:.1f}%)")
        
        # ì†ì‹¤ì´ ê°ì†Œí•´ì•¼ í•¨
        assert loss_reduction > 0.1, f"Insufficient loss reduction: {loss_reduction*100:.1f}%"
        assert late_avg < 5.0, f"Final loss too high: {late_avg:.4f}"
        
        # Loss varianceê°€ ì¤„ì–´ë“¤ì–´ì•¼ í•¨ (ì•ˆì •í™”)
        early_std = np.std(early_losses)
        late_std = np.std(late_losses)
        
        verbose_print(f"Loss stability: std {early_std:.4f} â†’ {late_std:.4f}")
        
        verbose_print("Learning convergence test passed!", "SUCCESS")


# pytestë¡œ ì‹¤í–‰ ì‹œ ìë™ìœ¼ë¡œ ì‹¤í–‰ë˜ëŠ” í†µí•© í…ŒìŠ¤íŠ¸
def test_spatial_aware_training_integration_summary():
    """ì „ì²´ Spatial-Aware í•™ìŠµ í…ŒìŠ¤íŠ¸ ìš”ì•½"""
    verbose_print("ğŸ§ª Spatial-Aware Training Test Suite Integration Summary", "INFO")
    verbose_print("=" * 70)
    
    # í…ŒìŠ¤íŠ¸ êµ¬ì„± ìš”ì†Œ í™•ì¸
    test_components = [
        "Basic training loop with random data",
        "GAP vs Spatial-Aware training comparison",
        "Multi-scale Spatial-Aware training",
        "Different spatial sizes training",
        "Gradient flow stability",
        "Inference after training",
        "Training reproducibility",
        "Memory efficiency with various configurations",
        "Learning convergence analysis"
    ]
    
    verbose_print("Test components covered:")
    for i, component in enumerate(test_components, 1):
        verbose_print(f"  {i:2d}. {component}")
    
    verbose_print(f"\nğŸ¯ Total {len(test_components)} training test categories covered!", "SUCCESS")
    verbose_print("\nğŸ“‹ Key Training Features Tested:")
    verbose_print("  âœ… Random data generation and training loop")
    verbose_print("  âœ… Loss calculation and backward propagation")
    verbose_print("  âœ… Gradient flow stability and convergence")
    verbose_print("  âœ… Multiple architecture training comparison")
    verbose_print("  âœ… Memory efficiency across configurations")
    verbose_print("  âœ… Training reproducibility")
    verbose_print("  âœ… Inference mode validation after training")
    
    verbose_print("\nRun individual tests with:")
    verbose_print("pytest tests/unit/models/image/draem_sevnet/test_spatial_aware_training.py::TestSpatialAwareTraining::test_<method_name> -v -s")


if __name__ == "__main__":
    # ì§ì ‘ ì‹¤í–‰ ì‹œì—ëŠ” pytest ì‹¤í–‰ì„ ê¶Œì¥
    print("\nğŸ§ª DRAEM-SevNet Spatial-Aware Training Test Suite")
    print("=" * 60)
    print("To run tests with verbose output:")
    print("pytest tests/unit/models/image/draem_sevnet/test_spatial_aware_training.py -v -s")
    print("\nTo run specific test class:")
    print("pytest tests/unit/models/image/draem_sevnet/test_spatial_aware_training.py::TestSpatialAwareTraining -v -s")
    print("\nTo run specific test method:")
    print("pytest tests/unit/models/image/draem_sevnet/test_spatial_aware_training.py::TestSpatialAwareTraining::test_basic_training_loop -v -s")
    print("\n" + "=" * 60)
