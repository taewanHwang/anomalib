"""Test suite for Enhanced DiscriminativeSubNetwork.

DRAEM-SevNetì„ ìœ„í•œ ìˆ˜ì •ëœ DiscriminativeSubNetworkì˜ 
encoder features ë…¸ì¶œ ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.

Run with: pytest tests/unit/models/image/draem_sevnet/test_discriminative_subnet_modification.py -v -s
Author: Taewan Hwang
"""

import torch
from anomalib.models.image.draem_sevnet.torch_model import DiscriminativeSubNetwork
from anomalib.models.image.draem_sevnet.severity_head import SeverityHead
from anomalib.models.image.draem.torch_model import DiscriminativeSubNetwork as OriginalSubNet

# ìƒì„¸ ì¶œë ¥ì„ ìœ„í•œ helper function
def verbose_print(message: str, level: str = "INFO"):
    """ìƒì„¸ ì¶œë ¥ì„ ìœ„í•œ í•¨ìˆ˜"""
    symbols = {"INFO": "â„¹ï¸", "SUCCESS": "âœ…", "WARNING": "âš ï¸", "ERROR": "âŒ"}
    print(f"\n{symbols.get(level, 'â„¹ï¸')} {message}")


class TestDiscriminativeSubNetworkEnhancement:
    """Enhanced DiscriminativeSubNetwork ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
    
    def test_backward_compatibility_mode(self):
        """Backward compatibility ëª¨ë“œ í…ŒìŠ¤íŠ¸ (expose_features=False)"""
        verbose_print("Testing backward compatibility mode...")
        
        subnet = DiscriminativeSubNetwork(
            in_channels=6, 
            out_channels=2, 
            expose_features=False
        )
        
        # ì…ë ¥: ì›ë³¸(3ch) + ë³µì›(3ch) = 6ch
        batch_size = 4
        input_tensor = torch.randn(batch_size, 6, 224, 224)
        
        verbose_print(f"Input shape: {input_tensor.shape}")
        verbose_print(f"expose_features=False")
        
        output = subnet(input_tensor)
        
        verbose_print(f"Output type: {type(output)}")
        verbose_print(f"Output shape: {output.shape}")
        
        # ê¸°ì¡´ ë°©ì‹ê³¼ ë™ì¼: mask_logitsë§Œ ë°˜í™˜
        assert isinstance(output, torch.Tensor)
        assert output.shape == (batch_size, 2, 224, 224)
        assert not isinstance(output, tuple)
        
        verbose_print("Backward compatibility test passed!", "SUCCESS")
        
    def test_feature_exposure_mode(self):
        """Feature exposure ëª¨ë“œ í…ŒìŠ¤íŠ¸ (expose_features=True)"""
        subnet = DiscriminativeSubNetwork(
            in_channels=6, 
            out_channels=2, 
            expose_features=True
        )
        
        batch_size = 4
        input_tensor = torch.randn(batch_size, 6, 224, 224)
        
        output = subnet(input_tensor)
        
        # Tuple ë°˜í™˜: (mask_logits, encoder_features)
        assert isinstance(output, tuple)
        assert len(output) == 2
        
        mask_logits, encoder_features = output
        
        # Mask logits ê²€ì¦
        assert mask_logits.shape == (batch_size, 2, 224, 224)
        
        # Encoder features ê²€ì¦
        assert isinstance(encoder_features, dict)
        expected_keys = ['act1', 'act2', 'act3', 'act4', 'act5', 'act6']
        assert set(encoder_features.keys()) == set(expected_keys)
        
        # Feature map í¬ê¸° ê²€ì¦ (base_width=64 ê¸°ì¤€)
        expected_shapes = {
            'act1': (batch_size, 64, 224, 224),    # Base resolution
            'act2': (batch_size, 128, 112, 112),   # 1/2 resolution  
            'act3': (batch_size, 256, 56, 56),     # 1/4 resolution
            'act4': (batch_size, 512, 28, 28),     # 1/8 resolution
            'act5': (batch_size, 512, 14, 14),     # 1/16 resolution
            'act6': (batch_size, 512, 7, 7),       # 1/32 resolution
        }
        
        for key, expected_shape in expected_shapes.items():
            actual_shape = encoder_features[key].shape
            assert actual_shape == expected_shape, f"{key}: expected {expected_shape}, got {actual_shape}"
            
    def test_default_behavior(self):
        """ê¸°ë³¸ ì„¤ì • í…ŒìŠ¤íŠ¸ (expose_features=Trueê°€ ê¸°ë³¸ê°’)"""
        subnet = DiscriminativeSubNetwork()  # ê¸°ë³¸ ì„¤ì •
        
        assert subnet.expose_features is True
        
        batch_size = 2
        input_tensor = torch.randn(batch_size, 6, 224, 224)
        
        output = subnet(input_tensor)
        
        # ê¸°ë³¸ê°’ì´ë¯€ë¡œ tuple ë°˜í™˜
        assert isinstance(output, tuple)
        assert len(output) == 2
        
    def test_different_input_sizes(self):
        """ë‹¤ì–‘í•œ ì…ë ¥ í¬ê¸° í…ŒìŠ¤íŠ¸"""
        subnet = DiscriminativeSubNetwork(expose_features=True)
        
        # 256x256 ì…ë ¥
        input_256 = torch.randn(2, 6, 256, 256)
        mask_logits_256, features_256 = subnet(input_256)
        
        assert mask_logits_256.shape == (2, 2, 256, 256)
        assert features_256['act1'].shape == (2, 64, 256, 256)
        assert features_256['act6'].shape == (2, 512, 8, 8)  # 256/32 = 8
        
        # 512x512 ì…ë ¥  
        input_512 = torch.randn(2, 6, 512, 512)
        mask_logits_512, features_512 = subnet(input_512)
        
        assert mask_logits_512.shape == (2, 2, 512, 512)
        assert features_512['act1'].shape == (2, 64, 512, 512)
        assert features_512['act6'].shape == (2, 512, 16, 16)  # 512/32 = 16
        
    def test_custom_base_width(self):
        """Custom base_width í…ŒìŠ¤íŠ¸"""
        custom_base_width = 32
        subnet = DiscriminativeSubNetwork(
            base_width=custom_base_width, 
            expose_features=True
        )
        
        batch_size = 2
        input_tensor = torch.randn(batch_size, 6, 224, 224)
        
        mask_logits, encoder_features = subnet(input_tensor)
        
        # Base widthì— ë”°ë¥¸ ì±„ë„ ìˆ˜ í™•ì¸
        expected_shapes = {
            'act1': (batch_size, custom_base_width, 224, 224),           # 32
            'act2': (batch_size, custom_base_width * 2, 112, 112),       # 64
            'act3': (batch_size, custom_base_width * 4, 56, 56),         # 128
            'act4': (batch_size, custom_base_width * 8, 28, 28),         # 256
            'act5': (batch_size, custom_base_width * 8, 14, 14),         # 256
            'act6': (batch_size, custom_base_width * 8, 7, 7),           # 256
        }
        
        for key, expected_shape in expected_shapes.items():
            actual_shape = encoder_features[key].shape
            assert actual_shape == expected_shape, f"{key}: expected {expected_shape}, got {actual_shape}"
            
    def test_custom_output_channels(self):
        """Custom output channels í…ŒìŠ¤íŠ¸"""
        custom_out_channels = 3
        subnet = DiscriminativeSubNetwork(
            out_channels=custom_out_channels,
            expose_features=True
        )
        
        batch_size = 2
        input_tensor = torch.randn(batch_size, 6, 224, 224)
        
        mask_logits, encoder_features = subnet(input_tensor)
        
        # Output channels í™•ì¸
        assert mask_logits.shape == (batch_size, custom_out_channels, 224, 224)
        
        # FeaturesëŠ” output channelsì™€ ë¬´ê´€í•˜ê²Œ ë™ì¼
        assert encoder_features['act6'].shape == (batch_size, 512, 7, 7)
        
    def test_gradient_flow(self):
        """Gradient íë¦„ í…ŒìŠ¤íŠ¸"""
        subnet = DiscriminativeSubNetwork(expose_features=True)
        input_tensor = torch.randn(2, 6, 224, 224, requires_grad=True)
        
        mask_logits, encoder_features = subnet(input_tensor)
        
        # Mask logitsì—ì„œ loss ê³„ì‚°
        mask_loss = mask_logits.sum()
        mask_loss.backward(retain_graph=True)
        
        # Input gradient í™•ì¸
        assert input_tensor.grad is not None
        assert not torch.all(input_tensor.grad == 0)
        
        # Features gradient í™•ì¸ (act6 ì‚¬ìš©)
        input_tensor.grad.zero_()
        feature_loss = encoder_features['act6'].sum()
        feature_loss.backward()
        
        assert input_tensor.grad is not None
        assert not torch.all(input_tensor.grad == 0)
        
    def test_reproducibility(self):
        """ì¬í˜„ì„± í…ŒìŠ¤íŠ¸"""
        torch.manual_seed(42)
        
        subnet = DiscriminativeSubNetwork(expose_features=True)
        input_tensor = torch.randn(2, 6, 224, 224)
        
        # ì²« ë²ˆì§¸ ì‹¤í–‰
        torch.manual_seed(42)
        mask_logits1, features1 = subnet(input_tensor)
        
        # ë‘ ë²ˆì§¸ ì‹¤í–‰ (ê°™ì€ seed)
        torch.manual_seed(42)
        mask_logits2, features2 = subnet(input_tensor)
        
        # ê²°ê³¼ ë™ì¼ì„± í™•ì¸
        assert torch.allclose(mask_logits1, mask_logits2, atol=1e-8)
        
        for key in features1.keys():
            assert torch.allclose(features1[key], features2[key], atol=1e-8)
            
    def test_memory_efficiency(self):
        """ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í…ŒìŠ¤íŠ¸"""
        subnet = DiscriminativeSubNetwork(expose_features=True)
        
        # í° ë°°ì¹˜ë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
        large_batch_size = 16
        input_tensor = torch.randn(large_batch_size, 6, 224, 224)
        
        mask_logits, encoder_features = subnet(input_tensor)
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ í•©ë¦¬ì ì¸ì§€ í™•ì¸
        total_elements = 0
        for key, feature in encoder_features.items():
            total_elements += feature.numel()
            
        # Featuresì˜ ì´ ì›ì†Œ ìˆ˜ê°€ í•©ë¦¬ì ì¸ ë²”ìœ„ì¸ì§€ í™•ì¸
        assert total_elements > 0
        print(f"Total feature elements: {total_elements:,}")
        
        # Memory cleanup
        del mask_logits, encoder_features, input_tensor


class TestDiscriminativeSubNetworkIntegration:
    """í†µí•© í…ŒìŠ¤íŠ¸"""
    
    def test_compatibility_with_original_draem(self):
        """Original DRAEMê³¼ì˜ í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸"""
        
        # ë™ì¼í•œ ì„¤ì •ìœ¼ë¡œ ìƒì„±
        original_subnet = OriginalSubNet(in_channels=6, out_channels=2, base_width=64)
        enhanced_subnet = DiscriminativeSubNetwork(
            in_channels=6, 
            out_channels=2, 
            base_width=64,
            expose_features=False  # ê¸°ì¡´ ë™ì‘ê³¼ ë™ì¼
        )
        
        batch_size = 2
        input_tensor = torch.randn(batch_size, 6, 224, 224)
        
        # Forward pass
        with torch.no_grad():
            original_output = original_subnet(input_tensor)
            enhanced_output = enhanced_subnet(input_tensor)
        
        # ì¶œë ¥ í˜•íƒœ ë™ì¼ì„± í™•ì¸
        assert original_output.shape == enhanced_output.shape
        assert original_output.shape == (batch_size, 2, 224, 224)
        
    def test_severity_head_integration_compatibility(self):
        """SeverityHeadì™€ì˜ í†µí•© í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸"""
        
        
        # Enhanced subnet ìƒì„±
        subnet = DiscriminativeSubNetwork(expose_features=True)
        
        # SeverityHead ìƒì„± (act6 single-scale)
        severity_head = SeverityHead(in_dim=512, mode="single_scale")
        
        batch_size = 4
        input_tensor = torch.randn(batch_size, 6, 224, 224)
        
        # Forward pass
        mask_logits, encoder_features = subnet(input_tensor)
        severity_scores = severity_head(encoder_features['act6'])
        
        # ì¶œë ¥ ê²€ì¦
        assert mask_logits.shape == (batch_size, 2, 224, 224)
        assert severity_scores.shape == (batch_size,)
        assert torch.all((severity_scores >= 0) & (severity_scores <= 1))
        
    def test_multi_scale_severity_head_integration(self):
        """Multi-scale SeverityHeadì™€ì˜ í†µí•© í…ŒìŠ¤íŠ¸"""
        
        # Enhanced subnet ìƒì„±
        subnet = DiscriminativeSubNetwork(expose_features=True)
        
        # Multi-scale SeverityHead ìƒì„±
        severity_head = SeverityHead(mode="multi_scale", base_width=64)
        
        batch_size = 4
        input_tensor = torch.randn(batch_size, 6, 224, 224)
        
        # Forward pass
        mask_logits, encoder_features = subnet(input_tensor)
        
        # Multi-scale features ì¶”ì¶œ (act2~act6)
        multi_scale_features = {
            key: encoder_features[key] 
            for key in ['act2', 'act3', 'act4', 'act5', 'act6']
        }
        
        severity_scores = severity_head(multi_scale_features)
        
        # ì¶œë ¥ ê²€ì¦
        assert mask_logits.shape == (batch_size, 2, 224, 224)
        assert severity_scores.shape == (batch_size,)
        assert torch.all((severity_scores >= 0) & (severity_scores <= 1))


# pytestë¡œ ì‹¤í–‰ ì‹œ ìë™ìœ¼ë¡œ ì‹¤í–‰ë˜ëŠ” í†µí•© í…ŒìŠ¤íŠ¸
def test_discriminative_subnet_integration_summary():
    """ì „ì²´ DiscriminativeSubNetwork ìˆ˜ì • í…ŒìŠ¤íŠ¸ ìš”ì•½"""
    verbose_print("ğŸ§ª Enhanced DiscriminativeSubNetwork Test Suite Integration Summary", "INFO")
    verbose_print("=" * 70)
    
    # í…ŒìŠ¤íŠ¸ êµ¬ì„± ìš”ì†Œ í™•ì¸
    test_components = [
        "Backward compatibility mode (expose_features=False)",
        "Feature exposure mode (expose_features=True)", 
        "Default behavior validation",
        "Different input sizes support",
        "Custom base width configuration",
        "Custom output channels configuration",
        "Gradient flow verification",
        "Reproducibility testing",
        "Memory efficiency validation",
        "Original DRAEM compatibility",
        "SeverityHead single-scale integration",
        "SeverityHead multi-scale integration"
    ]
    
    verbose_print("Test components covered:")
    for i, component in enumerate(test_components, 1):
        verbose_print(f"  {i:2d}. {component}")
    
    verbose_print(f"\nğŸ¯ Total {len(test_components)} test categories covered!", "SUCCESS")
    verbose_print("\nRun individual tests with: pytest tests/unit/models/image/draem_sevnet/test_discriminative_subnet_modification.py::TestDiscriminativeSubNetworkEnhancement::test_<method_name> -v -s")


if __name__ == "__main__":
    # ì§ì ‘ ì‹¤í–‰ ì‹œì—ëŠ” pytest ì‹¤í–‰ì„ ê¶Œì¥
    print("\nğŸ§ª Enhanced DiscriminativeSubNetwork Test Suite")
    print("=" * 60)
    print("To run tests with verbose output:")
    print("pytest tests/unit/models/image/draem_sevnet/test_discriminative_subnet_modification.py -v -s")
    print("\nTo run specific test class:")
    print("pytest tests/unit/models/image/draem_sevnet/test_discriminative_subnet_modification.py::TestDiscriminativeSubNetworkEnhancement -v -s")
    print("\nTo run specific test method:")
    print("pytest tests/unit/models/image/draem_sevnet/test_discriminative_subnet_modification.py::TestDiscriminativeSubNetworkEnhancement::test_feature_exposure_mode -v -s")
    print("\n" + "=" * 60)
