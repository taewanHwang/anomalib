#!/usr/bin/env python3
"""Target Domain Early Stopping Test for Custom DRAEM.

PyTorch Lightningì˜ EarlyStopping callbackì„ í™œìš©í•œ val_image_AUROC ê¸°ë°˜ 
early stopping ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•˜ëŠ” ëª¨ë“ˆì…ë‹ˆë‹¤.

ì£¼ìš” íŠ¹ì§•:
- PyTorch Lightning EarlyStopping callback í™œìš©
- Source Domain validation AUROC ëª¨ë‹ˆí„°ë§ (val_image_AUROC)
- íŒŒë¼ë¯¸í„°í™”ëœ early stopping ì„¤ì • (monitor, patience, min_delta)
- Multi-domain evaluation with automatic early stopping

Early Stopping ì„¤ì •:
- monitor: "val_image_AUROC" (source validation AUROC)
- patience: 2 (2 epochs ë™ì•ˆ ê°œì„  ì—†ìœ¼ë©´ ì¤‘ë‹¨)
- min_delta: 0.005 (ìµœì†Œ 0.5% ê°œì„  í•„ìš”)
- mode: "max" (AUROC ìµœëŒ€í™”)

ì‹¤í—˜ êµ¬ì¡°:
1. Source Domain (domain_A)ì—ì„œ ëª¨ë¸ í›ˆë ¨
2. ë§¤ validation epochë§ˆë‹¤ source validation AUROC ê³„ì‚°
3. ì„±ëŠ¥ ê°œì„ ì´ ì—†ìœ¼ë©´ early stopping ì‹¤í–‰
4. Best checkpointë¡œ final test evaluation

Run with: pytest tests/unit/models/image/custom_draem/test_early_stopping.py -v -s
"""

import os
import torch
import gc
import json
import warnings
from pathlib import Path
from datetime import datetime
from typing import Dict, Any
import logging

# Suppress warnings for cleaner test output
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=UserWarning, module="torchmetrics")
warnings.filterwarnings("ignore", category=UserWarning, module="lightning")
warnings.filterwarnings("ignore", message=".*Importing from timm.models.layers.*")
warnings.filterwarnings("ignore", message=".*multi-threaded.*fork.*")

# Lightning imports
from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint

# Anomalib imports
from anomalib.data.datamodules.image.multi_domain_hdmap import MultiDomainHDMAPDataModule
from anomalib.models.image.custom_draem import CustomDraem
from anomalib.engine import Engine
from anomalib.loggers import AnomalibTensorBoardLogger

# ê²½ê³  ë©”ì‹œì§€ ë¹„í™œì„±í™”
logging.getLogger("anomalib.visualization.image.item_visualizer").setLevel(logging.ERROR)
logging.getLogger("anomalib.visualization").setLevel(logging.ERROR)
logging.getLogger("anomalib.callbacks").setLevel(logging.ERROR)

# GPU ì„¤ì •
os.environ["CUDA_VISIBLE_DEVICES"] = "8"


def cleanup_gpu_memory():
    """GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ë° ìƒíƒœ ì¶œë ¥."""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        gc.collect()
        print(f"GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")


def run_custom_draem_with_early_stopping(
    source_domain: str = "domain_A",
    target_domains: str = "auto",
    max_epochs: int = 20,  # ë” ë§ì€ epochsë¡œ early stopping í™•ë¥  ì¦ê°€
    # Early Stopping íŒŒë¼ë¯¸í„°ë“¤  
    monitor: str = "val_image_AUROC",  # source domain validation AUROC
    patience: int = 2,  # ë” ë¹ ë¥¸ early stopping
    min_delta: float = 0.005,  # AUROCëŠ” 0-1 ë²”ìœ„ì´ë¯€ë¡œ ì‘ì€ ë³€í™”
    mode: str = "max",  # AUROCëŠ” ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ
    # DRAEM-SevNet ëª¨ë¸ íŒŒë¼ë¯¸í„°ë“¤
    severity_head_mode: str = "single_scale",
    score_combination: str = "simple_average", 
    severity_loss_type: str = "mse",
    # í•™ìŠµ íŒŒë¼ë¯¸í„°ë“¤
    learning_rate: float = 0.0001,
    batch_size: int = 16,
) -> Dict[str, Any]:
    """DRAEM-SevNetì— Early Stoppingì„ ì ìš©í•œ í•™ìŠµ ì‹¤í–‰.
    
    Args:
        source_domain: í›ˆë ¨ì— ì‚¬ìš©í•  source domain
        target_domains: í‰ê°€í•  target domains ("auto"ì´ë©´ ìë™ ì„¤ì •)
        max_epochs: ìµœëŒ€ í•™ìŠµ epochs
        monitor: Early stopping ëª¨ë‹ˆí„°ë§ ì§€í‘œ
        patience: Early stopping patience
        min_delta: Early stopping ìµœì†Œ ê°œì„ ê°’
        mode: Early stopping ëª¨ë“œ ("max" ë˜ëŠ” "min")
        severity_head_mode: SeverityHead ëª¨ë“œ ("single_scale" ë˜ëŠ” "multi_scale")
        score_combination: Score ê²°í•© ë°©ì‹ ("simple_average", "weighted_average", "maximum")
        severity_loss_type: Severity loss íƒ€ì… ("mse" ë˜ëŠ” "smooth_l1")
        learning_rate: í•™ìŠµë¥ 
        batch_size: ë°°ì¹˜ í¬ê¸°
        
    Returns:
        Dict[str, Any]: ì‹¤í—˜ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    print("ğŸš€ Custom DRAEM with Target Domain Early Stopping ì‹œì‘")
    print(f"ğŸ“Š Early Stopping ì„¤ì •: monitor={monitor} (source validation), patience={patience}, min_delta={min_delta}, mode={mode}")
    
    # ì‹¤í—˜ ì„¤ì • ì¶œë ¥
    config_info = {
        "source_domain": source_domain,
        "target_domains": target_domains,
        "max_epochs": max_epochs,
        "early_stopping": {
            "monitor": monitor,
            "patience": patience,
            "min_delta": min_delta,
            "mode": mode,
            "strategy": "source_domain_validation"
        },
        "model_config": {
            "severity_head_mode": severity_head_mode,
            "score_combination": score_combination,
            "severity_loss_type": severity_loss_type
        },
        "training_config": {
            "learning_rate": learning_rate,
            "batch_size": batch_size
        }
    }
    
    print("ğŸ“‹ ì‹¤í—˜ ì„¤ì •:")
    print(json.dumps(config_info, indent=2, ensure_ascii=False))
    
    # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
    cleanup_gpu_memory()
    
    try:
        # 1. DataModule ì„¤ì •
        print(f"\nğŸ“‚ DataModule ì„¤ì • (Source: {source_domain}, Targets: {target_domains})")
        datamodule = MultiDomainHDMAPDataModule(
            root="./datasets/HDMAP/1000_8bit_resize_pad_256x256",
            source_domain=source_domain,
            target_domains=target_domains,
            train_batch_size=batch_size,
            eval_batch_size=batch_size,
            num_workers=8,
        )
        
        # DataModule ì¤€ë¹„
        datamodule.prepare_data()
        datamodule.setup()
        
        print(f"âœ… Source Domain: {datamodule.source_domain}")
        print(f"âœ… Target Domains: {datamodule.target_domains}")
        
        # 2. ëª¨ë¸ ìƒì„±
        print(f"\nğŸ¤– DRAEM-SevNet ëª¨ë¸ ìƒì„±")
        model = CustomDraem(
            severity_head_mode="single_scale",  # DRAEM-SevNet íŒŒë¼ë¯¸í„°
            score_combination="simple_average",
            severity_loss_type="mse",
            learning_rate=learning_rate,
        )
        
        print(f"âœ… Severity Head Mode: single_scale")
        print(f"âœ… Score Combination: simple_average")
        print(f"âœ… Severity Loss Type: mse")
        
        # 3. Callbacks ì„¤ì •
        print(f"\nğŸ“‹ Callbacks ì„¤ì •")
        
        # Early Stopping Callback
        early_stopping = EarlyStopping(
            monitor=monitor,
            patience=patience,
            min_delta=min_delta,
            mode=mode,
            verbose=True,
            strict=True,  # ğŸ”§ ë””ë²„ê¹…: ì§€í‘œê°€ ì—†ìœ¼ë©´ ì¦‰ì‹œ ì˜¤ë¥˜ ë°œìƒí•˜ì—¬ ë¬¸ì œ í™•ì¸
        )
        
        # Model Checkpoint Callback
        checkpoint = ModelCheckpoint(
            monitor=monitor,
            mode=mode,
            save_top_k=1,
            save_last=True,
            filename=f"custom_draem_early_stop_{source_domain}_" + "{epoch:02d}_{target_avg_auroc:.4f}",
        )
        
        callbacks = [early_stopping, checkpoint]
        
        print(f"âœ… EarlyStopping: monitor={monitor}, patience={patience}, min_delta={min_delta}")
        print(f"âœ… ModelCheckpoint: monitor={monitor}, mode={mode}")
        
        # 4. Logger ì„¤ì •
        experiment_name = f"custom_draem_early_stopping_{source_domain}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        logger = AnomalibTensorBoardLogger(
            save_dir="logs/hdmap_early_stopping",
            name=experiment_name,
        )
        
        # 5. Engine ì„¤ì • ë° í•™ìŠµ
        print(f"\nğŸš‚ Engine ì„¤ì • ë° í•™ìŠµ ì‹œì‘")
        engine = Engine(
            callbacks=callbacks,
            logger=logger,
            max_epochs=max_epochs,
            accelerator="auto",
            devices=1,
            deterministic=False,  # ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´
            enable_checkpointing=True,
            enable_model_summary=True,
            enable_progress_bar=True,
        )
        
        print(f"âœ… Max Epochs: {max_epochs}")
        print(f"âœ… Logger: {experiment_name}")
        
        # í•™ìŠµ ì‹¤í–‰
        print(f"\nğŸ”¥ í•™ìŠµ ì‹œì‘!")
        start_time = datetime.now()
        
        engine.fit(model=model, datamodule=datamodule)
        
        end_time = datetime.now()
        training_time = (end_time - start_time).total_seconds()
        
        # Early stopping ì •ë³´ ìˆ˜ì§‘
        actual_epochs = engine.trainer.current_epoch + 1
        stopped_early = engine.trainer.current_epoch < max_epochs - 1
        best_score = early_stopping.best_score.item() if early_stopping.best_score is not None else None
        
        print(f"\nâœ… í•™ìŠµ ì™„ë£Œ!")
        print(f"ğŸ“Š ì´ í•™ìŠµ Epochs: {actual_epochs}/{max_epochs}")
        print(f"â° í•™ìŠµ ì‹œê°„: {training_time:.2f}ì´ˆ")
        
        # í•µì‹¬ Early Stopping ì •ë³´ (ë¡œê·¸ ë¶„ì„ìš©)
        if stopped_early:
            print(f"ğŸ›‘ Early Stopping ì ìš©: patience={patience}ì—ì„œ ì¤‘ë‹¨ë¨ (Best {monitor}: {best_score:.4f})")
        else:
            print(f"âœ… ì •ìƒ ì™„ë£Œ: ìµœëŒ€ epochsê¹Œì§€ í•™ìŠµ (Final {monitor}: {best_score:.4f})")
        
        # 6. ìµœì¢… í‰ê°€ (Best checkpoint ì‚¬ìš©)
        print(f"\nğŸ“Š ìµœì¢… ì„±ëŠ¥ í‰ê°€")
        if checkpoint.best_model_path:
            print(f"ğŸ“‚ Best checkpoint ì‚¬ìš©: {checkpoint.best_model_path}")
            test_results = engine.test(datamodule=datamodule, ckpt_path=checkpoint.best_model_path)
        else:
            print(f"âš ï¸ Best checkpoint ì—†ìŒ, í˜„ì¬ ëª¨ë¸ ì‚¬ìš©")
            test_results = engine.test(model=model, datamodule=datamodule)
        
        # ê²°ê³¼ ì •ë¦¬
        results = {
            "experiment_config": config_info,
            "training_info": {
                "actual_epochs": actual_epochs,
                "max_epochs": max_epochs,
                "stopped_early": stopped_early,
                "training_time_seconds": training_time,
                "best_score": best_score,
                "monitor_metric": monitor
            },
            "final_results": test_results,
            "model_path": str(checkpoint.best_model_path) if checkpoint.best_model_path else None,
            "log_dir": str(logger.log_dir)
        }
        
        # ê²°ê³¼ ì €ì¥
        results_file = f"results/early_stopping_{experiment_name}.json"
        os.makedirs("results", exist_ok=True)
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“ ê²°ê³¼ ì €ì¥: {results_file}")
        print(f"ğŸ“ ëª¨ë¸ ì €ì¥: {checkpoint.best_model_path}")
        print(f"ğŸ“ ë¡œê·¸ ì €ì¥: {logger.log_dir}")
        
        return results
        
    except Exception as e:
        print(f"\nâŒ ì‹¤í—˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return {"error": str(e)}
    
    finally:
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        cleanup_gpu_memory()


def run_early_stopping_ablation_study():
    """Early Stopping ì„¤ì •ì— ë”°ë¥¸ ablation study ì‹¤í–‰."""
    print("ğŸ”¬ Early Stopping Ablation Study ì‹œì‘")
    
    # ë‹¤ì–‘í•œ early stopping ì„¤ì •
    early_stopping_configs = [
        # ê¸°ë³¸ ì„¤ì • (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©)
        {"patience": 2, "min_delta": 0.01, "name": "default"},
        # ë” ê´€ëŒ€í•œ ì„¤ì • (ì˜¤ë˜ ê¸°ë‹¤ë¦¼) - ì£¼ì„ì²˜ë¦¬í•˜ë©´ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥
        # {"patience": 5, "min_delta": 0.003, "name": "patient"},
        # ë” ì—„ê²©í•œ ì„¤ì • (ë¹¨ë¦¬ ì¤‘ë‹¨) - ì£¼ì„ì²˜ë¦¬í•˜ë©´ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥
        # {"patience": 2, "min_delta": 0.01, "name": "strict"},
        # ë§¤ìš° ì—„ê²©í•œ ì„¤ì • - ì£¼ì„ì²˜ë¦¬í•˜ë©´ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥
        # {"patience": 1, "min_delta": 0.015, "name": "very_strict"},
    ]
    
    all_results = {}
    
    for config in early_stopping_configs:
        print(f"\nğŸ§ª ì‹¤í—˜: {config['name']} (patience={config['patience']}, min_delta={config['min_delta']})")
        
        results = run_custom_draem_with_early_stopping(
            source_domain="domain_A",
            target_domains="auto",
            max_epochs=8,  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ë‹¨ì¶•
            patience=config["patience"],
            min_delta=config["min_delta"],
            batch_size=16,
        )
        
        all_results[config["name"]] = results
        
        # ì¤‘ê°„ ê²°ê³¼ ì¶œë ¥
        if "training_info" in results:
            training_info = results["training_info"]
            print(f"ê²°ê³¼: {training_info['actual_epochs']}epochs, "
                  f"early_stop={training_info['stopped_early']}, "
                  f"best_score={training_info['best_score']:.4f}")
    
    # ì „ì²´ ê²°ê³¼ ì €ì¥
    ablation_results_file = f"results/early_stopping_ablation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(ablation_results_file, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ“Š Ablation Study ì™„ë£Œ!")
    print(f"ğŸ“ ì „ì²´ ê²°ê³¼ ì €ì¥: {ablation_results_file}")
    
    return all_results


def test_target_domain_early_stopping():
    """Target Domain Early Stopping ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸."""
    print("=" * 80)
    print("Target Domain Early Stopping Test")
    print("=" * 80)
    
    # ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    results = run_custom_draem_with_early_stopping(
        source_domain="domain_A",
        target_domains="auto",
        max_epochs=10,  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ë‹¨ì¶•  # ë” ë§ì€ epochsë¡œ early stopping í™•ë¥  ì¦ê°€
        monitor="val_image_AUROC",  # source domain validation AUROC
        patience=2,  # ë” ë¹ ë¥¸ early stopping
        min_delta=0.005,
        mode="max",  # AUROCëŠ” ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ
        batch_size=16,
    )
    
    # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ê²€ì¦
    assert "training_info" in results, "Training info missing in results"
    
    # Early stoppingì€ ì‘ë™í•  ìˆ˜ë„, ì•ˆ í•  ìˆ˜ë„ ìˆìŒ (ë°ì´í„°ì™€ í•™ìŠµì— ë”°ë¼)
    training_info = results["training_info"]
    if training_info["stopped_early"]:
        print(f"âœ… Early stopping ì‘ë™: {training_info['actual_epochs']}/{training_info['max_epochs']} epochs")
        assert training_info["actual_epochs"] < training_info["max_epochs"], "Should stop before max epochs"
    else:
        print(f"âœ… ì •ìƒ ì™„ë£Œ: {training_info['actual_epochs']}/{training_info['max_epochs']} epochs")
        assert training_info["actual_epochs"] == training_info["max_epochs"], "Should complete all epochs"
    
    print("\nâœ… Target Domain Early Stopping í…ŒìŠ¤íŠ¸ í†µê³¼!")
    # Note: Results validated and saved to file


def test_early_stopping_ablation_study():
    """Early Stopping ì„¤ì •ì— ë”°ë¥¸ ablation study í…ŒìŠ¤íŠ¸."""
    print("\n" + "=" * 80)
    print("Early Stopping Ablation Study Test")
    print("=" * 80)
    
    results = run_early_stopping_ablation_study()
    
    # ëª¨ë“  ì„¤ì •ì—ì„œ ê²°ê³¼ê°€ ìˆëŠ”ì§€ í™•ì¸
    assert len(results) == 4, "Should have 4 different configurations"
    for config_name, result in results.items():
        assert "training_info" in result, f"Training info missing for {config_name}"
    
    print("\nâœ… Early Stopping Ablation Study í…ŒìŠ¤íŠ¸ í†µê³¼!")
    # Note: Results validated and saved to file


# pytestë¡œ ì‹¤í–‰ ì‹œ ìë™ìœ¼ë¡œ ì‹¤í–‰ë˜ëŠ” í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ë“¤
def test_early_stopping_functionality():
    """Early stopping ê¸°ëŠ¥ í†µí•© í…ŒìŠ¤íŠ¸"""
    print("\nğŸ§ª Early Stopping Test Suite")
    print("=" * 50)
    print("Testing PyTorch Lightning EarlyStopping callback integration...")
    
    # ê¸°ë³¸ early stopping í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    result = test_target_domain_early_stopping()
    
    # ê²°ê³¼ ê²€ì¦
    assert result is not None, "Early stopping test should return results"
    assert "training_info" in result, "Training info should be available"
    assert "final_results" in result, "Final results should be available"
    
    print("\nâœ… All early stopping tests passed!")
    # Note: Results validated through assertions above


if __name__ == "__main__":
    print("\nğŸ§ª Early Stopping Test Suite")
    print("=" * 50)
    print("To run as pytest:")
    print("pytest tests/unit/models/image/custom_draem/test_early_stopping.py -v -s")
    print("\nRunning direct execution...")
    test_target_domain_early_stopping()
