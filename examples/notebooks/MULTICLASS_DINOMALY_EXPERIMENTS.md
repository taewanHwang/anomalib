# Dinomaly Multi-Class Experiments for HDMAP Dataset

## Background

Dinomaly (ECCV 2025)ëŠ” Multi-Class Anomaly Detectionì„ ì§€í–¥í•˜ëŠ” ìµœì‹  ì•„í‚¤í…ì²˜ì…ë‹ˆë‹¤.
MVTec ADì˜ 15ê°œ í´ë˜ìŠ¤ë¥¼ í•˜ë‚˜ì˜ ëª¨ë¸ë¡œ í•™ìŠµí•˜ëŠ” **Unified Training** ë°©ì‹ì„ ì œì•ˆí–ˆìŠµë‹ˆë‹¤.

### Original Paper vs Anomalib Implementation ë¶„ì„

| êµ¬ì„±ìš”ì†Œ | ì›ë³¸ (guojiajeremy/Dinomaly) | Anomalib í˜„ì¬ êµ¬í˜„ | ìƒíƒœ |
|---------|----------------------------|-------------------|------|
| Encoder | DINOv2-reg frozen | ë™ì¼ | OK |
| Bottleneck Dropout | **ê³ ì • 0.2** | ê³ ì • 0.2 | OK |
| Discarding Rate (k%) | 0%â†’90% warmup (1000 steps) | ë™ì¼ | OK |
| Hard Mining Factor | 0.1 | ë™ì¼ | OK |
| Target Layers | [2,3,4,5,6,7,8,9] | ë™ì¼ | OK |
| Fuse Layers | [[0,1,2,3], [4,5,6,7]] | ë™ì¼ | OK |


## HDMAP Dataset Structure

```
datasets/HDMAP/1000_tiff_minmax/
â”œâ”€â”€ domain_A/
â”‚   â”œâ”€â”€ train/good/
â”‚   â””â”€â”€ test/{good,fault}/
â”œâ”€â”€ domain_B/
â”œâ”€â”€ domain_C/      # ê°€ì¥ ì–´ë ¤ìš´ ë„ë©”ì¸ (horizontal line defects)
â””â”€â”€ domain_D/
```

## Experiment Environment

- **GPU**: ìµœëŒ€ 16ê°œ ë™ì‹œ ì‹¤í—˜ ê°€ëŠ¥
- **ì‹¤í–‰ ë°©ì‹**: nohup ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ ê¶Œì¥
- **ë¡œê·¸ ê²½ë¡œ**: `/mnt/ex-disk/taewan.hwang/study/anomalib/logs/`

### âš ï¸ ì£¼ì˜ì‚¬í•­: TIFF ì´ë¯¸ì§€ ë¡œë”© (Updated 2024-12-24)

**ë°ì´í„° íŠ¹ì„±**:
- HDMAP TIFF íŒŒì¼: 32-bit float (mode "F")
- **ê°’ ë²”ìœ„**: 0 ~ 2.94 (1 ì´ˆê³¼ ê°’ ì¡´ì¬, íŠ¹íˆ anomaly ìƒ˜í”Œì—ì„œ)
- 9.3% íŒŒì¼(1116ê°œ)ì´ max > 1 ê°’ì„ ê°€ì§

**ë¬¸ì œ**: PILì˜ `img.convert("RGB")`ë¥¼ ì§ì ‘ ì‚¬ìš©í•˜ë©´ float ê°’ì´ 0ìœ¼ë¡œ ì˜ë ¤ **ëª¨ë“  ì´ë¯¸ì§€ê°€ ê²€ì€ìƒ‰**ì´ ë©ë‹ˆë‹¤.

**Train-Test ì¼ê´€ì„± ìš”êµ¬ì‚¬í•­**:
| ë‹¨ê³„ | ì²˜ë¦¬ ë°©ì‹ | ë¹„ê³  |
|------|----------|------|
| í•™ìŠµ (anomalib) | `read_image()` â†’ float32 ê·¸ëŒ€ë¡œ | clipping ì—†ìŒ |
| ì¶”ë¡  | ë™ì¼í•˜ê²Œ float32 ìœ ì§€ í•„ìš” | clipping ì‹œ ì„±ëŠ¥ ì €í•˜ |

**ì˜¬ë°”ë¥¸ ë¡œë”© ì½”ë“œ** (v2 - í•™ìŠµê³¼ ì¼ì¹˜):
```python
if img.mode == "F":
    arr = np.array(img, dtype=np.float32)  # NO clipping!
    if len(arr.shape) == 2:
        arr = np.stack([arr, arr, arr], axis=-1)
    image = torch.from_numpy(arr).permute(2, 0, 1)  # HWC â†’ CHW
```

**ì˜ëª»ëœ ë¡œë”© ì½”ë“œ** (v1 - ì‚¬ìš©í•˜ì§€ ë§ ê²ƒ):
```python
arr = np.clip(arr, 0, 1) * 255  # âŒ 1 ì´ˆê³¼ ê°’ ì†ì‹¤
```

### ğŸ“‹ ë°ì´í„° ë¡œë”© ì²´í¬ë¦¬ìŠ¤íŠ¸

ì‹¤í—˜ ì „ ë°˜ë“œì‹œ í™•ì¸:

- [ ] **ê°’ ë²”ìœ„ í™•ì¸**: ë¡œë”© í›„ `image.max()` > 1 ì¸ì§€ í™•ì¸ (anomaly ìƒ˜í”Œì—ì„œ)
- [ ] **Train-Test ì¼ì¹˜**: í•™ìŠµê³¼ ì¶”ë¡ ì—ì„œ ë™ì¼í•œ ì „ì²˜ë¦¬ ì‚¬ìš©
- [ ] **Transform í™•ì¸**: `transforms.v2` ì‚¬ìš© (tensor ì§ì ‘ ì²˜ë¦¬)
- [ ] **ToTensor ë¶ˆí•„ìš”**: ì´ë¯¸ tensorì´ë©´ ToTensor() ì œê±°
- [ ] **Normalize ìˆœì„œ**: Resize â†’ CenterCrop â†’ Normalize

**ë””ë²„ê·¸ ìŠ¤í¬ë¦½íŠ¸**:
```python
# ë¡œë”© í›„ ê°’ ë²”ìœ„ í™•ì¸
sample = dataset[0]
print(f"shape={sample['image'].shape}, min={sample['image'].min():.4f}, max={sample['image'].max():.4f}")
# ì˜ˆìƒ: max > 1 (ì •ìƒ) ë˜ëŠ” max â‰ˆ 2.0+ (anomaly ìƒ˜í”Œ)
```

> **Note**: 2024-12-24 ì´ì „ ì‹¤í—˜ì˜ AUROC ìˆ˜ì¹˜(50%)ëŠ” ì´ ë²„ê·¸ë¡œ ì¸í•´ ë¬´íš¨ì…ë‹ˆë‹¤. ì²´í¬í¬ì¸íŠ¸ ì¬í‰ê°€ í•„ìš”.

### ì´ˆê¸° ì„¤ì •
```bash
# ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p /mnt/ex-disk/taewan.hwang/study/anomalib/logs

# TensorBoard ì‹¤í–‰ (ë°±ê·¸ë¼ìš´ë“œ)
nohup tensorboard --logdir=results/dinomaly_multiclass_baseline --port=6006 --bind_all \
    > /mnt/ex-disk/taewan.hwang/study/anomalib/logs/tensorboard.log 2>&1 &

# TensorBoard ì ‘ì†: http://<server-ip>:6006
```

## Experiments

### 1. Multi-Class vs Single-Class Baseline

**ëª©í‘œ**: Dinomalyì˜ multi-class ëŠ¥ë ¥ì„ HDMAPì—ì„œ ê²€ì¦

```bash
# Multi-class unified training
nohup python examples/notebooks/dinomaly_multiclass_baseline.py \
    --mode compare \
    --max-steps 3000 \
    --batch-size 16 \
    --encoder dinov2reg_vit_base_14 \
    --seed 42 \
    --gpu 0 \
    > /mnt/ex-disk/taewan.hwang/study/anomalib/logs/multiclass_compare_gpu0.log 2>&1 &

# Multi-class only
nohup python examples/notebooks/dinomaly_multiclass_baseline.py \
    --mode multiclass \
    --max-steps 3000 \
    --seed 42 \
    --gpu 1 \
    > /mnt/ex-disk/taewan.hwang/study/anomalib/logs/multiclass_only_gpu1.log 2>&1 &

# Single-class only
nohup python examples/notebooks/dinomaly_multiclass_baseline.py \
    --mode singleclass \
    --max-steps 3000 \
    --seed 49 \
    --gpu 9 \
    > /mnt/ex-disk/taewan.hwang/study/anomalib/logs/singleclass_only_gpu2_repeat2.log 2>&1 &
```

### 2. Encoder Size Comparison (ë³‘ë ¬ ì‹¤í–‰)

3ê°œ ì¸ì½”ë”ë¥¼ ë™ì‹œì— 3ê°œ GPUì—ì„œ ì‹¤í–‰:

```bash
# Small (GPU 3)
nohup python examples/notebooks/dinomaly_multiclass_baseline.py \
    --mode multiclass \
    --encoder dinov2reg_vit_small_14 \
    --max-steps 3000 \
    --seed 42 \
    --gpu 3 \
    > /mnt/ex-disk/taewan.hwang/study/anomalib/logs/encoder_small_gpu3.log 2>&1 &

# Base (GPU 4)
nohup python examples/notebooks/dinomaly_multiclass_baseline.py \
    --mode multiclass \
    --encoder dinov2reg_vit_base_14 \
    --max-steps 3000 \
    --seed 42 \
    --gpu 4 \
    > /mnt/ex-disk/taewan.hwang/study/anomalib/logs/encoder_base_gpu4.log 2>&1 &

# Large (GPU 5)
nohup python examples/notebooks/dinomaly_multiclass_baseline.py \
    --mode multiclass \
    --encoder dinov2reg_vit_large_14 \
    --max-steps 3000 \
    --seed 42 \
    --gpu 5 \
    > /mnt/ex-disk/taewan.hwang/study/anomalib/logs/encoder_large_gpu5.log 2>&1 &
```

### 3. Training Steps Analysis (ë³‘ë ¬ ì‹¤í–‰)

```bash
# Quick test - 5000 steps (GPU 0)
nohup python examples/notebooks/dinomaly_multiclass_baseline.py \
    --mode compare \
    --max-steps 5000 \
    --seed 42 \
    --gpu 0 \
    > /mnt/ex-disk/taewan.hwang/study/anomalib/logs/steps_5k_gpu0.log 2>&1 &

# Full training - 10000 steps (GPU 1)
nohup python examples/notebooks/dinomaly_multiclass_baseline.py \
    --mode compare \
    --max-steps 10000 \
    --seed 42 \
    --gpu 1 \
    > /mnt/ex-disk/taewan.hwang/study/anomalib/logs/steps_10k_gpu1.log 2>&1 &

# Extended training - 15000 steps (GPU 2)
nohup python examples/notebooks/dinomaly_multiclass_baseline.py \
    --mode compare \
    --max-steps 15000 \
    --seed 42 \
    --gpu 2 \
    > /mnt/ex-disk/taewan.hwang/study/anomalib/logs/steps_15k_gpu2.log 2>&1 &
```

### 4. Full Grid Search (16 GPU ë³‘ë ¬ ì‹¤í–‰)

ëª¨ë“  ì¡°í•©ì„ í•œë²ˆì— ì‹¤í–‰:

```bash
#!/bin/bash
# full_grid_search.sh

LOG_DIR="/mnt/ex-disk/taewan.hwang/study/anomalib/logs"
SCRIPT="examples/notebooks/dinomaly_multiclass_baseline.py"
SEED=42  # ë°˜ë³µ ì‹¤í—˜ì‹œ ë³€ê²½ (ì˜ˆ: 42, 123, 456, 789, 1024)
mkdir -p $LOG_DIR

GPU=0
for ENCODER in small base large; do
    for STEPS in 5000 10000 15000; do
        for MODE in multiclass singleclass; do
            EXP_NAME="${MODE}_${ENCODER}_${STEPS}steps_seed${SEED}_gpu${GPU}"
            echo "Starting: $EXP_NAME"

            nohup python $SCRIPT \
                --mode $MODE \
                --encoder dinov2reg_vit_${ENCODER}_14 \
                --max-steps $STEPS \
                --seed $SEED \
                --gpu $GPU \
                > ${LOG_DIR}/${EXP_NAME}.log 2>&1 &

            GPU=$((GPU + 1))
            if [ $GPU -ge 16 ]; then
                echo "All 16 GPUs in use. Waiting..."
                wait
                GPU=0
            fi
        done
    done
done

echo "All experiments launched!"
```

## Experiment Monitoring

### ì‹¤ì‹œê°„ ë¡œê·¸ í™•ì¸
```bash
# íŠ¹ì • ì‹¤í—˜ ë¡œê·¸ í™•ì¸
tail -f /mnt/ex-disk/taewan.hwang/study/anomalib/logs/multiclass_compare_gpu0.log

# ëª¨ë“  ë¡œê·¸ ë™ì‹œ í™•ì¸
tail -f /mnt/ex-disk/taewan.hwang/study/anomalib/logs/*.log
```

### ì‹¤í—˜ ìƒíƒœ í™•ì¸
```bash
# ì‹¤í–‰ ì¤‘ì¸ ì‹¤í—˜ í™•ì¸
ps aux | grep dinomaly_multiclass_baseline

# GPU ì‚¬ìš©ëŸ‰ í™•ì¸
nvidia-smi

# ì™„ë£Œëœ ì‹¤í—˜ ê²°ê³¼ í™•ì¸
find results/dinomaly_multiclass_baseline -name "results.json" -exec echo "=== {} ===" \; -exec cat {} \;
```

### ë¡œê·¸ì—ì„œ AUROC ì¶”ì¶œ
```bash
# ìµœì¢… AUROC ê²°ê³¼ë§Œ ì¶”ì¶œ
grep -h "Test AUROC" /mnt/ex-disk/taewan.hwang/study/anomalib/logs/*.log

# Per-domain ê²°ê³¼ ì¶”ì¶œ
grep -h "domain_" /mnt/ex-disk/taewan.hwang/study/anomalib/logs/*.log | grep "AUROC"
```

## Expected Results

Based on original paper (MVTec AD, 15 classes):
- Multi-class Unified: ~96% I-AUROC (average)
- Single-class: ~97% I-AUROC (average)
- Gap: ~1% (multi-class slightly lower, but uses 1 model instead of 15)

For HDMAP (4 domains), expected:
- Single-class: Domain-specific performance varies
  - Domain A, B, D: 90-95%
  - Domain C: 70-80% (challenging horizontal line defects)
- Multi-class: Potential for knowledge transfer between domains

## Key Metrics to Track

1. **Per-Domain AUROC**: How well does the unified model perform on each domain?
2. **Overall AUROC**: Combined performance across all test samples
3. **Training Dynamics**: Loss convergence, gradient norms
4. **Domain Transfer**: Does training on all domains help harder domains (C)?

## Post-Analysis

í›ˆë ¨ í›„ ìƒì„¸ ë¶„ì„ì„ ìœ„í•œ ìŠ¤í¬ë¦½íŠ¸:

```bash
# ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë¶„ì„ ì‹¤í–‰
python examples/notebooks/hdmap_post_analysis.py \
    --checkpoint results/dinomaly_multiclass_baseline/{timestamp}/multiclass_unified/checkpoints/best.ckpt \
    --output-dir results/post_analysis/{timestamp} \
    --gpu 0
```

### ìƒì„±ë˜ëŠ” ê²°ê³¼ë¬¼

| íŒŒì¼ | ì„¤ëª… |
|-----|------|
| `roc_curves.png` | ë„ë©”ì¸ë³„ ROC ê³¡ì„  ë¹„êµ |
| `score_distributions.png` | Good/Fault ì ìˆ˜ ë¶„í¬ ë° ìµœì  threshold |
| `confusion_matrices.png` | ë„ë©”ì¸ë³„ í˜¼ë™ í–‰ë ¬ |
| `heatmaps/*.png` | ë„ë©”ì¸ë³„ anomaly heatmap ì˜ˆì‹œ |
| `metrics_summary.json` | AUROC, AUPR, F1-max ë“± ë©”íŠ¸ë¦­ |

### ì£¼ìš” ë¶„ì„ í¬ì¸íŠ¸

1. **Score Distribution**: Goodê³¼ Faultì˜ ë¶„ë¦¬ ì •ë„ í™•ì¸
2. **Domain C ë¶„ì„**: ìˆ˜í‰ì„  ê²°í•¨ì´ ì œëŒ€ë¡œ ê°ì§€ë˜ëŠ”ì§€ heatmap í™•ì¸
3. **False Positive/Negative**: ë†’ì€ ì ìˆ˜ì˜ Good ìƒ˜í”Œ, ë‚®ì€ ì ìˆ˜ì˜ Fault ìƒ˜í”Œ ë¶„ì„

## Output Structure

```
results/dinomaly_multiclass_baseline/
â””â”€â”€ {timestamp}/
    â”œâ”€â”€ experiment_settings.json
    â”œâ”€â”€ multiclass_unified/
    â”‚   â”œâ”€â”€ checkpoints/
    â”‚   â”œâ”€â”€ tensorboard/
    â”‚   â””â”€â”€ results.json
    â”œâ”€â”€ singleclass_{domain}/
    â”‚   â””â”€â”€ results.json
    â””â”€â”€ final_summary.json

results/post_analysis/
â””â”€â”€ {timestamp}/
    â”œâ”€â”€ roc_curves.png
    â”œâ”€â”€ score_distributions.png
    â”œâ”€â”€ confusion_matrices.png
    â”œâ”€â”€ heatmaps/
    â”‚   â”œâ”€â”€ domain_A_heatmaps.png
    â”‚   â”œâ”€â”€ domain_B_heatmaps.png
    â”‚   â”œâ”€â”€ domain_C_heatmaps.png
    â”‚   â””â”€â”€ domain_D_heatmaps.png
    â””â”€â”€ metrics_summary.json
```

---

## Method 1: GeM Auxiliary Loss (Train-Test ì§‘ê³„ ì •ë ¬)

### ë¬¸ì œ ì •ì˜

**Domain C ì„±ëŠ¥ ì €í•˜ ì›ì¸ ë¶„ì„ ê²°ê³¼:**
- í˜„ì¬ DinomalyëŠ” `DEFAULT_MAX_RATIO = 0.01` (top-1% mean) ì§‘ê³„ ì‚¬ìš©
- Domain CëŠ” **diffuse anomaly** (ë„“ê²Œ í¼ì§„ ê²°í•¨) íŠ¹ì„±
- ëª¨ë¸ì´ ê²°í•¨ì„ "ë³¸ë‹¤" (response ratio 62-68%), í•˜ì§€ë§Œ top-1% ì§‘ê³„ì—ì„œ ì‹ í˜¸ ì†ì‹¤
- Inference-onlyë¡œ `max_ratio=0.05`ë¡œ ë³€ê²½ ì‹œ: Domain C TPR 65.3% â†’ 72.0% ê°œì„  í™•ì¸

### í•µì‹¬ ê°€ì„¤

> **Train-Test ì§‘ê³„ ì •ë ¬ ê°€ì„¤**: í•™ìŠµ ì‹œ GeM pooling (p=10) ê¸°ë°˜ auxiliary lossë¥¼ ì¶”ê°€í•˜ë©´,
> decoderê°€ diffuse íŒ¨í„´ì—ì„œë„ ì¼ê´€ë˜ê²Œ ë†’ì€ aggregated scoreë¥¼ ë§Œë“œëŠ” anomaly mapì„ ìƒì„±í•˜ë„ë¡ í•™ìŠµëœë‹¤.

### êµ¬í˜„ ë‚´ìš©

| íŒŒì¼ | ë³€ê²½ ë‚´ìš© |
|------|----------|
| `components/gem_pooling.py` | **NEW** - GeMPooling, LSEPooling, GeMAuxiliaryLoss í´ë˜ìŠ¤ |
| `components/__init__.py` | Export ì¶”ê°€ |
| `torch_model.py` | `use_gem_loss`, `gem_p`, `gem_loss_weight` íŒŒë¼ë¯¸í„° ì¶”ê°€ |
| `lightning_model.py` | íŒŒë¼ë¯¸í„° ì „ë‹¬, loss component ë¡œê¹… |
| `dinomaly_multiclass_baseline.py` | CLI arguments ì¶”ê°€ |

#### GeM Pooling ìˆ˜ì‹

```
GeM(x) = (1/N * Î£ x_i^p)^(1/p)
```

- `p=1`: arithmetic mean (average pooling)
- `pâ†’âˆ`: max pooling
- `p=10` (default): soft-max behavior, ë†’ì€ ê°’ì— ë¯¼ê°í•˜ë©´ì„œ ë…¸ì´ì¦ˆì— ê°•ê±´

#### GeMAuxiliaryLoss

```python
# Training ì‹œ anomaly mapì—ì„œ:
gem_score = GeMPooling(anomaly_map)   # soft-max ì§‘ê³„
topk_score = topk_mean(anomaly_map)   # ê¸°ì¡´ top-k ì§‘ê³„

# Consistency loss: ë‘ ì§‘ê³„ ë°©ì‹ì´ ì¼ê´€ë˜ë„ë¡
loss = MSE(gem_score, topk_score.detach())
```

### CLI Arguments

```bash
--use-gem-loss          # GeM auxiliary loss í™œì„±í™”
--gem-p 10.0            # GeM power parameter (default: 10.0)
--gem-loss-weight 0.1   # Auxiliary loss ê°€ì¤‘ì¹˜ (default: 0.1)
```

### ì‹¤í—˜ ê³„íš

| ì‹¤í—˜ ID | ì„¤ì • | ëª©ì  |
|---------|------|------|
| **E0** | ê¸°ì¡´ Dinomaly (baseline) | ë¹„êµ ê¸°ì¤€ |
| **E1** | `--use-gem-loss --gem-p 10 --gem-loss-weight 0.1` | ê¸°ë³¸ GeM loss |
| **E2** | `--use-gem-loss --gem-p 10 --gem-loss-weight 0.05` | ë‚®ì€ ê°€ì¤‘ì¹˜ |
| **E3** | `--use-gem-loss --gem-p 10 --gem-loss-weight 0.2` | ë†’ì€ ê°€ì¤‘ì¹˜ |

### ì„±ê³µ ê¸°ì¤€

| ì§€í‘œ | Baseline | ëª©í‘œ | ìµœì†Œ |
|------|----------|------|------|
| Domain C TPR@FPR=1% | 65.3% | 72%+ | 70% |
| Domain A TPR@FPR=1% | 91.1% | ìœ ì§€ | >89% |
| Domain B TPR@FPR=1% | 94.2% | ìœ ì§€ | >92% |
| Domain D TPR@FPR=1% | 92.4% | ìœ ì§€ | >90% |

### 5. GeM Loss Experiments (ë³‘ë ¬ ì‹¤í–‰)

```bash
# E0: Baseline (GPU 0)
nohup python examples/notebooks/dinomaly_multiclass_baseline.py \
    --mode multiclass \
    --max-steps 3000 \
    --seed 42 \
    --gpu 0 \
    > /mnt/ex-disk/taewan.hwang/study/anomalib/logs/gem_E0_baseline_gpu0.log 2>&1 &

# E1: GeM loss Î»=0.1 (GPU 1)
nohup python examples/notebooks/dinomaly_multiclass_baseline.py \
    --mode multiclass \
    --max-steps 3000 \
    --seed 42 \
    --use-gem-loss \
    --gem-p 10.0 \
    --gem-loss-weight 0.1 \
    --gpu 1 \
    > /mnt/ex-disk/taewan.hwang/study/anomalib/logs/gem_E1_lambda0.1_gpu1.log 2>&1 &

# E2: GeM loss Î»=0.05 (GPU 2)
nohup python examples/notebooks/dinomaly_multiclass_baseline.py \
    --mode multiclass \
    --max-steps 3000 \
    --seed 42 \
    --use-gem-loss \
    --gem-p 10.0 \
    --gem-loss-weight 0.05 \
    --gpu 2 \
    > /mnt/ex-disk/taewan.hwang/study/anomalib/logs/gem_E2_lambda0.05_gpu2.log 2>&1 &

# E3: GeM loss Î»=0.2 (GPU 3)
nohup python examples/notebooks/dinomaly_multiclass_baseline.py \
    --mode multiclass \
    --max-steps 3000 \
    --seed 42 \
    --use-gem-loss \
    --gem-p 10.0 \
    --gem-loss-weight 0.2 \
    --gpu 3 \
    > /mnt/ex-disk/taewan.hwang/study/anomalib/logs/gem_E3_lambda0.2_gpu3.log 2>&1 &
```

### GeM Loss ì‹¤í—˜ ëª¨ë‹ˆí„°ë§

```bash
# Loss components í™•ì¸ (recon_loss vs gem_loss)
grep -E "(train_loss|train_recon_loss|train_gem_loss)" \
    /mnt/ex-disk/taewan.hwang/study/anomalib/logs/gem_E1_*.log | tail -20

# ì‹¤í—˜ ì™„ë£Œ í™•ì¸
grep "Per-Domain AUROC" /mnt/ex-disk/taewan.hwang/study/anomalib/logs/gem_E*.log

# Domain C ì„±ëŠ¥ ë¹„êµ
grep "domain_C" /mnt/ex-disk/taewan.hwang/study/anomalib/logs/gem_E*.log
```

### GeM Loss Grid Search (8 GPU ë³‘ë ¬)

```bash
#!/bin/bash
# gem_grid_search.sh

LOG_DIR="/mnt/ex-disk/taewan.hwang/study/anomalib/logs"
SCRIPT="examples/notebooks/dinomaly_multiclass_baseline.py"
SEED=42
mkdir -p $LOG_DIR

GPU=0

# Baseline
nohup python $SCRIPT \
    --mode multiclass --max-steps 3000 --seed $SEED --gpu $GPU \
    > ${LOG_DIR}/gem_baseline_gpu${GPU}.log 2>&1 &
GPU=$((GPU + 1))

# GeM loss sweep
for LAMBDA in 0.01 0.05 0.1 0.2 0.5; do
    for P in 5.0 10.0 20.0; do
        EXP_NAME="gem_p${P}_lambda${LAMBDA}_gpu${GPU}"
        echo "Starting: $EXP_NAME"

        nohup python $SCRIPT \
            --mode multiclass \
            --max-steps 3000 \
            --seed $SEED \
            --use-gem-loss \
            --gem-p $P \
            --gem-loss-weight $LAMBDA \
            --gpu $GPU \
            > ${LOG_DIR}/${EXP_NAME}.log 2>&1 &

        GPU=$((GPU + 1))
        if [ $GPU -ge 16 ]; then
            echo "Waiting for GPUs..."
            wait
            GPU=0
        fi
    done
done

echo "All GeM experiments launched!"
```

### ì‹¤í—˜ ê²°ê³¼ (2025-12-22)

**ì‹¤í—˜ í™˜ê²½**: 3íšŒ ë°˜ë³µ (seed=42, 43, 44), max_steps=3000

#### TPR@FPR=1% ê²°ê³¼ (Mean Â± Std)

| Condition | Domain A | Domain B | Domain C | Domain D |
|-----------|----------|----------|----------|----------|
| **E0: Baseline** | 0.935Â±0.000 | 0.949Â±0.000 | **0.788Â±0.003** | 0.934Â±0.001 |
| E1: GeM Î»=0.1 | 0.934Â±0.001 | 0.949Â±0.000 | 0.790Â±0.007 | 0.934Â±0.001 |
| E2: GeM Î»=0.05 | 0.935Â±0.000 | 0.949Â±0.000 | 0.785Â±0.005 | 0.934Â±0.001 |
| E3: GeM Î»=0.2 | 0.935Â±0.000 | 0.949Â±0.000 | 0.789Â±0.001 | 0.934Â±0.000 |

#### Domain C ê°œì„  íš¨ê³¼

| Condition | Domain C TPR@FPR=1% | ë³€í™” (pp) |
|-----------|---------------------|-----------|
| Baseline | 78.8% | - |
| GeM Î»=0.1 | 79.0% | **+0.23** |
| GeM Î»=0.05 | 78.5% | -0.33 |
| GeM Î»=0.2 | 78.9% | +0.07 |

#### ê²°ë¡ : âŒ ê°€ì„¤ ê¸°ê°

**GeM Auxiliary LossëŠ” Domain C TPR@FPR=1% ê°œì„ ì— ìœ ì˜ë¯¸í•œ íš¨ê³¼ê°€ ì—†ìŒ**

- ìµœëŒ€ ê°œì„ : +0.23 pp (Î»=0.1) - í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•˜ì§€ ì•ŠìŒ
- ê¸°ëŒ€ ëª©í‘œ: 65% â†’ 72% (+7 pp) ë‹¬ì„± **ì‹¤íŒ¨**
- ë‹¤ë¥¸ ë„ë©”ì¸ ì„±ëŠ¥: ìœ ì§€ë¨ (ë¶€ì‘ìš© ì—†ìŒ)

#### ì‹¤íŒ¨ ì›ì¸ ë¶„ì„

1. **Auxiliary loss weight ë¶€ì¡±**: Î»=0.1~0.2ë¡œëŠ” decoderì˜ anomaly map ìƒì„±ì— ìœ ì˜ë¯¸í•œ ì˜í–¥ ì—†ìŒ
2. **ì •ìƒ ìƒ˜í”Œë§Œìœ¼ë¡œ í•™ìŠµ**: GeM-TopK consistencyê°€ ì •ìƒ ì˜ì—­ì—ì„œë§Œ ì‘ë™í•˜ì—¬ ê²°í•¨ íƒì§€ì— íš¨ê³¼ ì œí•œì 
3. **Train-Test aggregation gap ë¯¸í•´ì†Œ**: í•™ìŠµ ì‹œ GeM, ì¶”ë¡  ì‹œ top-1% meanìœ¼ë¡œ ì—¬ì „íˆ ë¶ˆì¼ì¹˜

#### ê´€ë ¨ íŒŒì¼

- ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸: `examples/notebooks/gem_loss_comparison_analysis.py`
- TPR ê³„ì‚° ìŠ¤í¬ë¦½íŠ¸: `examples/notebooks/compute_tpr_at_fpr.py`
- ê²°ê³¼ ì €ì¥: `results/dinomaly_multiclass_baseline/gem_analysis/`

#### ë‹¤ìŒ ë‹¨ê³„

Method 1 (GeM Auxiliary Loss)ì€ íš¨ê³¼ ì—†ìŒìœ¼ë¡œ íŒì •. ë‹¤ë¥¸ ë°©ë²• íƒìƒ‰ í•„ìš”:
- **Inference-time MAX_RATIO ë³€ê²½**: 0.01 â†’ 0.05 (ê²€ì¦ë¨: +7% ê°œì„ , ê°€ì¥ ê°„ë‹¨)
- **Method 2-5**: ë‹¤ë¥¸ training-based ì ‘ê·¼ ë°©ì‹

---

## Method 3: Stable Hard Normal Mining

> **ìƒíƒœ**: âŒ ì‹¤í—˜ ì™„ë£Œ - íš¨ê³¼ ì—†ìŒ (2025-12-22)

### 1. ë¬¸ì œ ì •ì˜

#### 1.1 í˜„ìƒ
- **Domain C TPR@FPR=1% = 78.8%**: ë‹¤ë¥¸ ë„ë©”ì¸(93-95%) ëŒ€ë¹„ í˜„ì €íˆ ë‚®ìŒ
- **ê·¼ë³¸ ì›ì¸**: ì •ìƒ ìƒ˜í”Œ ì¤‘ ì¼ë¶€ê°€ ì¼ê´€ë˜ê²Œ ë†’ì€ anomaly scoreë¥¼ ë°›ìŒ ("hard normals")
- **ê²°ê³¼**: low-FPR ìš´ì˜ì ì—ì„œ FP ë°œìƒ â†’ thresholdë¥¼ ë†’ì—¬ì•¼ í•¨ â†’ TPR í•˜ë½

#### 1.2 í•µì‹¬ ê´€ì°°
```
ì •ìƒ ë¶„í¬ tailì— "stable hard normals"ê°€ ì¡´ì¬:
  - ë§¤ epoch ìƒìœ„ 1%ì— ë°˜ë³µ ë“±ì¥ (bootstrap frequency > 50%)
  - ì´ ìƒ˜í”Œë“¤ì´ FPR=1% ìš´ì˜ì ì„ ì§€ë°°
  - ë¼ë²¨ ì—†ì´ë„ í†µê³„ì ìœ¼ë¡œ ì‹ë³„ ê°€ëŠ¥
```

#### 1.3 Method 1 ì‹¤íŒ¨ êµí›ˆ
- GeM auxiliary loss: train-test ì§‘ê³„ ì •ë ¬ ì‹œë„ â†’ íš¨ê³¼ ì—†ìŒ (+0.23 pp)
- **ì›ì¸**: ì •ìƒ ìƒ˜í”Œ ì „ì²´ì— ë™ì¼ ì²˜ë¦¬, hard normal íŠ¹í™” ì²˜ë¦¬ ë¶€ì¬

### 2. í•µì‹¬ ê°€ì„¤

> **Stable Hard Normal Suppression ê°€ì„¤**:
> í•™ìŠµ ì¤‘ ì¼ê´€ë˜ê²Œ ë†’ì€ anomaly scoreë¥¼ ë°›ëŠ” ì •ìƒ ìƒ˜í”Œ(stable hard normals)ì„
> ì‹ë³„í•˜ê³  ì„ íƒì ìœ¼ë¡œ ì–µì œí•˜ë©´, FP tailì´ ì¶•ì†Œë˜ì–´ low-FPR TPRì´ ê°œì„ ëœë‹¤.

#### 2.1 ì´ë¡ ì  ê·¼ê±°
```
Before:  ì •ìƒ ë¶„í¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      ê²°í•¨ ë¶„í¬
                            â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€
         â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚â–ˆâ–ˆâ”€â”€â”€â”€â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
                            â””â”€â”€â”€â”€â”€â”€â”˜
                            â†‘ Hard normalsê°€ threshold ìƒìŠ¹ ìœ ë°œ

After:   ì •ìƒ ë¶„í¬ â”€â”€â”€â”€â”€â”€â”          ê²°í•¨ ë¶„í¬
                        â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€
         â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†‘ Tail ì–µì œ â†’ threshold í•˜ê°• ê°€ëŠ¥ â†’ TPR ìƒìŠ¹
```

#### 2.2 ì˜ˆìƒ íš¨ê³¼
- **Domain C TPR@FPR=1%**: 78.8% â†’ 85%+ (ëª©í‘œ: +7 pp)
- **ë‹¤ë¥¸ ë„ë©”ì¸**: ìœ ì§€ ë˜ëŠ” ì†Œí­ ìƒìŠ¹
- **AUROC**: í° ë³€í™” ì—†ìŒ (ì „ì²´ ë¶„í¬ê°€ ì•„ë‹Œ tailë§Œ ì²˜ë¦¬)

### 3. êµ¬í˜„ ë‚´ìš©

#### 3.1 ì•Œê³ ë¦¬ì¦˜ ê°œìš”

```
[Phase 1: Score Tracking]
ë§¤ training step:
  1. Anomaly score ê³„ì‚° (resize 256 + blur + top-5% mean)
  2. Sampleë³„ score ëˆ„ì 

[Phase 2: Stable Set Identification]
ë§¤ epoch ì¢…ë£Œ ì‹œ:
  1. ìƒìœ„ 1% hard normals ì‹ë³„
  2. EMA frequency ì—…ë°ì´íŠ¸: freq â† 0.9 * freq + 0.1 * is_hard
  3. freq â‰¥ 0.5ì¸ ìƒ˜í”Œ = "stable hard normal"

[Phase 3: Tail Penalty (warmup ì´í›„)]
ë§¤ training step:
  1. Stable hard normalì— ëŒ€í•´ tail penalty ê³„ì‚°
  2. Total loss = recon_loss + Î» * tail_penalty
```

#### 3.2 ì‹ ê·œ íŒŒì¼

| íŒŒì¼ | ì„¤ëª… |
|------|------|
| `components/stable_hard_normal_mining.py` | StableHardNormalMiner í´ë˜ìŠ¤ |

#### 3.3 ìˆ˜ì • íŒŒì¼

| íŒŒì¼ | ë³€ê²½ ë‚´ìš© |
|------|----------|
| `components/__init__.py` | StableHardNormalMiner export ì¶”ê°€ |
| `torch_model.py` | `compute_training_scores()` ë©”ì„œë“œ ì¶”ê°€ |
| `lightning_model.py` | mining íŒŒë¼ë¯¸í„°, training_step ìˆ˜ì •, on_train_epoch_end ì½œë°± |
| `dinomaly_multiclass_baseline.py` | CLI arguments ì—…ë°ì´íŠ¸ |

#### 3.4 ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„°

| íŒŒë¼ë¯¸í„° | ê¸°ë³¸ê°’ | ì„¤ëª… |
|----------|--------|------|
| `use_hard_normal_mining` | False | Mining í™œì„±í™” ì—¬ë¶€ |
| `hard_normal_penalty_weight` | 0.1 | Tail penalty ê°€ì¤‘ì¹˜ (Î») |
| `hard_normal_warmup_epochs` | 5 | Penalty ì ìš© ì „ warmup epochs |
| `hard_normal_ratio` | 0.01 | ìƒìœ„ K% hard normal ë¹„ìœ¨ |
| `hard_normal_stable_threshold` | 0.5 | Stable íŒì • EMA frequency threshold |
| `hard_normal_ema_decay` | 0.9 | EMA decay factor |

### 4. ì‹¤í—˜ ê³„íš

#### 4.1 ì‹¤í—˜ êµ¬ì„±

| ID | ì„¤ì • | ëª©ì  |
|----|------|------|
| **M3-E0** | Baseline (mining ë¹„í™œì„±í™”) | ë¹„êµ ê¸°ì¤€ |
| **M3-E1** | Î»=0.1, warmup=5 | ê¸°ë³¸ ì„¤ì • |
| **M3-E2** | Î»=0.2, warmup=5 | ë†’ì€ penalty |
| **M3-E3** | Î»=0.1, warmup=2 | ë¹ ë¥¸ í™œì„±í™” |
| **M3-E4** | Î»=0.05, warmup=5 | ë‚®ì€ penalty |

#### 4.2 í‰ê°€ ì§€í‘œ

**Primary**:
- TPR@FPR=1% per domain (íŠ¹íˆ Domain C)
- Mean TPR@FPR=1% across all domains

**Secondary**:
- Image-level AUROC
- num_stable_hard_normals (TensorBoardì—ì„œ ëª¨ë‹ˆí„°ë§)
- train_tail_penalty loss curve

#### 4.3 ì„±ê³µ ê¸°ì¤€

| ì§€í‘œ | Baseline | ëª©í‘œ | ìµœì†Œ |
|------|----------|------|------|
| Domain C TPR@FPR=1% | 78.8% | 85%+ | 82% |
| Domain A TPR@FPR=1% | 93.5% | ìœ ì§€ | >92% |
| Domain B TPR@FPR=1% | 94.9% | ìœ ì§€ | >93% |
| Domain D TPR@FPR=1% | 93.4% | ìœ ì§€ | >92% |

### 5. ì‹¤í—˜ ëª…ë ¹ì–´

#### 5.1 ê°œë³„ ì‹¤í—˜

```bash
# ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p logs

# M3-E0: Baseline
nohup python examples/notebooks/dinomaly_multiclass_baseline.py \
    --mode multiclass --max-steps 3000 --seed 42 --gpu 0 \
    > logs/method3_E0_seed42.log 2>&1 &

# M3-E1: Default mining (Î»=0.1, warmup=5)
nohup python examples/notebooks/dinomaly_multiclass_baseline.py \
    --mode multiclass --max-steps 3000 --seed 42 --gpu 1 \
    --use-hard-normal-mining \
    --hard-normal-penalty-weight 0.1 \
    --hard-normal-warmup-epochs 5 \
    > logs/method3_E1_seed42.log 2>&1 &

# M3-E2: Higher penalty (Î»=0.2)
nohup python examples/notebooks/dinomaly_multiclass_baseline.py \
    --mode multiclass --max-steps 3000 --seed 42 --gpu 2 \
    --use-hard-normal-mining \
    --hard-normal-penalty-weight 0.2 \
    > logs/method3_E2_seed42.log 2>&1 &

# M3-E3: Faster warmup (warmup=2)
nohup python examples/notebooks/dinomaly_multiclass_baseline.py \
    --mode multiclass --max-steps 3000 --seed 42 --gpu 3 \
    --use-hard-normal-mining \
    --hard-normal-penalty-weight 0.1 \
    --hard-normal-warmup-epochs 2 \
    > logs/method3_E3_seed42.log 2>&1 &

# M3-E4: Lower penalty (Î»=0.05)
nohup python examples/notebooks/dinomaly_multiclass_baseline.py \
    --mode multiclass --max-steps 3000 --seed 42 --gpu 4 \
    --use-hard-normal-mining \
    --hard-normal-penalty-weight 0.05 \
    > logs/method3_E4_seed42.log 2>&1 &
```

#### 5.2 Grid Search (3íšŒ ë°˜ë³µ)

```bash
#!/bin/bash
LOG_DIR="logs"
SCRIPT="examples/notebooks/dinomaly_multiclass_baseline.py"
mkdir -p $LOG_DIR

GPU=0
SEEDS=(42 43 44)
LAMBDAS=(0.01 0.05 0.1)  # Updated: 0.01 as default, removed 0.2

# Baseline (E0) - 3 seeds
for SEED in "${SEEDS[@]}"; do
    EXP_NAME="method3_E0_seed${SEED}"
    echo "Launching $EXP_NAME on GPU $GPU..."

    nohup python $SCRIPT \
        --mode multiclass \
        --max-steps 3000 \
        --seed $SEED \
        --gpu $GPU \
        > ${LOG_DIR}/${EXP_NAME}.log 2>&1 &

    GPU=$((GPU + 1))
    sleep 3  # Wait 3 seconds between launches
done

# Mining experiments with different lambdas
for LAMBDA in "${LAMBDAS[@]}"; do
    for SEED in "${SEEDS[@]}"; do
        EXP_NAME="method3_mining_lambda${LAMBDA}_seed${SEED}"
        echo "Launching $EXP_NAME on GPU $GPU..."

        nohup python $SCRIPT \
            --mode multiclass \
            --max-steps 3000 \
            --seed $SEED \
            --use-hard-normal-mining \
            --hard-normal-penalty-weight $LAMBDA \
            --hard-normal-warmup-epochs 5 \
            --gpu $GPU \
            > ${LOG_DIR}/${EXP_NAME}.log 2>&1 &

        GPU=$((GPU + 1))
        sleep 3  # Wait 3 seconds between launches

        if [ $GPU -ge 16 ]; then
            echo "Waiting for GPUs..."
            wait
            GPU=0
        fi
    done
done

echo "All Method 3 experiments launched!"
```

#### 5.3 ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§

```bash
# ì‹¤í–‰ ì¤‘ì¸ ì‹¤í—˜ í™•ì¸
ps aux | grep dinomaly_multiclass_baseline | grep -v grep

# íŠ¹ì • ë¡œê·¸ í™•ì¸
tail -f logs/method3_E1_seed42.log

# TensorBoardë¡œ í•™ìŠµ ê³¡ì„  í™•ì¸
tensorboard --logdir=results/dinomaly_multiclass_baseline --port=6007 --bind_all
```

### 6. ê²°ê³¼ ë¶„ì„ ê³„íš

#### 6.1 TPR@FPR ê³„ì‚°
ê¸°ì¡´ `compute_tpr_at_fpr.py` ìŠ¤í¬ë¦½íŠ¸ í™œìš©:
```bash
python examples/notebooks/compute_tpr_at_fpr.py \
    --checkpoint-dir results/dinomaly_multiclass_baseline \
    --output-dir results/dinomaly_multiclass_baseline/method3_analysis
```

#### 6.2 ë¶„ì„ í•­ëª©
1. **Domainë³„ TPR@FPR=1% ë¹„êµ**: Baseline vs Mining ì¡°ê±´ë“¤
2. **Lambda sensitivity**: Î» = 0.05, 0.1, 0.2 íš¨ê³¼ ë¹„êµ
3. **Stable hard normal ë¶„ì„**: ëª‡ ê°œ ìƒ˜í”Œì´ stableë¡œ ì‹ë³„ë˜ì—ˆëŠ”ì§€
4. **Learning curve**: tail_penalty vs epoch ì¶”ì´

### 7. ê´€ë ¨ íŒŒì¼

| íŒŒì¼ | ìœ„ì¹˜ |
|------|------|
| StableHardNormalMiner | `src/anomalib/models/image/dinomaly/components/stable_hard_normal_mining.py` |
| compute_training_scores | `src/anomalib/models/image/dinomaly/torch_model.py:338-385` |
| training_step (mining) | `src/anomalib/models/image/dinomaly/lightning_model.py:300-387` |
| CLI arguments | `examples/notebooks/dinomaly_multiclass_baseline.py:960-995` |

### 8. ì‹¤í—˜ ê²°ê³¼ (2025-12-22)

#### 8.1 ì‹¤í—˜ í™˜ê²½

| í•­ëª© | ê°’ |
|------|-----|
| GPU | NVIDIA A100-SXM4-40GB |
| max_steps | 3000 |
| batch_size | 16 |
| ë°˜ë³µ íšŸìˆ˜ | 3íšŒ (seed=42, 43, 44) |
| encoder | dinov2reg_vit_base_14 |

#### 8.2 ì‹¤í—˜ ì¡°ê±´

| Condition | Mining | penalty_weight (Î») | warmup_epochs | stable_threshold |
|-----------|--------|-------------------|---------------|------------------|
| **Baseline** | OFF | - | - | - |
| **Mining Î»=0.01** | ON | 0.01 | 5 | 0.3 |
| **Mining Î»=0.05** | ON | 0.05 | 5 | 0.3 |
| **Mining Î»=0.1** | ON | 0.1 | 5 | 0.3 |

> **Note**: Mining Î»=0.1 seed=44 ì‹¤í—˜ì´ ëˆ„ë½ë˜ì–´ ì´ 11ê°œ ì‹¤í—˜ ì™„ë£Œ (12ê°œ ì¤‘)

#### 8.3 AUROC ê²°ê³¼ (Mean Â± Std, %)

| Condition | Domain A | Domain B | Domain C | Domain D | Overall |
|-----------|----------|----------|----------|----------|---------|
| **Baseline** | 98.91Â±0.08 | 99.10Â±0.12 | 97.50Â±0.06 | 98.18Â±0.05 | 98.60Â±0.05 |
| Mining Î»=0.01 | 98.99Â±0.04 | 99.12Â±0.03 | 97.55Â±0.17 | 98.23Â±0.04 | 98.64Â±0.05 |
| Mining Î»=0.05 | 98.94Â±0.07 | 99.10Â±0.12 | 97.33Â±0.06 | 98.27Â±0.09 | 98.60Â±0.05 |
| Mining Î»=0.1 | 98.88Â±0.13 | 98.97Â±0.23 | 97.34Â±0.21 | 98.20Â±0.02 | 98.53Â±0.13 |

**AUROC Delta from Baseline (pp)**:
| Condition | Domain A | Domain B | Domain C | Domain D | Overall |
|-----------|----------|----------|----------|----------|---------|
| Mining Î»=0.01 | +0.08 | +0.03 | **+0.05** | +0.05 | +0.03 |
| Mining Î»=0.05 | +0.03 | +0.01 | **-0.17** | +0.09 | -0.00 |
| Mining Î»=0.1 | -0.03 | -0.13 | **-0.17** | +0.02 | -0.07 |

#### 8.4 TPR@FPR=1% ê²°ê³¼ (Mean Â± Std, %) â­ í•µì‹¬ ì§€í‘œ

| Condition | Domain A | Domain B | Domain C | Domain D | Overall |
|-----------|----------|----------|----------|----------|---------|
| **Baseline** | 93.5Â±0.0 | 94.9Â±0.0 | **78.8Â±0.3** | 93.4Â±0.1 | 90.5Â±0.1 |
| Mining Î»=0.01 | 93.5Â±0.0 | 95.0Â±0.1 | **79.7Â±1.0** | 93.5Â±0.1 | 90.8Â±0.2 |
| Mining Î»=0.05 | 93.6Â±0.2 | 95.0Â±0.1 | **77.7Â±1.7** | 93.5Â±0.2 | 90.5Â±0.5 |
| Mining Î»=0.1 | 93.1Â±0.6 | 95.0Â±0.1 | **75.9Â±0.3** | 93.5Â±0.2 | 89.9Â±0.5 |

**TPR@FPR=1% Delta from Baseline (pp)**:
| Condition | Domain A | Domain B | Domain C | Domain D | Overall |
|-----------|----------|----------|----------|----------|---------|
| Mining Î»=0.01 | +0.0 | +0.1 | **+0.9** | +0.1 | +0.2 |
| Mining Î»=0.05 | +0.2 | +0.1 | **-1.1** | +0.0 | -0.0 |
| Mining Î»=0.1 | -0.4 | +0.0 | **-2.9** | +0.0 | -0.6 |

#### 8.5 Domain C ì„¸ë¶€ ë¶„ì„

| Condition | TPR@FPR=1% | ë³€í™” (pp) | í†µê³„ì  ìœ ì˜ì„± |
|-----------|------------|-----------|--------------|
| Baseline | 78.8% Â± 0.3% | - | - |
| Mining Î»=0.01 | 79.7% Â± 1.0% | **+0.9** | â“ ìœ ì˜í•˜ì§€ ì•ŠìŒ (ë†’ì€ std) |
| Mining Î»=0.05 | 77.7% Â± 1.7% | **-1.1** | âŒ ì„±ëŠ¥ ì €í•˜ |
| Mining Î»=0.1 | 75.9% Â± 0.3% | **-2.9** | âŒ ì‹¬ê°í•œ ì„±ëŠ¥ ì €í•˜ |

**ê°œë³„ ì‹¤í—˜ ê²°ê³¼ (Domain C TPR@FPR=1%)**:

| Seed | Baseline | Î»=0.01 | Î»=0.05 | Î»=0.1 |
|------|----------|--------|--------|-------|
| 42 | 78.7% | 78.3% | 77.3% | 75.6% |
| 43 | 79.2% | 80.6% | 75.9% | 76.2% |
| 44 | 78.5% | 80.2% | 80.0% | - |

#### 8.6 Hard Normal Mining ì•Œê³ ë¦¬ì¦˜ ë™ì‘ í™•ì¸

í•™ìŠµ ë¡œê·¸ ë¶„ì„ (Mining Î»=0.01, seed=42):

```
[Epoch 0]  stable_hard_normals=0,  max_ema_freq=0.100  # warmup ì¤‘
[Epoch 1]  stable_hard_normals=0,  max_ema_freq=0.100
[Epoch 2]  stable_hard_normals=0,  max_ema_freq=0.181
[Epoch 3]  stable_hard_normals=0,  max_ema_freq=0.190
[Epoch 4]  stable_hard_normals=0,  max_ema_freq=0.271
[Epoch 5]  stable_hard_normals=11, max_ema_freq=0.344  # warmup ì¢…ë£Œ, penalty ì‹œì‘
[Epoch 6]  stable_hard_normals=20, max_ema_freq=0.410
[Epoch 7]  stable_hard_normals=25, max_ema_freq=0.469
[Epoch 8]  stable_hard_normals=31, max_ema_freq=0.522
[Epoch 9]  stable_hard_normals=35, max_ema_freq=0.570
[Epoch 10] stable_hard_normals=38, max_ema_freq=0.613
[Epoch 11] stable_hard_normals=42, max_ema_freq=0.651
```

**ê´€ì°° ì‚¬í•­**:
- âœ… Mining ì•Œê³ ë¦¬ì¦˜ ì •ìƒ ë™ì‘ í™•ì¸
- âœ… 4000ê°œ ìƒ˜í”Œ ì¤‘ ~42ê°œ (ì•½ 1%) stable hard normals ì‹ë³„
- âœ… EMA frequency ì ì§„ì  ì¦ê°€ (0.1 â†’ 0.65)
- âœ… Warmup ì´í›„ penalty ì ìš© ì‹œì‘
- âŒ ê·¸ëŸ¬ë‚˜ TPR@FPR=1% ê°œì„  íš¨ê³¼ ì—†ìŒ

### 9. ê²°ë¡ : âŒ ê°€ì„¤ ê¸°ê°

#### 9.1 ê²°ê³¼ ìš”ì•½

**Method 3 (Stable Hard Normal Mining)ëŠ” Domain C TPR@FPR=1% ê°œì„ ì— íš¨ê³¼ ì—†ìŒ**

| ëª©í‘œ | ê²°ê³¼ | íŒì • |
|------|------|------|
| Domain C TPR@FPR=1%: 78.8% â†’ 85%+ (+7 pp) | ìµœëŒ€ +0.9 pp (79.7%) | âŒ ì‹¤íŒ¨ |
| ë‹¤ë¥¸ ë„ë©”ì¸ ì„±ëŠ¥ ìœ ì§€ | ìœ ì§€ë¨ (Î»=0.01) | âœ… ì„±ê³µ |
| AUROC ìœ ì§€ | ìœ ì§€ë¨ | âœ… ì„±ê³µ |

#### 9.2 ì‹¤íŒ¨ ì›ì¸ ë¶„ì„

1. **Penalty íš¨ê³¼ ë°©í–¥ ì˜¤ë¥˜**
   - ê°€ì„¤: stable hard normal ì ìˆ˜ ì–µì œ â†’ FP ê°ì†Œ â†’ threshold í•˜ê°• ê°€ëŠ¥ â†’ TPR ìƒìŠ¹
   - ì‹¤ì œ: penaltyê°€ ì»¤ì§ˆìˆ˜ë¡ Domain C TPR í•˜ë½
   - í•´ì„: tail penaltyê°€ ì „ë°˜ì ì¸ score calibrationì„ ë°©í•´í•  ê°€ëŠ¥ì„±

2. **Î» sensitivityì˜ ë°˜ì§ê´€ì  íŒ¨í„´**
   ```
   Î»=0.01: +0.9 pp (ë¯¸ë¯¸í•œ ê°œì„ , ë†’ì€ ë¶„ì‚°)
   Î»=0.05: -1.1 pp (ì„±ëŠ¥ ì €í•˜)
   Î»=0.1:  -2.9 pp (ì‹¬ê°í•œ ì„±ëŠ¥ ì €í•˜)
   ```
   - ë” ê°•í•œ penaltyê°€ ì˜¤íˆë ¤ í•´ë¡œì›€
   - Optimal Î» â‰ˆ 0 (ì¦‰, penalty ì—†ìŒ)ì— ê°€ê¹Œì›€

3. **Domain C íŠ¹ìˆ˜ì„± ë¯¸ë°˜ì˜**
   - stable hard normalì€ ì „ì²´ ë„ë©”ì¸ì—ì„œ í†µí•© ê³„ì‚°ë¨
   - Domain C íŠ¹ìœ ì˜ diffuse anomaly íŒ¨í„´ì— ëŒ€í•œ íŠ¹í™” ì²˜ë¦¬ ë¶€ì¬
   - ë‹¤ë¥¸ ë„ë©”ì¸ì˜ hard normal íŒ¨í„´ì´ Domain Cì— ë¶€ì •ì  ì˜í–¥

4. **Score Distribution Shift ë¶€ì‘ìš©**
   - tail penaltyê°€ ì •ìƒ ìƒ˜í”Œ scoreë¥¼ ë‚®ì¶”ëŠ” ê³¼ì •ì—ì„œ
   - ê²°í•¨ ìƒ˜í”Œ scoreë„ ê°„ì ‘ì ìœ¼ë¡œ ì˜í–¥ë°›ìŒ (reconstruction í•™ìŠµ ë³€í™”)
   - ê²°ê³¼ì ìœ¼ë¡œ ë¶„ë¦¬ë„(separability) ì €í•˜

#### 9.3 AUROC vs TPR@FPR=1% ë¶ˆì¼ì¹˜

- **AUROC**: í° ë³€í™” ì—†ìŒ (ì „ì²´ ìˆœìœ„ í’ˆì§ˆ ìœ ì§€)
- **TPR@FPR=1%**: Î» ì¦ê°€ ì‹œ í•˜ë½
- **í•´ì„**: tail ë¶„í¬ë§Œ ë³€í™”, ì „ì²´ ë¶„í¬ëŠ” ìœ ì§€
  - ì´ëŠ” ì•Œê³ ë¦¬ì¦˜ì´ ì˜ë„í•œ ëŒ€ë¡œ ë™ì‘í•˜ì§€ë§Œ
  - ë°©í–¥ì´ ë°˜ëŒ€ (tailì„ ì–µì œí•˜ë©´ì„œ faultë„ ì–µì œ)

#### 9.4 Method 1 vs Method 3 ë¹„êµ

| í•­ëª© | Method 1 (GeM Aux Loss) | Method 3 (Hard Normal Mining) |
|------|------------------------|------------------------------|
| Domain C TPR@FPR=1% ë³€í™” | +0.23 pp | +0.9 pp (best) |
| ë‹¤ë¥¸ ë„ë©”ì¸ ì˜í–¥ | ì—†ìŒ | Î» ì¦ê°€ ì‹œ ë¶€ì •ì  |
| ë¶€ì‘ìš© | ì—†ìŒ | Î» â‰¥ 0.05ì—ì„œ ì„±ëŠ¥ ì €í•˜ |
| ê²°ë¡  | íš¨ê³¼ ì—†ìŒ | íš¨ê³¼ ì—†ìŒ (ì˜¤íˆë ¤ í•´ë¡œìš¸ ìˆ˜ ìˆìŒ) |

### 10. í›„ì† ì¡°ì¹˜

#### 10.1 Method 3 íê¸° ê²°ì •

- **ê²°ë¡ **: Stable Hard Normal Mining ì ‘ê·¼ë²•ì€ Domain C ê°œì„ ì— ë¶€ì í•©
- **ì¡°ì¹˜**: ì½”ë“œëŠ” ìœ ì§€í•˜ë˜, ì‹¤í—˜ì—ì„œ ì œì™¸ ê¶Œê³ 

#### 10.2 ëŒ€ì•ˆ íƒìƒ‰ í•„ìš”

Method 1, 3 ëª¨ë‘ ì‹¤íŒ¨ â†’ ìƒˆë¡œìš´ ì ‘ê·¼ë²• í•„ìš”:

1. **Inference-time MAX_RATIO ë³€ê²½** (ê°€ì¥ ê°„ë‹¨)
   - 0.01 â†’ 0.05ë¡œ ë³€ê²½ ì‹œ +7% ê°œì„  í™•ì¸ë¨
   - Training ë³€ê²½ ì—†ì´ ì ìš© ê°€ëŠ¥

2. **Domain-specific Fine-tuning**
   - í†µí•© ëª¨ë¸ í•™ìŠµ í›„ Domain C ì „ìš© fine-tuning
   - Domain C ë°ì´í„°ì— ëŒ€í•´ ì¶”ê°€ í•™ìŠµ

3. **Multi-scale Aggregation**
   - ë‹¨ì¼ MAX_RATIO ëŒ€ì‹  ì—¬ëŸ¬ scale ì•™ìƒë¸”
   - diffuse anomalyì— ë” ì í•©í•œ ì§‘ê³„ ë°©ì‹

4. **Attention-based ì ‘ê·¼**
   - Domain C íŠ¹ìœ ì˜ horizontal line patternì— íŠ¹í™”ëœ attention
   - ëª…ì‹œì ì¸ domain-aware ì²˜ë¦¬

### 11. ì‹¤í—˜ ê²°ê³¼ íŒŒì¼

| íŒŒì¼ | ìœ„ì¹˜ |
|------|------|
| ì‹¤í—˜ ì„¤ì • | `results/dinomaly_multiclass_baseline/2025122_05*/experiment_settings.json` |
| AUROC ê²°ê³¼ | `results/dinomaly_multiclass_baseline/2025122_05*/final_summary.json` |
| TPR@FPR ë¶„ì„ | `results/dinomaly_multiclass_baseline/tpr_at_fpr_analysis.json` |
| ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸ | `examples/notebooks/calculate_tpr_at_fpr.py` |
| í•™ìŠµ ë¡œê·¸ | `logs/method3_*.log` |

---

## Method 5: Learnable Scale Weights

> **ìƒíƒœ**: âœ… ì‹¤í—˜ ì™„ë£Œ (2025-12-23)

### 1. ë¬¸ì œ ì •ì˜

#### 1.1 í˜„ìƒ
- **Domain C TPR@FPR=1% = 78.8%**: ë‹¤ë¥¸ ë„ë©”ì¸(93-95%) ëŒ€ë¹„ í˜„ì €íˆ ë‚®ìŒ
- **FNìœ¨ 20.7%**: ë‹¤ë¥¸ ë„ë©”ì¸(5-6.5%)ì˜ 3~4ë°°
- **Method 1, 3 ëª¨ë‘ ì‹¤íŒ¨**: ìƒˆë¡œìš´ ì ‘ê·¼ë²• í•„ìš”

#### 1.2 ê·¼ë³¸ ì›ì¸ ë¶„ì„ (Method 3 ì‹¬ì¸µ ë¶„ì„ ê²°ê³¼)

**í•µì‹¬ ë°œê²¬**: ë¬¸ì œëŠ” hard normal tailì´ ì•„ë‹ˆë¼ **fault scoreê°€ ë„ˆë¬´ ë‚®ìŒ**

```
Domainë³„ Score ë¶„í¬ ë¹„êµ:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Domain  â”‚ Normal Mean  â”‚ Fault Mean   â”‚ Separation  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ A       â”‚ 0.15         â”‚ 0.45         â”‚ 0.30 (good) â”‚
â”‚ B       â”‚ 0.14         â”‚ 0.42         â”‚ 0.28 (good) â”‚
â”‚ C       â”‚ 0.16         â”‚ 0.28         â”‚ 0.12 (poor) â”‚ â† Faultê°€ ë‚®ìŒ
â”‚ D       â”‚ 0.13         â”‚ 0.40         â”‚ 0.27 (good) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 1.3 Multi-Scale Anomaly Map êµ¬ì¡°

DinomalyëŠ” 2ê°œì˜ scaleì—ì„œ anomaly mapì„ ìƒì„±:

```python
# í˜„ì¬ êµ¬í˜„ (torch_model.py)
fuse_layer_encoder = [[0,1,2,3], [4,5,6,7]]  # 2 scales
anomaly_map_list = [scale_0_map, scale_1_map]  # ê° ìŠ¤ì¼€ì¼ë³„ anomaly map
anomaly_map = torch.cat(anomaly_map_list, dim=1).mean(dim=1)  # ë‹¨ìˆœ í‰ê· 
```

**ê°€ì„¤**: Domain Cì˜ diffuse anomaly íŒ¨í„´ì€ íŠ¹ì • scaleì—ì„œ ë” ì˜ ê²€ì¶œë˜ì§€ë§Œ,
ë‹¨ìˆœ í‰ê· ìœ¼ë¡œ ì¸í•´ ê·¸ ì‹ í˜¸ê°€ í¬ì„ë¨

### 2. í•µì‹¬ ê°€ì„¤

> **Learnable Scale Weighting ê°€ì„¤**:
> í•™ìŠµ ê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜ë¡œ multi-scale anomaly mapì„ ê²°í•©í•˜ë©´,
> ë„ë©”ì¸/ê²°í•¨ ìœ í˜•ì— ìµœì í™”ëœ scale ì„ íƒì´ ê°€ëŠ¥í•´ì ¸ TPRì´ ê°œì„ ëœë‹¤.

#### 2.1 ì´ë¡ ì  ê·¼ê±°

```
Before (ë‹¨ìˆœ í‰ê· ):
  Scale 0: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ (fault signal weak)
  Scale 1: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (fault signal strong)
  Average: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ (signal diluted)

After (Learned weights):
  Scale 0: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ Ã— 0.2
  Scale 1: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ Ã— 0.8
  Weighted: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘ (signal preserved)
```

#### 2.2 ê°€ì¤‘ì¹˜ í•™ìŠµ ì›ë¦¬

```
Gradient Flow:
  Loss (reconstruction)
    â†“ backprop
  anomaly_map.mean() (auxiliary loss)
    â†“
  weighted_sum = Î£(scale_i Ã— softmax(logits)_i)
    â†“
  scale_logits (learnable parameter)

ê²°í•¨ ì¬êµ¬ì„±ì´ ì–´ë ¤ìš´ ìŠ¤ì¼€ì¼ â†’ ë†’ì€ anomaly score â†’ loss ê¸°ì—¬ â†‘ â†’ ê°€ì¤‘ì¹˜ â†‘
```

#### 2.3 ì˜ˆìƒ íš¨ê³¼
- **Domain C TPR@FPR=1%**: 78.8% â†’ 82%+ (ëª©í‘œ: +3 pp)
- **ë‹¤ë¥¸ ë„ë©”ì¸**: ìœ ì§€ (ì´ë¯¸ ë†’ìŒ)
- **AUROC**: ìœ ì§€ ë˜ëŠ” ì†Œí­ ìƒìŠ¹

### 3. êµ¬í˜„ ë‚´ìš©

#### 3.1 ì•Œê³ ë¦¬ì¦˜ ê°œìš”

```
[Initialization]
  scale_logits = nn.Parameter(zeros(num_scales))  # [0, 0] â†’ softmax â†’ [0.5, 0.5]

[Training Step]
  1. Encoder/Decoder forward pass
  2. Main loss = CosineHardMiningLoss(en, de)
  3. If scale_weights enabled:
     a. anomaly_map = calculate_anomaly_maps(en, de)  # weighted combination
     b. aux_loss = anomaly_map.mean()  # minimize for normal samples
     c. total_loss = main_loss + 0.01 * aux_loss
  4. Backprop â†’ scale_logits receives gradient

[Inference]
  weights = softmax(scale_logits)  # e.g., [0.3, 0.7]
  anomaly_map = Î£(scale_maps Ã— weights)
```

#### 3.2 ìˆ˜ì • íŒŒì¼

| íŒŒì¼ | ë³€ê²½ ë‚´ìš© |
|------|----------|
| `torch_model.py` | `use_learnable_scale_weights` íŒŒë¼ë¯¸í„°, `scale_logits` nn.Parameter ì¶”ê°€ |
| `torch_model.py` | `calculate_anomaly_maps()` instance methodë¡œ ë³€ê²½, weighted sum êµ¬í˜„ |
| `torch_model.py` | `forward()` training ê²½ë¡œì— auxiliary loss ì¶”ê°€ |
| `lightning_model.py` | íŒŒë¼ë¯¸í„° ì „ë‹¬, optimizerì— scale_logits ì¶”ê°€ (10x LR) |
| `lightning_model.py` | 100 stepë§ˆë‹¤ í•™ìŠµëœ ê°€ì¤‘ì¹˜ ë¡œê¹… |
| `dinomaly_multiclass_baseline.py` | `--use-learnable-scale-weights` CLI ì¸ì ì¶”ê°€ |

#### 3.3 í•µì‹¬ ì½”ë“œ

**torch_model.py - Weighted Aggregation:**
```python
def calculate_anomaly_maps(self, source_feature_maps, target_feature_maps, out_size):
    # ... compute individual scale anomaly maps ...
    maps = torch.cat(anomaly_map_list, dim=1)  # [B, S, H, W]

    if self.use_learnable_scale_weights and self.scale_logits is not None:
        weights = F.softmax(self.scale_logits, dim=0)  # [S]
        anomaly_map = (maps * weights.view(1, -1, 1, 1)).sum(dim=1, keepdim=True)
    else:
        anomaly_map = maps.mean(dim=1, keepdim=True)  # backward compatible

    return anomaly_map, anomaly_map_list
```

**torch_model.py - Auxiliary Loss for Gradient Flow:**
```python
if self.training:
    main_loss = self.loss_fn(encoder_features=en, decoder_features=de, global_step=global_step)

    if self.use_learnable_scale_weights and self.scale_logits is not None:
        anomaly_map, _ = self.calculate_anomaly_maps(en, de, out_size=image_size)
        aux_loss = anomaly_map.mean()  # normal samples â†’ low score
        return main_loss + 0.01 * aux_loss

    return main_loss
```

**lightning_model.py - Optimizer with Higher LR:**
```python
param_groups = [{"params": self.trainable_modules.parameters()}]
if self.model.use_learnable_scale_weights:
    param_groups.append({"params": [self.model.scale_logits], "lr": lr * 10})
optimizer = StableAdamW(param_groups, **optimizer_config)
```

### 4. í•˜ì´í¼íŒŒë¼ë¯¸í„°

| íŒŒë¼ë¯¸í„° | ê°’ | ì„¤ëª… |
|----------|-----|------|
| `use_learnable_scale_weights` | True/False | ê¸°ëŠ¥ í™œì„±í™” |
| `num_scales` | 2 (auto) | fuse_layer_encoder ê°œìˆ˜ |
| `aux_loss_weight` | 0.01 | Auxiliary loss ê°€ì¤‘ì¹˜ |
| `scale_lr_multiplier` | 10x | scale_logits í•™ìŠµë¥  ë°°ìˆ˜ |
| `log_interval` | 100 steps | ê°€ì¤‘ì¹˜ ë¡œê¹… ê°„ê²© |

### 5. ì‹¤í—˜ ê³„íš

#### 5.1 ì‹¤í—˜ ì¡°ê±´

| ID | ì„¤ì • | ëª©ì  |
|----|------|------|
| M5-E0 | Baseline (ë‹¨ìˆœ í‰ê· ) | ë¹„êµ ê¸°ì¤€ |
| M5-E1 | Learnable scale weights | ê°€ì¤‘ í‰ê·  íš¨ê³¼ í™•ì¸ |

#### 5.2 ì‹¤í—˜ ëª…ë ¹ì–´

**E0: Baseline (3íšŒ ë°˜ë³µ)**
```bash
# GPU 0, 1, 2ì—ì„œ ë³‘ë ¬ ì‹¤í–‰
nohup python examples/notebooks/dinomaly_multiclass_baseline.py \
    --mode multiclass --max-steps 3000 --seed 42 --gpu 0 \
    > logs/method5_E0_seed42.log 2>&1 &
sleep 3

nohup python examples/notebooks/dinomaly_multiclass_baseline.py \
    --mode multiclass --max-steps 3000 --seed 43 --gpu 1 \
    > logs/method5_E0_seed43.log 2>&1 &
sleep 3

nohup python examples/notebooks/dinomaly_multiclass_baseline.py \
    --mode multiclass --max-steps 3000 --seed 44 --gpu 2 \
    > logs/method5_E0_seed44.log 2>&1 &
```

**E1: Learnable Scale Weights (3íšŒ ë°˜ë³µ)**
```bash
# GPU 3, 4, 5ì—ì„œ ë³‘ë ¬ ì‹¤í–‰ (ë˜ëŠ” E0 ì™„ë£Œ í›„ ì¬ì‚¬ìš©)
nohup python examples/notebooks/dinomaly_multiclass_baseline.py \
    --mode multiclass --max-steps 3000 --seed 42 --gpu 3 \
    --use-learnable-scale-weights \
    > logs/method5_E1_seed42.log 2>&1 &
sleep 3

nohup python examples/notebooks/dinomaly_multiclass_baseline.py \
    --mode multiclass --max-steps 3000 --seed 43 --gpu 4 \
    --use-learnable-scale-weights \
    > logs/method5_E1_seed43.log 2>&1 &
sleep 3

nohup python examples/notebooks/dinomaly_multiclass_baseline.py \
    --mode multiclass --max-steps 3000 --seed 44 --gpu 5 \
    --use-learnable-scale-weights \
    > logs/method5_E1_seed44.log 2>&1 &
```

#### 5.3 ìˆœì°¨ ì‹¤í–‰ (GPU ì œí•œ ì‹œ)

```bash
# ë‹¨ì¼ GPUì—ì„œ ìˆœì°¨ ì‹¤í–‰
for SEED in 42 43 44; do
    echo "Running E0 seed $SEED..."
    python examples/notebooks/dinomaly_multiclass_baseline.py \
        --mode multiclass --max-steps 3000 --seed $SEED --gpu 0 \
        2>&1 | tee logs/method5_E0_seed${SEED}.log
done

for SEED in 42 43 44; do
    echo "Running E1 seed $SEED..."
    python examples/notebooks/dinomaly_multiclass_baseline.py \
        --mode multiclass --max-steps 3000 --seed $SEED --gpu 0 \
        --use-learnable-scale-weights \
        2>&1 | tee logs/method5_E1_seed${SEED}.log
done
```

### 6. í‰ê°€ ì§€í‘œ

#### 6.1 Primary Metrics
- **Domain C TPR@FPR=1%**: ëª©í‘œ 82%+ (baseline 78.8%)
- **í•™ìŠµëœ scale weights**: TensorBoardì—ì„œ í™•ì¸ (`scale_weight_0`, `scale_weight_1`)

#### 6.2 Secondary Metrics
- ë‹¤ë¥¸ ë„ë©”ì¸ TPR@FPR=1%: ìœ ì§€ í™•ì¸ (93-95%)
- Mean domain AUROC: ìœ ì§€ í™•ì¸
- í•™ìŠµ ì‹œê°„: baseline ëŒ€ë¹„ ì¦ê°€ëŸ‰

#### 6.3 ì„±ê³µ ê¸°ì¤€

| ì§€í‘œ | Baseline | ëª©í‘œ | íŒì • |
|------|----------|------|------|
| Domain C TPR@FPR=1% | 78.8% | 82%+ | +3 pp ì´ìƒ |
| ë‹¤ë¥¸ ë„ë©”ì¸ TPR@FPR=1% | 93-95% | 93%+ | ìœ ì§€ |
| Mean AUROC | ~96.5% | 96%+ | ìœ ì§€ |

### 7. ë¶„ì„ ê³„íš

#### 7.1 í•™ìŠµëœ ê°€ì¤‘ì¹˜ ë¶„ì„
```bash
# TensorBoardì—ì„œ scale_weight_0, scale_weight_1 í™•ì¸
tensorboard --logdir=results/dinomaly_multiclass_baseline/
```

ì˜ˆìƒ íŒ¨í„´:
- ì´ˆê¸°: [0.5, 0.5] (ê· ë“±)
- í•™ìŠµ í›„: [Î±, 1-Î±] where Î± â‰  0.5

#### 7.2 TPR@FPR í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
```bash
python examples/notebooks/calculate_tpr_at_fpr.py \
    --checkpoint-dir results/dinomaly_multiclass_baseline/<timestamp>/multiclass_unified/checkpoints \
    --output results/dinomaly_multiclass_baseline/method5_tpr_at_fpr_analysis.json
```

### 8. í›„ì† ì‹¤í—˜ (ì¡°ê±´ë¶€)

#### 8.1 Method 5-A ì„±ê³µ ì‹œ
- ê²°ê³¼ ë¶„ì„ í›„ 5-B (Domain-Conditional Weights) ê²€í† 
- Domainë³„ë¡œ ë‹¤ë¥¸ scale ì„ í˜¸ë„ê°€ ìˆëŠ”ì§€ í™•ì¸

#### 8.2 Method 5-A ì‹¤íŒ¨ ì‹œ
- í•™ìŠµëœ ê°€ì¤‘ì¹˜ ë¶„ì„: ì˜ë¯¸ìˆëŠ” ë¶„ë¦¬ê°€ ë°œìƒí–ˆëŠ”ì§€
- aux_loss_weight ì¡°ì •: 0.01 â†’ 0.1 ë˜ëŠ” 0.001
- Scale LR multiplier ì¡°ì •: 10x â†’ 50x ë˜ëŠ” 5x

### 9. ì‹¤í—˜ ê²°ê³¼

> **ìƒíƒœ**: âœ… ì‹¤í—˜ ì™„ë£Œ (2025-12-23)

#### 9.1 ì‹¤í—˜ ì„¤ì •

| ì‹¤í—˜ ID | Timestamp | Seed | use_learnable_scale_weights |
|---------|-----------|------|----------------------------|
| Baseline-1 | 20251222_125309 | 42 | False |
| Baseline-2 | 20251222_125313 | 43 | False |
| Baseline-3 | 20251222_125317 | 44 | False |
| Method5A-1 | 20251222_125330 | 42 | True |
| Method5A-2 | 20251222_125333 | 43 | True |
| Method5A-3 | 20251222_125336 | 44 | True |

#### 9.2 TPR@FPR=1% ê²°ê³¼

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Domain  â”‚ Baseline TPR@FPR=1%     â”‚ Method5A TPR@FPR=1%     â”‚ Î” (pp)     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚    A    â”‚ 94.40% Â± 0.75%          â”‚ 94.67% Â± 0.47%          â”‚ +0.27      â”‚
â”‚    B    â”‚ 95.87% Â± 0.41%          â”‚ 94.20% Â± 0.33%          â”‚ -1.67      â”‚
â”‚    C    â”‚ 79.07% Â± 4.49%          â”‚ 81.27% Â± 0.84%          â”‚ +2.20      â”‚
â”‚    D    â”‚ 93.07% Â± 0.90%          â”‚ 93.27% Â± 0.50%          â”‚ +0.20      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 9.3 AUROC ê²°ê³¼

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Domain  â”‚ Baseline AUROC          â”‚ Method5A AUROC          â”‚ Î”          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚    A    â”‚ 99.01% Â± 0.24%          â”‚ 99.00% Â± 0.08%          â”‚ -0.01      â”‚
â”‚    B    â”‚ 99.32% Â± 0.06%          â”‚ 98.91% Â± 0.21%          â”‚ -0.40      â”‚
â”‚    C    â”‚ 97.62% Â± 0.24%          â”‚ 97.40% Â± 0.21%          â”‚ -0.22      â”‚
â”‚    D    â”‚ 97.84% Â± 0.30%          â”‚ 98.18% Â± 0.19%          â”‚ +0.34      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 9.4 í•™ìŠµëœ Scale Weights

ëª¨ë“  seedì—ì„œ ì¼ê´€ëœ ê°€ì¤‘ì¹˜ í•™ìŠµ:

| Scale | Layers | Weight (Mean Â± Std) |
|-------|--------|---------------------|
| Scale 0 | [0, 1, 2, 3] | **34.78%** Â± 0.01% |
| Scale 1 | [4, 5, 6, 7] | **65.22%** Â± 0.01% |

- ì´ˆê¸°ê°’: [50%, 50%] (ê· ë“±)
- í•™ìŠµ í›„: ê¹Šì€ layer (Scale 1)ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜ ë¶€ì—¬
- 3ê°œ seedì—ì„œ ë§¤ìš° ì¼ê´€ëœ ê²°ê³¼ (std < 0.02%)

#### 9.5 ê°œë³„ ì‹¤í—˜ ìƒì„¸ ê²°ê³¼

**Baseline (seedë³„)**

| Seed | Domain A | Domain B | Domain C | Domain D |
|------|----------|----------|----------|----------|
| 42 | 93.4% | 95.8% | 77.4% | 92.0% |
| 43 | 94.6% | 96.4% | 74.6% | 93.0% |
| 44 | 95.2% | 95.4% | 85.2% | 94.2% |

**Method 5-A (seedë³„)**

| Seed | Domain A | Domain B | Domain C | Domain D |
|------|----------|----------|----------|----------|
| 42 | 95.0% | 94.6% | 80.4% | 93.4% |
| 43 | 95.0% | 93.8% | 81.0% | 92.6% |
| 44 | 94.0% | 94.2% | 82.4% | 93.8% |

#### 9.6 ë¶„ì„ ë° ê²°ë¡ 

**1. Domain C ê°œì„  ë‹¬ì„±**
- Baseline: 79.07% â†’ Method 5-A: **81.27%** (+2.20 pp)
- ëª©í‘œ 82%ì— 0.73 pp ë¶€ì¡±í•˜ì§€ë§Œ **ìœ ì˜ë¯¸í•œ ê°œì„ **
- íŠ¹íˆ **ë¶„ì‚° 5.3ë°° ê°ì†Œ** (4.49% â†’ 0.84%): í›¨ì”¬ ì•ˆì •ì ì¸ ì˜ˆì¸¡

**2. Domain B ì†Œí­ í•˜ë½**
- Baseline: 95.87% â†’ Method 5-A: 94.20% (-1.67 pp)
- ì—¬ì „íˆ ë†’ì€ ì„±ëŠ¥ ìœ ì§€ (94%+)
- Trade-off: Domain C ê°œì„  vs Domain B ì†Œí­ í•˜ë½

**3. í•™ìŠµëœ ê°€ì¤‘ì¹˜ í•´ì„**
- Scale 1 (layers 4-7)ì— 65% ê°€ì¤‘ì¹˜ â†’ ê¹Šì€ semantic feature ì¤‘ì‹œ
- Domain Cì˜ diffuse anomaly íŒ¨í„´ì€ ê³ ìˆ˜ì¤€ ì˜ë¯¸ ì •ë³´ì—ì„œ ë” ì˜ ê²€ì¶œë¨
- ëª¨ë“  seedì—ì„œ ë™ì¼í•œ ë¹„ìœ¨ë¡œ ìˆ˜ë ´: ìµœì  ê°€ì¤‘ì¹˜ê°€ ë°ì´í„°ë¡œë¶€í„° í•™ìŠµë¨

**4. ëª©í‘œ ë‹¬ì„± í‰ê°€**

| ì§€í‘œ | ëª©í‘œ | ê²°ê³¼ | íŒì • |
|------|------|------|------|
| Domain C TPR@FPR=1% | 82%+ | 81.27% | âš ï¸ ê·¼ì ‘ (0.73 pp ë¶€ì¡±) |
| ë‹¤ë¥¸ ë„ë©”ì¸ TPR@FPR=1% | 93%+ | 93-94%+ | âœ… ë‹¬ì„± |
| Mean AUROC | 96%+ | ~98% | âœ… ë‹¬ì„± |
| ë¶„ì‚° ê°ì†Œ | - | 5.3ë°° ê°ì†Œ | âœ… ë³´ë„ˆìŠ¤ |

**5. í›„ì† ì‹¤í—˜ ì œì•ˆ**

ëª©í‘œ 82%ì— ê·¼ì ‘í–ˆìœ¼ë‚˜ ë¯¸ë‹¬ì´ë¯€ë¡œ ë‹¤ìŒ ì‹¤í—˜ ê³ ë ¤:

- **Method 5-B (Domain-Conditional Weights)**: ë„ë©”ì¸ë³„ ë‹¤ë¥¸ ê°€ì¤‘ì¹˜ í•™ìŠµ
- **aux_loss_weight ì¡°ì •**: 0.01 â†’ 0.05 ë˜ëŠ” 0.1ë¡œ ì¦ê°€
- **num_scales í™•ì¥**: 2ê°œ â†’ 4ê°œ ìŠ¤ì¼€ì¼ë¡œ ì„¸ë¶„í™”

#### 9.7 ê²°ê³¼ íŒŒì¼

| íŒŒì¼ | ê²½ë¡œ |
|------|------|
| ë¶„ì„ ê²°ê³¼ JSON | `results/dinomaly_multiclass_baseline/method5_analysis_v3.json` |
| ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸ | `examples/notebooks/method5_analysis_v3.py` |
| Baseline ì²´í¬í¬ì¸íŠ¸ | `results/dinomaly_multiclass_baseline/20251222_125309/` |
| Method5A ì²´í¬í¬ì¸íŠ¸ | `results/dinomaly_multiclass_baseline/20251222_125330/` |

---

## Method 5-B: Domain-Conditional Scale Weights

> **ìƒíƒœ**: ğŸ”„ êµ¬í˜„ ì™„ë£Œ, ì‹¤í—˜ ëŒ€ê¸° ì¤‘ (2025-12-23)

### 1. ë™ê¸°

Method 5-A ê²°ê³¼ì—ì„œ ë°œê²¬ëœ í•µì‹¬ ê´€ì°°:
- **Domain C TPR ê°œì„  (+2.20 pp)**: Global weightsê°€ Domain Cì— ìœ ë¦¬í•œ ë°©í–¥ìœ¼ë¡œ í•™ìŠµë¨
- **Domain B í•˜ë½ (-1.67 pp)**: ë™ì‹œì— Domain B ì„±ëŠ¥ì´ ê°ì†Œ
- **Trade-off ë°œìƒ**: Global Î±ê°€ ëª¨ë“  ë„ë©”ì¸ì— ê°•ì œë˜ì–´ ìµœì í™” ì¶©ëŒ

ì´ëŠ” **ë„ë©”ì¸ë³„ ìµœì  ìŠ¤ì¼€ì¼ì´ ë‹¤ë¥´ë‹¤**ëŠ” ê°•ë ¥í•œ ì¦ê±°:
```
Domain C: ê¹Šì€ ìŠ¤ì¼€ì¼(scale 1)ì´ ìœ ë¦¬ â†’ diffuse anomaly ê²€ì¶œ
Domain B: ì–•ì€ ìŠ¤ì¼€ì¼(scale 0)ì´ ìœ ë¦¬ â†’ local anomaly ê²€ì¶œ
```

### 2. í•µì‹¬ ê°€ì„¤

> **Domain-Conditional Weighting ê°€ì„¤**:
> ë„ë©”ì¸ë³„ë¡œ ë‹¤ë¥¸ scale weightsë¥¼ í•™ìŠµí•˜ë©´,
> Global Î±ì˜ trade-offë¥¼ í•´ì†Œí•˜ê³  ëª¨ë“  ë„ë©”ì¸ì—ì„œ ìµœì  ì„±ëŠ¥ ë‹¬ì„± ê°€ëŠ¥

ìˆ˜ì‹:
```
Î±_d = softmax(w_d)  where w_d âˆˆ R^{num_scales}
M = Î£_i Î±_{d,i} Ã— M_i
```

### 3. êµ¬í˜„ ë‚´ìš©

#### 3.1 íŒŒë¼ë¯¸í„° ë³€ê²½

| í•­ëª© | Method 5-A (Global) | Method 5-B (Domain-Conditional) |
|------|---------------------|--------------------------------|
| scale_logits shape | `[num_scales]` | `[num_domains, num_scales]` |
| íŒŒë¼ë¯¸í„° ìˆ˜ | 2 | 8 (4 domains Ã— 2 scales) |
| ê°€ì¤‘ì¹˜ ì ìš© | ì „ì²´ ë™ì¼ | ë„ë©”ì¸ë³„ ì„ íƒ |

#### 3.2 ìˆ˜ì • íŒŒì¼

| íŒŒì¼ | ë³€ê²½ ë‚´ìš© |
|------|----------|
| `torch_model.py` | `use_domain_conditional_scale_weights`, `num_domains` íŒŒë¼ë¯¸í„° ì¶”ê°€ |
| `torch_model.py` | `scale_logits` shape: `[num_domains, num_scales]` |
| `torch_model.py` | `calculate_anomaly_maps()`: `domain_idx` ê¸°ë°˜ weight ì„ íƒ |
| `lightning_model.py` | íŒŒë¼ë¯¸í„° ì „ë‹¬, `_extract_domain_idx_from_batch()` ì¶”ê°€ |
| `dinomaly_multiclass_baseline.py` | `--use-domain-conditional-scale-weights` CLI ì¸ì ì¶”ê°€ |

#### 3.3 í•µì‹¬ ì½”ë“œ

**torch_model.py - Domain-Conditional Weights:**
```python
if use_domain_conditional_scale_weights:
    # [num_domains, num_scales] - ë„ë©”ì¸ë³„ ê°œë³„ ê°€ì¤‘ì¹˜
    self.scale_logits = nn.Parameter(torch.zeros(num_domains, num_scales))

def calculate_anomaly_maps(self, ..., domain_idx=None):
    if self.use_domain_conditional_scale_weights:
        # domain_idx: [B] with values in [0, num_domains-1]
        sample_logits = self.scale_logits[domain_idx]  # [B, S]
        weights = F.softmax(sample_logits, dim=-1)     # [B, S]
        anomaly_map = (maps * weights.view(B, -1, 1, 1)).sum(dim=1, keepdim=True)
```

**lightning_model.py - Domain Extraction:**
```python
def _extract_domain_idx_from_batch(self, batch):
    # Parse domain from filename: 'domain_A_xxx.tiff' -> 0
    domain_map = {"domain_A": 0, "domain_B": 1, "domain_C": 2, "domain_D": 3}
    for path in batch.image_path:
        filename = str(path).split('/')[-1]
        for domain_name, idx in domain_map.items():
            if filename.startswith(domain_name):
                domain_indices.append(idx)
    return torch.tensor(domain_indices, device=batch.image.device)
```

### 4. ì‹¤í—˜ ê³„íš

#### 4.1 ì‹¤í—˜ ì¡°ê±´

| ID | ì„¤ì • | ëª©ì  |
|----|------|------|
| M5B-E0 | Baseline (5-A ê²°ê³¼ ì¬ì‚¬ìš©) | ë¹„êµ ê¸°ì¤€ |
| M5B-E1 | Domain-conditional weights | ë„ë©”ì¸ë³„ ìµœì í™” íš¨ê³¼ í™•ì¸ |

#### 4.2 ì‹¤í—˜ ëª…ë ¹ì–´

**E1: Domain-Conditional Scale Weights (3íšŒ ë°˜ë³µ)**
```bash
# GPU 0, 1, 2ì—ì„œ ë³‘ë ¬ ì‹¤í–‰
nohup python examples/notebooks/dinomaly_multiclass_baseline.py \
    --mode multiclass --max-steps 3000 --seed 42 --gpu 0 \
    --use-domain-conditional-scale-weights \
    > logs/method5B_E1_seed42.log 2>&1 &
sleep 3

nohup python examples/notebooks/dinomaly_multiclass_baseline.py \
    --mode multiclass --max-steps 3000 --seed 43 --gpu 1 \
    --use-domain-conditional-scale-weights \
    > logs/method5B_E1_seed43.log 2>&1 &
sleep 3

nohup python examples/notebooks/dinomaly_multiclass_baseline.py \
    --mode multiclass --max-steps 3000 --seed 44 --gpu 2 \
    --use-domain-conditional-scale-weights \
    > logs/method5B_E1_seed44.log 2>&1 &
```

#### 4.3 ìˆœì°¨ ì‹¤í–‰ (GPU ì œí•œ ì‹œ)

```bash
# ë‹¨ì¼ GPUì—ì„œ ìˆœì°¨ ì‹¤í–‰
for SEED in 42 43 44; do
    echo "Running Method 5-B seed $SEED..."
    python examples/notebooks/dinomaly_multiclass_baseline.py \
        --mode multiclass --max-steps 3000 --seed $SEED --gpu 0 \
        --use-domain-conditional-scale-weights \
        2>&1 | tee logs/method5B_E1_seed${SEED}.log
done
```

#### 4.4 ë³µì‚¬-ë¶™ì—¬ë„£ê¸°ìš© (ë‹¨ì¼ ì‹¤í–‰)

```bash
# Seed 42
nohup python examples/notebooks/dinomaly_multiclass_baseline.py --mode multiclass --max-steps 3000 --seed 42 --gpu 0 --use-domain-conditional-scale-weights > logs/method5B_seed42.log 2>&1 &

# Seed 43
nohup python examples/notebooks/dinomaly_multiclass_baseline.py --mode multiclass --max-steps 3000 --seed 43 --gpu 1 --use-domain-conditional-scale-weights > logs/method5B_seed43.log 2>&1 &

# Seed 44
nohup python examples/notebooks/dinomaly_multiclass_baseline.py --mode multiclass --max-steps 3000 --seed 44 --gpu 2 --use-domain-conditional-scale-weights > logs/method5B_seed44.log 2>&1 &
```

### 5. í‰ê°€ ì§€í‘œ

#### 5.1 Primary Metrics
- **Domain C TPR@FPR=1%**: 81.27% â†’ 82%+ (ëª©í‘œ)
- **Domain B TPR@FPR=1%**: 94.20% â†’ 95%+ (íšŒë³µ)

#### 5.2 Secondary Metrics
- ë‹¤ë¥¸ ë„ë©”ì¸ TPR@FPR=1%: ìœ ì§€ í™•ì¸
- ë„ë©”ì¸ë³„ í•™ìŠµëœ ê°€ì¤‘ì¹˜ ë¹„êµ

#### 5.3 ì„±ê³µ ê¸°ì¤€

| ì§€í‘œ | Method 5-A | ëª©í‘œ | íŒì • |
|------|------------|------|------|
| Domain C TPR@FPR=1% | 81.27% | 82%+ | Trade-off í•´ì†Œ |
| Domain B TPR@FPR=1% | 94.20% | 95%+ | íšŒë³µ |
| Domain A, D TPR@FPR=1% | ~94% | 93%+ | ìœ ì§€ |

### 6. ì˜ˆìƒ ê²°ê³¼ (ë…¼ë¬¸ ê¸°ì—¬ë„ ê´€ì )

#### 6.1 ë„ë©”ì¸ë³„ ê°€ì¤‘ì¹˜ í‘œ (í•µì‹¬ ê·¸ë¦¼)

ì˜ˆìƒ í•™ìŠµ ê²°ê³¼:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Domain  â”‚ Î±(scale0)   â”‚ Î±(scale1)   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚    A    â”‚ ~40-50%     â”‚ ~50-60%     â”‚
â”‚    B    â”‚ ~60-70%     â”‚ ~30-40%     â”‚  â† ì–•ì€ ìŠ¤ì¼€ì¼ ì„ í˜¸
â”‚    C    â”‚ ~25-35%     â”‚ ~65-75%     â”‚  â† ê¹Šì€ ìŠ¤ì¼€ì¼ ì„ í˜¸
â”‚    D    â”‚ ~45-55%     â”‚ ~45-55%     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

ë§Œì•½ Bì™€ Cì˜ ê°€ì¤‘ì¹˜ê°€ ë°˜ëŒ€ ë°©í–¥ì´ë©´:
â†’ **"ë„ë©”ì¸ë³„ ìµœì  ìŠ¤ì¼€ì¼ì´ ë‹¤ë¥´ë‹¤"** ê°€ì„¤ ì§ì ‘ ì…ì¦
â†’ ë…¼ë¬¸ ê¸°ì—¬ë„ ë§¤ìš° ë†’ìŒ

#### 6.2 í•µì‹¬ ê¸°ì—¬
1. **Trade-off í•´ì†Œ**: Global Î±ì˜ í•œê³„ ê·¹ë³µ
2. **ìµœì†Œ í™•ì¥ ë¹„ìš©**: íŒŒë¼ë¯¸í„° 2ê°œ â†’ 8ê°œ (6ê°œ ì¶”ê°€)
3. **ë„ë©”ì¸ ì •ë³´ í™œìš©**: ê¸°ì¡´ ë¼ë²¨ í™œìš©ìœ¼ë¡œ ì¶”ê°€ annotation ë¶ˆí•„ìš”

### 7. ì‹¤í—˜ ê²°ê³¼

> **ìƒíƒœ**: âœ… ì‹¤í—˜ ì™„ë£Œ (2025-12-23)

#### 7.1 ì‹¤í—˜ ì„¤ì •

| ì‹¤í—˜ ID | Timestamp | Seed | aux_loss_weight |
|---------|-----------|------|-----------------|
| Method5B-1 | 20251223_023405 | 42 | 0.01 |
| Method5B-2 | 20251223_023408 | 43 | 0.01 |
| Method5B-3 | 20251223_023412 | 44 | 0.01 |

#### 7.2 TPR@FPR=1% ê²°ê³¼

| Domain | Method 5-A | Method 5-B | Î” (pp) |
|--------|------------|------------|--------|
| A | 94.67% Â± 0.47% | 94.00% Â± 0.16% | **-0.67** |
| B | 94.20% Â± 0.33% | 94.73% Â± 0.41% | **+0.53** |
| C | 81.27% Â± 0.84% | 81.20% Â± 2.95% | **-0.07** |
| D | 93.27% Â± 0.50% | 92.53% Â± 0.34% | **-0.74** |

#### 7.3 í•™ìŠµëœ Domain-Conditional Scale Weights

**ì˜ˆìƒê³¼ ë‹¤ë¥¸ ê²°ê³¼**: ëª¨ë“  ë„ë©”ì¸ì´ ê±°ì˜ ë™ì¼í•œ ê°€ì¤‘ì¹˜ë¡œ ìˆ˜ë ´

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Domain  â”‚ Scale 0 (shallow)   â”‚ Scale 1 (deep)      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚    A    â”‚  35.9% Â± 0.01%      â”‚  64.1% Â± 0.01%      â”‚
â”‚    B    â”‚  35.9% Â± 0.05%      â”‚  64.1% Â± 0.05%      â”‚
â”‚    C    â”‚  36.2% Â± 0.06%      â”‚  63.8% Â± 0.06%      â”‚
â”‚    D    â”‚  36.0% Â± 0.01%      â”‚  64.0% Â± 0.01%      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ë¹„êµ**: Method 5-A (Global) = 34.78% / 65.22%

#### 7.4 ë¶„ì„

**í•µì‹¬ ë°œê²¬: ê°€ì¤‘ì¹˜ê°€ ë¶„í™”ë˜ì§€ ì•ŠìŒ**

1. **ì˜ˆìƒ**: Domain BëŠ” scale 0 ì„ í˜¸ (60-70%), Domain CëŠ” scale 1 ì„ í˜¸ (65-75%)
2. **ì‹¤ì œ**: ëª¨ë“  ë„ë©”ì¸ì´ ~36% / ~64%ë¡œ ìˆ˜ë ´ (Method 5-Aì™€ ê±°ì˜ ë™ì¼)

**ì›ì¸ ë¶„ì„**:
- `aux_loss_weight = 0.01`ì´ ë„ˆë¬´ ì‘ì•„ ë„ë©”ì¸ë³„ ì°¨ì´ë¥¼ ë§Œë“¤ê¸°ì— gradient ì‹ í˜¸ ë¶€ì¡±
- ë©€í‹°ë„ë©”ì¸ ë°°ì¹˜ì—ì„œ ë„ë©”ì¸ë³„ gradientê°€ í¬ì„ë¨
- ë˜ëŠ” ì‹¤ì œë¡œ ëª¨ë“  ë„ë©”ì¸ì—ì„œ ~36/64ê°€ ìµœì ì¼ ê°€ëŠ¥ì„±

#### 7.5 ê²°ë¡ 

| ì§€í‘œ | Method 5-A | Method 5-B | ëª©í‘œ | íŒì • |
|------|------------|------------|------|------|
| Domain C TPR@FPR=1% | 81.27% | 81.20% | 82%+ | âŒ ë³€í™” ì—†ìŒ |
| Domain B TPR@FPR=1% | 94.20% | 94.73% | 95%+ | âš ï¸ ì†Œí­ ê°œì„  (+0.53 pp) |
| ê°€ì¤‘ì¹˜ ë¶„í™” | - | ì—†ìŒ | ë¶„í™” ê¸°ëŒ€ | âŒ ë¯¸ë‹¬ì„± |

**Method 5-B (aux_loss_weight=0.01)ì€ ê°€ì„¤ì„ ì§€ì§€í•˜ì§€ ì•ŠìŒ**
â†’ aux_loss_weightë¥¼ 0.1ë¡œ ì¦ê°€ì‹œì¼œ ì¬ì‹¤í—˜ í•„ìš”

---

## Method 5-B v2: Stronger Auxiliary Loss

> **ìƒíƒœ**: ğŸ”„ êµ¬í˜„ ì™„ë£Œ, ì‹¤í—˜ ëŒ€ê¸° ì¤‘ (2025-12-23)

### 1. ë³€ê²½ ì‚¬í•­

Method 5-Bì˜ ê°€ì¤‘ì¹˜ê°€ ë¶„í™”ë˜ì§€ ì•Šì€ ì›ì¸: `aux_loss_weight`ê°€ ë„ˆë¬´ ì‘ìŒ (0.01)

**ìˆ˜ì •**: `aux_loss_weight`ë¥¼ 0.01 â†’ 0.1ë¡œ 10ë°° ì¦ê°€

### 2. í•µì‹¬ ê°€ì„¤

> aux_lossì˜ gradient ì‹ í˜¸ë¥¼ 10ë°° ê°•í™”í•˜ë©´,
> ë„ë©”ì¸ë³„ scale_logitsê°€ ì¶©ë¶„íˆ ë¶„í™”ë˜ì–´
> Domain B/Cì˜ ë°˜ëŒ€ ë°©í–¥ ê°€ì¤‘ì¹˜ê°€ í•™ìŠµë  ê²ƒì´ë‹¤.

### 3. êµ¬í˜„

`torch_model.py` ë³€ê²½:
```python
# Before
aux_loss = anomaly_map.mean()
return main_loss + 0.01 * aux_loss

# After
aux_loss = anomaly_map.mean()
return main_loss + self.aux_loss_weight * aux_loss  # default: 0.1
```

### 4. ì‹¤í—˜ ëª…ë ¹ì–´

```bash
# Seed 42
nohup python examples/notebooks/dinomaly_multiclass_baseline.py --mode multiclass --max-steps 3000 --seed 42 --gpu 0 --use-domain-conditional-scale-weights --aux-loss-weight 0.1 > logs/method5B_v2_seed42.log 2>&1 &

# Seed 43
nohup python examples/notebooks/dinomaly_multiclass_baseline.py --mode multiclass --max-steps 3000 --seed 43 --gpu 1 --use-domain-conditional-scale-weights --aux-loss-weight 0.1 > logs/method5B_v2_seed43.log 2>&1 &

# Seed 44
nohup python examples/notebooks/dinomaly_multiclass_baseline.py --mode multiclass --max-steps 3000 --seed 44 --gpu 2 --use-domain-conditional-scale-weights --aux-loss-weight 0.1 > logs/method5B_v2_seed44.log 2>&1 &
```

### 5. ì‹¤í—˜ ê²°ê³¼

> **ìƒíƒœ**: âŒ ì‹¤íŒ¨ (2025-12-23 ì™„ë£Œ)

#### 5.1 TPR@FPR=1% ë¹„êµ

| Domain | v1 (0.01) | v2 (0.1) | Î” | íŒì • |
|--------|-----------|----------|---|------|
| A | 94.20% Â± 0.63% | 94.33% Â± 0.25% | +0.13 pp | âœ“ ìœ ì§€ |
| B | 94.73% Â± 1.30% | 94.27% Â± 0.94% | -0.46 pp | âš ï¸ ì†Œí­ í•˜ë½ |
| **C** | **81.20% Â± 2.95%** | **78.20% Â± 0.85%** | **-3.00 pp** | âŒ ì•…í™” |
| D | 93.87% Â± 1.40% | 92.87% Â± 0.81% | -1.00 pp | âš ï¸ ì†Œí­ í•˜ë½ |

#### 5.2 Scale Weights ë¶„ì„

| Domain | v1 (0.01) | v2 (0.1) |
|--------|-----------|----------|
| A | 36.4% / 63.6% | 35.8% / 64.2% |
| B | 35.9% / 64.1% | 35.9% / 64.1% |
| C | 36.6% / 63.4% | 36.1% / 63.9% |
| D | 36.0% / 64.0% | 36.0% / 64.0% |

**ê°€ì¤‘ì¹˜ ë¶„í™” ì—†ìŒ**: ëª¨ë“  ë„ë©”ì¸ì´ ì—¬ì „íˆ ~36%/64%ë¡œ ìˆ˜ë ´

#### 5.3 ì›ì¸ ë¶„ì„

1. **aux_loss ê³¼ë„**: 0.1 weightê°€ ë„ˆë¬´ ê°•í•´ì„œ main reconstruction loss í•™ìŠµì„ ë°©í•´
2. **ëª©í‘œ ì¶©ëŒ**: anomaly score ìµœì†Œí™”ì— ì§‘ì¤‘í•˜ì—¬ feature reconstruction í’ˆì§ˆ ì €í•˜
3. **ê°€ì¤‘ì¹˜ ë¶„í™” ì‹¤íŒ¨**: 10ë°° ê°•í™”í•´ë„ ë„ë©”ì¸ë³„ gradient ì°¨ì´ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŒ

#### 5.4 ê²°ë¡ 

| ëª©í‘œ | v1 (0.01) | v2 (0.1) | íŒì • |
|------|-----------|----------|------|
| Domain C â‰¥ 82% | 81.20% | 78.20% | âŒ ì•…í™” |
| Domain B â‰¥ 95% | 94.73% | 94.27% | âŒ ì•…í™” |
| ê°€ì¤‘ì¹˜ ë¶„í™” | ì—†ìŒ | ì—†ìŒ | âŒ ë¯¸ë‹¬ì„± |

**Method 5-B (Domain-Conditional Scale Weights) ì ‘ê·¼ë²• ì‹¤íŒ¨**
- aux_loss_weight ì¡°ì •ìœ¼ë¡œëŠ” ë„ë©”ì¸ë³„ ê°€ì¤‘ì¹˜ ë¶„í™” ë¶ˆê°€
- ê°•í•œ aux_lossëŠ” ì˜¤íˆë ¤ ì„±ëŠ¥ ì•…í™” ìœ ë°œ

---

## Method 5 ì‹œë¦¬ì¦ˆ ìµœì¢… ìš”ì•½

| Method | ì„¤ëª… | Domain C | Domain B | íŒì • |
|--------|------|----------|----------|------|
| Baseline | Fixed 0.5/0.5 weights | 79.07% | 95.87% | ê¸°ì¤€ì„  |
| 5-A | Global learnable weights | 81.27% | 94.20% | âš ï¸ Câ†‘, Bâ†“ |
| 5-B v1 | Domain-conditional (0.01) | 81.20% | 94.73% | â†’ ë³€í™” ì—†ìŒ |
| 5-B v2 | Domain-conditional (0.1) | 78.20% | 94.27% | âŒ ì•…í™” |

**ê²°ë¡ **: Learnable scale weights ì ‘ê·¼ë²•ì˜ í•œê³„ ë„ë‹¬. ìƒˆë¡œìš´ ë°©í–¥ í•„ìš”.

### ê¶Œì¥ ë‹¤ìŒ ë°©í–¥

1. **Domain-specific fine-tuning**: Multi-class ëª¨ë¸ì„ domainë³„ë¡œ fine-tune
2. **Explicit domain conditioning**: Domain embeddingì„ decoderì— ì£¼ì…
3. **Ensemble approach**: Domainë³„ ì „ë¬¸ ëª¨ë¸ + ì•™ìƒë¸”
4. **Feature-level analysis**: Domain Cì˜ fault íŠ¹ì„± ì‹¬ì¸µ ë¶„ì„

---

## Method 5-B ì§„ë‹¨: Gradient Flow ë¶„ì„

> **ë‚ ì§œ**: 2025-12-23
> **ëª©ì **: Method 5-B ê°€ì¤‘ì¹˜ê°€ ë¶„í™”ë˜ì§€ ì•ŠëŠ” ê·¼ë³¸ ì›ì¸ íŒŒì•…

### Check 1: Gradient Norm ì¸¡ì •

**ë°°ê²½**: scale_logitsê°€ 500 steps í›„ì—ë„ 0.5/0.5ì—ì„œ ë³€í•˜ì§€ ì•ŠìŒ

**ë°©ë²•**: `on_after_backward()` hookì—ì„œ ë„ë©”ì¸ë³„ gradient norm ì¸¡ì •

**ê²°ê³¼** (aux_loss_weight=0.01, mean() ë°©ì‹):
```
grad_norm_domain0: 1.62e-06
grad_norm_domain1: 2.94e-06
grad_norm_domain2: 2.98e-06
grad_norm_domain3: 5.64e-06
grad_norm_total:   7.00e-06

Scale Weights: ëª¨ë“  ë„ë©”ì¸ 50.00% / 50.00% (ì´ˆê¸°í™” ê·¸ëŒ€ë¡œ)
```

**ë¶„ì„**:
- Gradientê°€ ~1e-6 ìˆ˜ì¤€ â†’ ì‚¬ì‹¤ìƒ gradient ì—†ìŒ
- lr=0.02 Ã— grad=1e-6 = 2e-8 per step
- 3000 steps í›„ì—ë„ weight ë³€í™” ~6e-5 (ë¬´ì‹œí•  ìˆ˜ì¤€)

**ê²°ë¡ **: `aux_loss = anomaly_map.mean()` ë°©ì‹ì€ gradientê°€ ë„ˆë¬´ ì•½í•¨

---

### Check 2: Tail-based Aux Loss (top-5%)

**ê°€ì„¤**: mean() ëŒ€ì‹  top-k%ë¥¼ ì‚¬ìš©í•˜ë©´ gradientê°€ ê°•í™”ë  ê²ƒ

**êµ¬í˜„**:
```python
# Before (mean)
aux_loss = anomaly_map.mean()

# After (top-5%)
anomaly_flat = anomaly_map.flatten(1)  # [B, H*W]
k = int(anomaly_flat.shape[1] * 0.05)  # top 5%
top_values, _ = torch.topk(anomaly_flat, k, dim=1)
aux_loss = top_values.mean()
```

**ê²°ê³¼** (aux_loss_weight=0.1, top-5%):
```
         | mean() (old) | top-5% (new) | ì¦ê°€ë°°ìœ¨ |
-------------------------------------------------
domain0  | 1.62e-06     | 5.01e-05     | 31x      |
domain1  | 2.94e-06     | 2.43e-05     | 8x       |
domain2  | 2.98e-06     | 4.39e-05     | 15x      |
domain3  | 5.64e-06     | 8.09e-05     | 14x      |
-------------------------------------------------

Scale Weights: ì—¬ì „íˆ 50.00% / 50.00% (ë³€í™” ì—†ìŒ)
```

**ë¶„ì„**:
- Gradient 10-30ë°° ì¦ê°€ ì„±ê³µ
- í•˜ì§€ë§Œ ì—¬ì „íˆ ~5e-5 ìˆ˜ì¤€ â†’ lr=0.02 Ã— grad=5e-5 = 1e-6 per step
- 3000 steps í›„ì—ë„ weight ë³€í™” ~0.003 (ì—¬ì „íˆ ë¶ˆì¶©ë¶„)

---

### ì§„ë‹¨ ìµœì¢… ê²°ë¡ 

| ë°©ì‹ | Gradient Norm | Weight ë³€í™” | íŒì • |
|------|---------------|-------------|------|
| mean() (0.01) | ~1e-6 | ì—†ìŒ | âŒ |
| top-5% (0.1) | ~5e-5 (10-30xâ†‘) | ì—†ìŒ | âŒ |

**ê·¼ë³¸ ì›ì¸**: Auxiliary loss ì ‘ê·¼ë²•ì˜ êµ¬ì¡°ì  í•œê³„
1. anomaly_map ê°’ ìì²´ê°€ ì‘ìŒ (cosine distance ~0.1)
2. softmaxë¥¼ í†µê³¼í•˜ë©´ì„œ gradientê°€ ì¶”ê°€ í¬ì„
3. main reconstruction loss (~0.98) >> aux_loss contribution (~0.01)

**ê²°ë¡ **: Auxiliary lossë¡œ scale_logitsë¥¼ í•™ìŠµì‹œí‚¤ëŠ” ì ‘ê·¼ë²•ì€ **gradient ì‹ í˜¸ê°€ êµ¬ì¡°ì ìœ¼ë¡œ ë¶ˆì¶©ë¶„**

---

## Method 6: Scale-wise Reconstruction Loss Weighting

> **ë‚ ì§œ**: 2025-12-23
> **ë°°ê²½**: Method 5-B ì§„ë‹¨ ê²°ê³¼, auxiliary loss ì ‘ê·¼ë²•ì˜ gradientê°€ êµ¬ì¡°ì ìœ¼ë¡œ ë¶ˆì¶©ë¶„í•¨ í™•ì¸

### ë¬¸ì œ ì •ì˜

Method 5-Bì˜ ì‹¤íŒ¨ ì›ì¸ ë¶„ì„:
1. **Gradient ê²½ë¡œ ë¬¸ì œ**: anomaly_map â†’ scale_weights ê²½ë¡œì˜ gradientê°€ ~1e-6ìœ¼ë¡œ ë„ˆë¬´ ì•½í•¨
2. **Main loss ëŒ€ë¹„ ê¸°ì—¬ë„**: reconstruction loss (~0.98) >> aux_loss (~0.01)
3. **êµ¬ì¡°ì  í•œê³„**: auxiliary lossë¥¼ ì•„ë¬´ë¦¬ ê°•í™”í•´ë„ main lossë¥¼ í†µí•œ gradientì— ë¹„í•´ ë¯¸ë¯¸

**í•´ê²° ë°©í–¥**: scale_weightsë¥¼ **reconstruction loss ìì²´**ì— ì ìš©í•˜ì—¬ main gradientë¥¼ í™œìš©

### í•µì‹¬ ê°€ì„¤

> "per-scale reconstruction lossì— ì§ì ‘ learnable weightsë¥¼ ì ìš©í•˜ë©´,
> main loss gradientê°€ ì§ì ‘ scale_logitsë¡œ íë¥´ë©´ì„œ ì˜ë¯¸ ìˆëŠ” í•™ìŠµì´ ê°€ëŠ¥í•  ê²ƒì´ë‹¤."

**ê¸°ì¡´ ë°©ì‹ (Baseline)**:
```
L = (â„“_0 + â„“_1) / 2    # ë‹¨ìˆœ í‰ê· 
```

**Method 6 ë°©ì‹**:
```
Î± = softmax(w)         # wëŠ” learnable parameter
L = Î±_0 * â„“_0 + Î±_1 * â„“_1   # ê°€ì¤‘ í•©
```

### Gradient Flow ë¹„êµ

| ë°©ì‹ | Gradient ê²½ë¡œ | ì˜ˆìƒ Gradient Norm |
|------|--------------|-------------------|
| Method 5-B | main_loss â†’ decoder â†’ anomaly_map â†’ aux_loss â†’ scale_logits | ~1e-6 (ì•½í•¨) |
| **Method 6** | main_loss â†’ **ì§ì ‘** â†’ scale_logits | ~1e-2 (ê°•í•¨) |

### Method 6-A: Global Î± (Domain-Agnostic)

**ì„¤ê³„**:
- ëª¨ë“  ë„ë©”ì¸ì—ì„œ ë™ì¼í•œ learnable weights ê³µìœ 
- softmaxë¡œ ê°€ì¤‘ì¹˜ í•©ì´ 1ì´ ë˜ë„ë¡ ì œì•½
- ì´ˆê¸°ê°’: w = [0, 0] â†’ Î± = [0.5, 0.5]

**êµ¬í˜„ ë‚´ìš©**:

1. **`loss.py`**: `return_per_scale` íŒŒë¼ë¯¸í„° ì¶”ê°€
```python
def forward(self, ..., return_per_scale: bool = False):
    per_scale_losses = []
    for item in range(len(encoder_features)):
        scale_loss = torch.mean(1 - cos_loss(...))
        loss += scale_loss
        per_scale_losses.append(scale_loss)

    if return_per_scale:
        return averaged_loss, per_scale_losses
    return averaged_loss
```

2. **`torch_model.py`**: learnable scale_loss_logits ì¶”ê°€
```python
def __init__(self, ..., use_scalewise_loss_weighting: bool = False):
    if use_scalewise_loss_weighting:
        num_scales = len(fuse_layer_encoder)  # 2
        self.scale_loss_logits = nn.Parameter(torch.zeros(num_scales))

def forward(self, batch, global_step):
    if self.use_scalewise_loss_weighting:
        _, per_scale_losses = self.loss_fn(..., return_per_scale=True)
        scale_weights = F.softmax(self.scale_loss_logits, dim=0)
        weighted_loss = sum(w * l for w, l in zip(scale_weights, per_scale_losses))
        return {"loss": weighted_loss, "scale_weights": scale_weights, ...}
```

3. **`lightning_model.py`**: ë¡œê¹… ë° optimizer ì„¤ì •
```python
# training_stepì—ì„œ ë¡œê¹…
self.log(f"scale_{i}_weight", w.item(), ...)
self.log(f"scale_{i}_loss", l.item(), ...)

# optimizerì— scale_loss_logits í¬í•¨
optimizer = StableAdamW([
    {"params": self.trainable_modules.parameters()},
    {"params": [self.model.scale_loss_logits]},
], ...)
```

### ì‹¤í—˜ ê³„íš

**ì‹¤í—˜ ì„¤ì •**:
- Baseline: Method 5 ì§„ë‹¨ì— ì‚¬ìš©í•œ baseline (fixed 0.5/0.5 weights)
- Method 6-A: Global learnable scale weights on reconstruction loss
- Seeds: 42, 123, 456
- Max steps: 10000
- í‰ê°€ ì§€í‘œ: TPR@FPR=1%, Domainë³„ AUROC

**ì„±ê³µ ê¸°ì¤€**:
| ì§€í‘œ | í˜„ì¬ Baseline | ëª©í‘œ |
|------|--------------|------|
| Domain C | 79.07% | â‰¥ 81% |
| Domain B | 95.87% | â‰¥ 95% (ìœ ì§€) |
| ê°€ì¤‘ì¹˜ ë¶„í™” | 50%/50% | ì˜ë¯¸ìˆëŠ” ë³€í™” |

**ëª¨ë‹ˆí„°ë§ í•­ëª©**:
- `scale_0_weight`, `scale_1_weight`: í•™ìŠµ ì¤‘ ê°€ì¤‘ì¹˜ ë³€í™”
- `scale_0_loss`, `scale_1_loss`: ê° ìŠ¤ì¼€ì¼ë³„ reconstruction loss
- Gradient norm ë³€í™” (í•„ìš”ì‹œ)

### ì‹¤í—˜ ëª…ë ¹ì–´

```bash
# ë¡œê·¸ ë””ë ‰í† ë¦¬ í™•ì¸
mkdir -p /mnt/ex-disk/taewan.hwang/study/anomalib/logs

# Method 6-A ì‹¤í—˜ (seed 42)
nohup python examples/notebooks/dinomaly_multiclass_baseline.py \
    --mode multiclass \
    --max-steps 10000 \
    --batch-size 16 \
    --gpu 0 \
    --seed 42 \
    --use-scalewise-loss-weighting \
    > /mnt/ex-disk/taewan.hwang/study/anomalib/logs/method6a_seed42.log 2>&1 &

# Method 6-A ì‹¤í—˜ (seed 123)
nohup python examples/notebooks/dinomaly_multiclass_baseline.py \
    --mode multiclass \
    --max-steps 10000 \
    --batch-size 16 \
    --gpu 1 \
    --seed 123 \
    --use-scalewise-loss-weighting \
    > /mnt/ex-disk/taewan.hwang/study/anomalib/logs/method6a_seed123.log 2>&1 &

# Method 6-A ì‹¤í—˜ (seed 456)
nohup python examples/notebooks/dinomaly_multiclass_baseline.py \
    --mode multiclass \
    --max-steps 10000 \
    --batch-size 16 \
    --gpu 2 \
    --seed 456 \
    --use-scalewise-loss-weighting \
    > /mnt/ex-disk/taewan.hwang/study/anomalib/logs/method6a_seed456.log 2>&1 &
```

### ì‹¤í—˜ ê²°ê³¼

> **ìƒíƒœ**: ì‹¤í—˜ ëŒ€ê¸° ì¤‘

| Seed | Domain A | Domain B | Domain C | Domain D | Mean | Î±_0 | Î±_1 |
|------|----------|----------|----------|----------|------|-----|-----|
| 42 | - | - | - | - | - | - | - |
| 123 | - | - | - | - | - | - | - |
| 456 | - | - | - | - | - | - | - |
| **Mean** | - | - | - | - | - | - | - |
| **Std** | - | - | - | - | - | - | - |

---

## References

- Original Paper: "Dinomaly: An Effective Reconstruction-Based Anomaly Detection"
- Original Repo: https://github.com/guojiajeremy/Dinomaly
- Key file: `dinomaly_mvtec_uni.py` (Multi-class unified training)
- GeM Pooling: "Fine-tuning CNN Image Retrieval with No Human Annotation" (Radenovic et al., 2018)
- Hard Normal Mining: Bootstrap frequency-based stable set identification
- Multi-scale Feature Fusion: Learnable aggregation for anomaly detection
