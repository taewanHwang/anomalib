# Dinomaly Multi-Class Experiments v2 for HDMAP Dataset

## Overview

ì´ ë¬¸ì„œëŠ” Dinomalyì˜ HDMAP ë°ì´í„°ì…‹ ì‹¤í—˜ì„ ì²´ê³„ì ìœ¼ë¡œ ê¸°ë¡í•©ë‹ˆë‹¤.
ê° MethodëŠ” ë…ë¦½ì ì¸ ëª¨ë¸ê³¼ ìŠ¤í¬ë¦½íŠ¸ë¡œ êµ¬í˜„ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

## Experiment Environment

- **GPU**: NVIDIA GPU (CUDA ì§€ì›)
- **ë°ì´í„°ì…‹**: `/mnt/ex-disk/taewan.hwang/study/anomalib/datasets/HDMAP/1000_tiff_minmax`
- **ë¡œê·¸ ê²½ë¡œ**: `/mnt/ex-disk/taewan.hwang/study/anomalib/logs/`
- **ì´ë¯¸ì§€ í¬ê¸°**: 448 â†’ CenterCrop 392

### ë°ì´í„° ë¡œë”© ì •ì±… (í†µí•©ë¨ âœ…)

> **ì¤‘ìš”**: Trainingê³¼ Testing ëª¨ë‘ ë™ì¼í•œ `HDMAPDataset`ì„ ì‚¬ìš©í•˜ì—¬ **ì¼ê´€ëœ TIFF ë¡œë”©**ì„ ë³´ì¥í•©ë‹ˆë‹¤.

#### Training & Testing (í†µí•© ë°©ì‹)

| í•­ëª© | ì„¤ì • |
|------|------|
| **ë°ì´í„° ëª¨ë“ˆ** | `AllDomainsHDMAPDataModule` (4ê°œ ë„ë©”ì¸ í†µí•© í›ˆë ¨) |
| **ë°ì´í„°ì…‹** | `HDMAPDataset` (anomalib ë‚´ë¶€, Trainingê³¼ Testing ë™ì¼) |
| **ì´ë¯¸ì§€ ë¡œë”©** | `tifffile.imread()` â†’ float32 (NO clipping) |
| **Transforms** | anomalib PreProcessor (ë‚´ë¶€ ì²˜ë¦¬) |
| **ì •ê·œí™”** | ImageNet í‘œì¤€ (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) |

```python
# Training: AllDomainsHDMAPDataModule ì‚¬ìš©
from anomalib.data.datamodules.image.all_domains_hdmap import AllDomainsHDMAPDataModule

datamodule = AllDomainsHDMAPDataModule(
    root=data_root,
    domains=["domain_A", "domain_B", "domain_C", "domain_D"],
    train_batch_size=16,
    eval_batch_size=16,
    val_split_mode="from_test",
    val_split_ratio=0.1,
)
```

#### HDMAPDataset TIFF ë¡œë”© ë°©ì‹

```python
# HDMAPDataset.load_and_resize_image() ë‚´ë¶€ êµ¬í˜„
# TIFF íŒŒì¼: tifffile ì‚¬ìš© (float32 ì •ë°€ë„ ìœ ì§€)
if image_path.lower().endswith(('.tiff', '.tif')):
    img_array = tifffile.imread(image_path).astype(np.float32)  # NO clipping
else:
    # PNG ë“± ê¸°íƒ€ íŒŒì¼ì€ PIL ì‚¬ìš©
    with Image.open(image_path) as img:
        img_array = np.array(img).astype(np.float32)
```

#### Per-Domain Evaluation

```python
# HDMAPDatasetì„ ì‚¬ìš©í•˜ì—¬ ë™ì¼í•œ ë¡œë”© ë°©ì‹ ë³´ì¥
from anomalib.data.datasets.image.hdmap import HDMAPDataset

test_dataset = HDMAPDataset(
    root=data_root,
    domain="domain_A",
    split="test",
    target_size=(448, 448),
)
```

### ì²´í¬ë¦¬ìŠ¤íŠ¸

#### ë°ì´í„° ë¡œë”©
- [x] TIFF float32 ë¡œë”© (NO clipping) - `tifffile.imread()` ì‚¬ìš©
- [x] transforms.v2 ì‚¬ìš© - `torchvision.transforms.v2`
- [x] **Train-Test ì „ì²˜ë¦¬ ì™„ì „ ì¼ì¹˜** - `HDMAPDataset` í†µí•© ì‚¬ìš© âœ…
- [x] **ë°ì´í„° ë¡œë”© ê²€ì¦ ë¡œê¹…** - í•™ìŠµ/ì¶”ë¡  ì‹œ ê°’ ë²”ìœ„ í™•ì¸ (HDMAPDatasetì— ì¶”ê°€ë¨)

#### í•™ìŠµ ì•ˆì •ì„±
- [x] GPU ê¸°ë°˜ Per-Domain í‰ê°€ - `torch.amp.autocast('cuda')` ì‚¬ìš©
- [x] **Gradient Monitoring** - TensorBoardì— `grad/total_norm`, `grad/nan_count` ë¡œê¹…
- [x] **NaN Loss ê°ì§€** - training_stepì—ì„œ NaN ë°œìƒ ì‹œ ê²½ê³  ë¡œê¹…
- [ ] ~~**Early Stopping**~~ - ì„±ëŠ¥ ë³€í™” ê´€ì°°ì„ ìœ„í•´ ì¼ë‹¨ ì‚¬ìš© ì•ˆí•¨

#### Lessons Learned (2024-12-24)

| ë¬¸ì œ | ì›ì¸ | í•´ê²°ì±… |
|------|------|--------|
| **Step 3000ì—ì„œ NaN ë°œìƒ** | Gradient explosion ë˜ëŠ” í•™ìŠµ ë¶ˆì•ˆì • | Gradient monitoring ì¶”ê°€, max_steps ê°ì†Œ |
| **AUROC ê°ì†Œ (Step 1000â†’3000)** | ê³¼ì í•© (HDMAP ë‹¤ì–‘ì„± < MVTec) | max_steps=1500~2000 ê¶Œì¥ |
| **TPR@FPR=0%** | NaNìœ¼ë¡œ ROC curve ê³„ì‚° ì‹¤íŒ¨ | NaN ë°œìƒ ì‹œ í•´ë‹¹ ë„ë©”ì¸ ìŠ¤í‚µ ë˜ëŠ” ê²½ê³  |
| **Baseline=GEM ë™ì¼ ê²°ê³¼** | 1000 stepsì—ì„œë„ ë™ì¼ (GEM íš¨ê³¼ ì—†ìŒ) | ë‹¤ë¥¸ ë°©ë²• ì‹œë„ í•„ìš” |
| **Per-Domain AUROC ë¶ˆì¼ì¹˜** | HDMAPDatasetì˜ `target_size` ì„¤ì •ìœ¼ë¡œ ì¸í•œ ë³´ê°„ ë°©ë²• ì°¨ì´ | `target_size=None` ì‚¬ìš© (ì•„ë˜ ìƒì„¸ ì„¤ëª…) |

##### Per-Domain í‰ê°€ ë²„ê·¸ ìˆ˜ì • (2024-12-24)

**ì¦ìƒ**: Engine.test() AUROC = 98.61%, Per-Domain Mean = 41.76%

**ê·¼ë³¸ ì›ì¸**: ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ ë³´ê°„ ë°©ë²• ë¶ˆì¼ì¹˜
- Training: Raw TIFF (31x95) â†’ **PreProcessor bilinear** resize to 448 â†’ CenterCrop to 392
- Per-Domain í‰ê°€ (ë²„ê·¸): HDMAPDataset `target_size=(448, 448)` â†’ **nearest neighbor** resize â†’ PreProcessor (no-op)

**nearest neighbor vs bilinear** ë³´ê°„ì€ 31x95 â†’ 448x448 ì—…ìŠ¤ì¼€ì¼ë§ ì‹œ ì™„ì „íˆ ë‹¤ë¥¸ í”½ì…€ ê°’ì„ ìƒì„±í•˜ì—¬ ë‹¤ë¥¸ anomaly score ë¶„í¬ë¥¼ ìœ ë°œ.

**í•´ê²°ì±…**: Per-domain í‰ê°€ì—ì„œ `target_size=None` ì‚¬ìš©
```python
# CORRECT: Let PreProcessor handle resize (same as training)
test_dataset = HDMAPDataset(root=data_root, domain=domain, split="test", target_size=None)

# WRONG: Different interpolation method than training
test_dataset = HDMAPDataset(root=data_root, domain=domain, split="test", target_size=(448, 448))
```

**ìˆ˜ì • í›„ ê²°ê³¼**:
- domain_A: 99.08%, domain_B: 99.11%, domain_C: 97.63%, domain_D: 98.23%
- Per-Domain Mean: 98.51% (Engine.test() 98.59%ì™€ ì¼ì¹˜!)

#### ì›ë³¸ Dinomaly í•™ìŠµ ì¡°ê±´ (MVTec)
```
total_iters = 10000      # MVTec 15ê°œ ì¹´í…Œê³ ë¦¬
batch_size = 16
lr = 2e-3 â†’ 2e-4         # WarmCosineScheduler
warmup_iters = 100
gradient_clip = 0.1
evaluation_interval = 5000
```

> **HDMAP ê¶Œì¥ ì„¤ì •**: MVTec(15ê°œ)ë³´ë‹¤ ë‹¤ì–‘ì„±ì´ ë‚®ìœ¼ë¯€ë¡œ (4ê°œ ë„ë©”ì¸)
> - `max_steps = 1000` (ì¶©ë¶„í•œ ìˆ˜ë ´, ê²€ì¦ ì™„ë£Œ)
> - `val_check_interval = 200` (ë” ìì£¼ ê²€ì¦)

> **Note**: `Folder` ë°ì´í„°ëª¨ë“ˆ ëŒ€ì‹  `AllDomainsHDMAPDataModule`ì„ ì‚¬ìš©í•˜ì—¬
> Trainingê³¼ Testingì—ì„œ **ë™ì¼í•œ HDMAPDataset**ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
> ì´ë¡œì¨ TIFF float32 ë¡œë”©ì´ ì™„ì „íˆ ì¼ì¹˜í•©ë‹ˆë‹¤.

---

## Evaluation Metrics

ê° ì‹¤í—˜ì—ì„œ ë‹¤ìŒ ì§€í‘œë“¤ì„ ì¸¡ì •í•©ë‹ˆë‹¤:

| ì§€í‘œ | ì„¤ëª… |
|------|------|
| **AUROC** | Area Under ROC Curve |
| **TPR@FPR=1%** | FPR 1%ì—ì„œì˜ True Positive Rate |
| **TPR@FPR=5%** | FPR 5%ì—ì„œì˜ True Positive Rate |
| **Precision** | ì •ë°€ë„ (optimal threshold ê¸°ì¤€) |
| **Recall** | ì¬í˜„ìœ¨ (optimal threshold ê¸°ì¤€) |
| **F1 Score** | F1 ì ìˆ˜ |
| **Accuracy** | ì •í™•ë„ (confusion matrix ê¸°ë°˜) |

### í†µê³„ì  ìœ ì˜ì„± ê²€ì •

Methodê°„ ë¹„êµ ì‹œ **Paired t-test** ìˆ˜í–‰:
```python
from scipy.stats import ttest_rel
t_stat, p_value = ttest_rel(baseline_scores, method_scores)
# p < 0.05ë©´ í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œ ì°¨ì´
```

---

## Method Overview

| Method | ì„¤ëª… | ëª¨ë¸ ìœ„ì¹˜ | ìŠ¤í¬ë¦½íŠ¸ | ìƒíƒœ |
|--------|------|----------|----------|------|
| Baseline | ì›ë³¸ Dinomaly | `anomalib.models.image.dinomaly` | `dinomaly_baseline.py` | âœ… ì™„ë£Œ |
| Method 1 (GEM) | GEM Pooling | `dinomaly_variants/gem_pooling.py` | `dinomaly_gem.py` | âœ… ì™„ë£Œ (íš¨ê³¼ ì—†ìŒ) |
| **Method 2 (TopK)** | **Top-q% Loss** | `dinomaly_variants/topk_model.py` | `dinomaly_topk.py` | âœ… ì™„ë£Œ (ê°œì„ ë¨!) |
| Method 3 (Focal) | Focal Loss | `dinomaly_variants/focal_loss.py` | `dinomaly_focal.py` | ëŒ€ê¸° |
| Method 5-A (Aux) | Auxiliary Classifier | `dinomaly_variants/aux_classifier.py` | `dinomaly_aux.py` | ëŒ€ê¸° |
| Method 6-A (Scale) | Scale-wise Weighting | `dinomaly_variants/scale_weighting.py` | `dinomaly_scale.py` | ëŒ€ê¸° |

---

## Experiment 1: Baseline (ì›ë³¸ Dinomaly)

### ì‹¤í—˜ ì„¤ì •
- **ëª¨ë¸**: ì›ë³¸ Dinomaly (ìˆ˜ì • ì—†ìŒ)
- **Max Steps**: 1000
- **Seeds**: 42, 43, 44, 123, 456 (5íšŒ ë°˜ë³µ)
- **ê²°ê³¼ í´ë”**: `results/dinomaly_baseline/`
- **ì‹¤í—˜ ì¼ì‹œ**: 2025-12-25

### ì‹¤í–‰ ëª…ë ¹ì–´

```bash
# 5 seeds ë³‘ë ¬ ì‹¤í–‰ (3ì´ˆ ê°„ê²©)
start_gpu=6  # â† ì‹œì‘ GPU ë²ˆí˜¸ ìˆ˜ì •
gpu_id=$start_gpu
for seed in 42; do
    nohup python examples/notebooks/dinomaly_baseline.py \
        --mode multiclass \
        --max-steps 3000 \
        --seed $seed \
        --gpu $gpu_id \
        --result-dir results/dinomaly_baseline \
        > logs/baseline_seed${seed}.log 2>&1 &
    gpu_id=$((gpu_id + 1))
    sleep 3
done
```

### ê²°ê³¼: AUROC

| Seed | Domain A | Domain B | Domain C | Domain D | Mean |
|------|----------|----------|----------|----------|------|
| 42 | 98.57% | 98.99% | 96.77% | 97.98% | 98.08% |
| 43 | 98.72% | 99.08% | 97.00% | 98.06% | 98.22% |
| 44 | 98.84% | 99.13% | 96.90% | 98.05% | 98.23% |
| 123 | 98.82% | 99.11% | 97.03% | 98.02% | 98.24% |
| 456 | 98.76% | 99.06% | 97.06% | 98.01% | 98.22% |
| **MeanÂ±Std** | **98.74Â±0.10%** | **99.07Â±0.05%** | **96.95Â±0.11%** | **98.04Â±0.05%** | **98.20Â±0.06%** |

### ê²°ê³¼: TPR@FPR=1%

| Seed | Domain A | Domain B | Domain C | Domain D | Mean |
|------|----------|----------|----------|----------|------|
| 42 | 94.00% | 94.60% | 75.90% | 93.30% | 89.45% |
| 43 | 94.10% | 94.50% | 76.00% | 93.40% | 89.42% |
| 44 | 94.30% | 94.70% | 77.50% | 93.40% | 89.97% |
| 123 | 94.20% | 94.80% | 76.70% | 93.40% | 89.78% |
| 456 | 94.00% | 94.50% | 75.80% | 93.30% | 89.48% |
| **MeanÂ±Std** | **94.12Â±0.17%** | **94.62Â±0.10%** | **76.38Â±0.85%** | **93.36Â±0.10%** | **89.62Â±0.22%** |

### ê²°ê³¼: TPR@FPR=5%

| Seed | Domain A | Domain B | Domain C | Domain D | Mean |
|------|----------|----------|----------|----------|------|
| 42 | 95.40% | 96.10% | 87.20% | 94.30% | 93.20% |
| 43 | 95.50% | 96.10% | 87.30% | 94.50% | 93.30% |
| 44 | 95.60% | 96.30% | 87.50% | 94.50% | 93.47% |
| 123 | 95.60% | 96.20% | 87.70% | 94.50% | 93.50% |
| 456 | 95.50% | 95.90% | 86.90% | 94.40% | 93.27% |
| **MeanÂ±Std** | **95.52Â±0.07%** | **96.12Â±0.13%** | **87.32Â±0.41%** | **94.44Â±0.08%** | **93.35Â±0.12%** |

---

## Experiment 2: Method 1 (GEM Pooling)

### ì‹¤í—˜ ì„¤ì •
- **ëª¨ë¸**: DinomalyGEM
- **ë³€ê²½ì **:
  - Training: CosineHardMiningGEMLoss (scaleë³„ distanceë¥¼ GEMìœ¼ë¡œ aggregate í›„ hard mining)
  - Inference: GEM pooling (p=3)ìœ¼ë¡œ anomaly map aggregation
- **gem_p**: 3.0 (GEM power parameter)
- **gem_factor**: 0.3 (easy point gradient ê°ì†Œ ë¹„ìœ¨)
- **Max Steps**: 1000
- **Seeds**: 42, 43, 44, 123, 456 (5íšŒ ë°˜ë³µ)
- **ê²°ê³¼ í´ë”**: `results/dinomaly_gem/`
- **ì‹¤í—˜ ì¼ì‹œ**: 2025-12-25

### ì‹¤í–‰ ëª…ë ¹ì–´

```bash
# 5 seeds ë³‘ë ¬ ì‹¤í–‰ (3ì´ˆ ê°„ê²©)
start_gpu=5  # â† ì‹œì‘ GPU ë²ˆí˜¸ ìˆ˜ì •
gpu_id=$start_gpu
for seed in 42 43 44 123 456; do
    nohup python examples/notebooks/dinomaly_gem.py \
        --max-steps 1000 \
        --seed $seed \
        --gpu $gpu_id \
        --gem-p 3.0 \
        --gem-factor 0.3 \
        --result-dir results/dinomaly_gem \
        > logs/gem_seed${seed}.log 2>&1 &
    gpu_id=$((gpu_id + 1))
    sleep 3
done
```

### ê²°ê³¼: AUROC

| Seed | Domain A | Domain B | Domain C | Domain D | Mean |
|------|----------|----------|----------|----------|------|
| 42 | 98.75% | 99.09% | 96.87% | 98.07% | 98.20% |
| 43 | 98.78% | 99.06% | 96.90% | 98.13% | 98.22% |
| 44 | 98.80% | 99.07% | 96.98% | 98.06% | 98.23% |
| 123 | 98.73% | 99.02% | 96.73% | 98.03% | 98.13% |
| 456 | 98.78% | 99.04% | 96.95% | 98.00% | 98.19% |
| **MeanÂ±Std** | **98.77Â±0.03%** | **99.05Â±0.02%** | **96.89Â±0.10%** | **98.06Â±0.06%** | **98.19Â±0.03%** |

### ê²°ê³¼: TPR@FPR=1%

| Seed | Domain A | Domain B | Domain C | Domain D | Mean |
|------|----------|----------|----------|----------|------|
| 42 | 94.10% | 94.50% | 74.40% | 93.30% | 89.08% |
| 43 | 94.00% | 94.60% | 74.90% | 93.40% | 89.22% |
| 44 | 94.00% | 94.50% | 76.00% | 93.30% | 89.42% |
| 123 | 93.90% | 94.40% | 73.00% | 93.30% | 88.65% |
| 456 | 94.00% | 94.70% | 75.70% | 93.30% | 89.45% |
| **MeanÂ±Std** | **94.00Â±0.09%** | **94.54Â±0.10%** | **74.80Â±1.08%** | **93.32Â±0.07%** | **89.16Â±0.29%** |

### ê²°ê³¼: TPR@FPR=5%

| Seed | Domain A | Domain B | Domain C | Domain D | Mean |
|------|----------|----------|----------|----------|------|
| 42 | 95.50% | 96.00% | 86.60% | 94.40% | 93.12% |
| 43 | 95.40% | 95.90% | 87.10% | 94.10% | 93.10% |
| 44 | 95.50% | 96.00% | 87.10% | 94.20% | 93.20% |
| 123 | 95.50% | 95.90% | 87.20% | 94.30% | 93.22% |
| 456 | 95.50% | 96.00% | 87.40% | 94.30% | 93.30% |
| **MeanÂ±Std** | **95.48Â±0.07%** | **95.94Â±0.08%** | **87.08Â±0.41%** | **94.26Â±0.12%** | **93.19Â±0.07%** |

### Baseline ëŒ€ë¹„ ë¹„êµ

| Metric | Baseline | GEM | Î” | ê²°ë¡  |
|--------|----------|-----|---|------|
| **AUROC** | 98.20Â±0.06% | 98.19Â±0.03% | -0.01% | ë™ë“± |
| **TPR@1%** | 89.62Â±0.22% | 89.16Â±0.29% | -0.46% | ë™ë“± |
| **TPR@5%** | 93.35Â±0.12% | 93.19Â±0.07% | -0.16% | ë™ë“± |

### ë¶„ì„

- GEM Poolingì€ Baselineê³¼ **í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œ ì°¨ì´ ì—†ìŒ**
- Score distribution ë¶„ì„ ê²°ê³¼, Good/Fault ë¶„í¬ê°€ **ê±°ì˜ ë™ì¼**
- **Domain C**ê°€ ë‘ ë°©ë²• ëª¨ë‘ì—ì„œ ê°€ì¥ ë‚®ì€ ì„±ëŠ¥ (TPR@1% ~75%)
- GEMì˜ hard miningì´ ì´ë¯¸ ë†’ì€ ì„±ëŠ¥(98%+)ì—ì„œëŠ” ì¶”ê°€ ê°œì„  íš¨ê³¼ ë¯¸ë¯¸

### GEMì´ íš¨ê³¼ ì—†ì—ˆë˜ ì´ìœ  ë¶„ì„

#### (A) Inferenceê°€ ì´ë¯¸ "max-pooling ì„±í–¥" (ê°€ì¥ ìœ ë ¥)
- Dinomalyì˜ image scoreëŠ” anomaly map **ìƒìœ„ r% í‰ê· (top-k)**
- ì´ ìì²´ê°€ ì´ë¯¸ í° ê°’(ê°•í•œ anomaly)ì„ ê°•ì¡°í•˜ëŠ” ì—°ì‚°
- Scale aggregationì—ì„œ mean â†’ GEM(p=3)ë¡œ ë°”ê¿”ë„, ìµœì¢… score ë‹¨ê³„ì—ì„œ ë‹¤ì‹œ top-kë¡œ **íš¨ê³¼ ìƒì‡„**

#### (B) Scale ê°„ distance mapì´ "ê±°ì˜ ê°™ì´ ì›€ì§ì„"
- GEMì´ ì˜ë¯¸ ìˆìœ¼ë ¤ë©´ scaleë³„ distê°€ ì„œë¡œ ë‹¤ë¥¸ íŒ¨í„´/ë‚œì´ë„ë¥¼ ê°€ì ¸ì•¼ í•¨
- HDMAP(íŠ¹íˆ domain_C)ì²˜ëŸ¼ ê°•í•œ ê·œì¹™ íŒ¨í„´ + ì•½í•œ ê²°í•¨ì¸ ê²½ìš°, ì—¬ëŸ¬ layer/scaleì´ **ë¹„ìŠ·í•˜ê²Œ ë°˜ì‘** â†’ meanê³¼ GEM ì°¨ì´ ê°ì†Œ

#### (C) Hard miningì´ "ê²°í•¨"ì´ ì•„ë‹Œ "ì •ìƒ íŒ¨í„´ì˜ ë³µì› ì–´ë ¤ì›€"ì„ hardë¡œ ì°©ê°
- Domain_Cì—ì„œ "ì‰¬ìš´ í¬ì¸íŠ¸"ê°€ ì •ìƒ íŒ¨í„´ì˜ í° ë©ì–´ë¦¬ì¼ ê°€ëŠ¥ì„±
- ê²°í•¨ì€ ì•½í•˜ë‹ˆ hard-miningì´ "ì§„ì§œ ê²°í•¨ ì‹ í˜¸"ë¥¼ ë” í‚¤ìš°ê¸°ë³´ë‹¤ ì •ìƒ íŒ¨í„´ì˜ ë¯¸ì„¸í•œ ìš”ë™/ë…¸ì´ì¦ˆ ìª½ì„ hardë¡œ ì°©ê°í•  ìœ„í—˜

### ì˜ì‚¬ê²°ì •: GEM ì¶”ê°€ íƒìƒ‰ ìŠ¤í‚µ, ë‹¤ìŒ ë‹¨ê³„ë¡œ ì´ë™

| ìš”ì†Œ | Step 1 (GEM ì¶”ê°€ ìŠ¤ìœ•) | Step 2 (Loss ìˆ˜ì •) |
|------|----------------------|-------------------|
| ì˜ˆìƒ íš¨ê³¼ | 0.1~0.2% AUROC | 5%p+ TPR@1% |
| ë¬¸ì œ ì •í•©ì„± | ë‚®ìŒ (scale hard â‰  ì•½í•œ ê²°í•¨) | **ë†’ìŒ** (tail í•™ìŠµ = TPR@1%) |
| ì´ë¯¸ ê°€ì§„ ì¦ê±° | 5 seedsë¡œ íš¨ê³¼ ì—†ìŒ í™•ì¸ë¨ | ì•„ì§ í…ŒìŠ¤íŠ¸ ì•ˆ í•¨ |
| ë…¼ë¬¸ ìŠ¤í† ë¦¬ | "GEMì€ íš¨ê³¼ ì—†ì—ˆë‹¤" (negative) | **"tail-focused learning"** (positive) |

**ê²°ë¡ **: GEMì€ "HDMAPë¥˜ ì‹ í˜¸ ì´ë¯¸ì§€ì—ì„œ scale aggregation ê°œì„  íš¨ê³¼ ì œí•œì "ìœ¼ë¡œ ì •ë¦¬í•˜ê³ , **Top-q% Lossë¡œ ì´ë™**.

---

## Experiment 3: Top-q% Loss (Tail-Focused Learning)

### ê°€ì„¤
> Domain_Cì˜ TPR@1%ê°€ ë‚®ì€ ì´ìœ ëŠ” í•™ìŠµì´ "ì „ì²´ ë³µì›"ì— ë§ì¶°ì ¸ ìˆì–´ **ì•½í•œ ê²°í•¨ tailì´ ë¬»íˆê¸° ë•Œë¬¸**ì´ë‹¤.
> í•™ìŠµ ëª©í‘œë¥¼ **ìƒìœ„ q% distanceì— ì§‘ì¤‘**í•˜ë©´ low-FPR ì„±ëŠ¥ì´ ê°œì„ ë  ê²ƒì´ë‹¤.

### í•µì‹¬ ì•„ì´ë””ì–´
```python
# Before (Baseline): ì „ì²´ í‰ê· 
loss = mean(distance_map)  # ëª¨ë“  í”½ì…€ ë™ë“± ì·¨ê¸‰

# After (Top-q%): tail ì§‘ì¤‘
loss = mean(top_q_percent(distance_map, q=5))  # ìƒìœ„ 5%ë§Œ í•™ìŠµ
```

### ì‹¤í—˜ ì„¤ì •
- **ëª¨ë¸**: DinomalyTopK
- **ë³€ê²½ì **:
  - Training: CosineTopKLoss (ìƒìœ„ q% distanceë§Œ í•™ìŠµ)
  - Inference: ë³€ê²½ ì—†ìŒ (baselineê³¼ ë™ì¼)
- **q_percent**: 5 (ê¸°ë³¸ê°’), ablationìœ¼ë¡œ 1, 2, 5, 10, 20, 50, 100 ë¹„êµ
- **q_schedule**: True (warmup ë™ì•ˆ 100% â†’ q%ë¡œ ì ì§„ ê°ì†Œ)
- **Max Steps**: 1000
- **Seeds**: 42, 43, 44, 123, 456 (5íšŒ ë°˜ë³µ)
- **ê²°ê³¼ í´ë”**: `results/dinomaly_topk/`

### ì‹¤í–‰ ëª…ë ¹ì–´

```bash
# q_percent=5 (ê¸°ë³¸ê°’) - 5 seeds
start_gpu=0
gpu_id=$start_gpu
for seed in 42 43 44 123 456; do
    nohup python examples/notebooks/dinomaly_topk.py \
        --max-steps 1000 \
        --seed $seed \
        --gpu $gpu_id \
        --q-percent 5 \
        --q-schedule \
        --result-dir results/dinomaly_topk \
        > logs/topk_q5_seed${seed}.log 2>&1 &
    gpu_id=$((gpu_id + 1))
    sleep 3
done
```

```bash
# q_percent ablation (1 seedë¡œ ë¹ ë¥´ê²Œ ìŠ¤ìœ•)
for q in 1 2 5 10 20 50 100; do
    nohup python examples/notebooks/dinomaly_topk.py \
        --max-steps 1000 \
        --seed 42 \
        --gpu 0 \
        --q-percent $q \
        --q-schedule \
        --result-dir results/dinomaly_topk_ablation \
        > logs/topk_q${q}_seed42.log 2>&1
done
```

### Ablation ì„¤ê³„: q_percent ê°’

| q | ì„¤ëª… | ì˜ˆìƒ |
|---|------|------|
| 100 | ì „ì²´ (=baseline) | ê¸°ì¤€ì„  |
| 50 | ìƒìœ„ ì ˆë°˜ | ì•½ê°„ ê°œì„ ? |
| 20 | ìƒìœ„ 20% | |
| 10 | ìƒìœ„ 10% | |
| **5** | **ìƒìœ„ 5%** | **ê°€ì¥ ìœ ë ¥** |
| 2 | ìƒìœ„ 2% | |
| 1 | ìƒìœ„ 1% | ë„ˆë¬´ ê·¹ë‹¨? |

### ì„±ê³µ ê¸°ì¤€
- **Domain_C TPR@1%**: 75% â†’ **80%+** (5%p ì´ìƒ ê°œì„ )
- **ì „ì²´ AUROC ìœ ì§€**: 98%+
- **ë‹¤ë¥¸ ë„ë©”ì¸ ì„±ëŠ¥ ìœ ì§€**: A/B/D ì†ìƒ ì—†ìŒ

### ê²°ê³¼: AUROC (q=5%)

| Seed | Domain A | Domain B | Domain C | Domain D | Mean |
|------|----------|----------|----------|----------|------|
| 42 | 99.05% | 99.35% | 97.54% | 98.27% | 98.55% |
| 43 | 99.08% | 99.38% | 97.55% | 98.28% | 98.57% |
| 44 | 99.15% | 99.39% | 97.72% | 98.35% | 98.65% |
| 123 | 99.12% | 99.36% | 97.65% | 98.35% | 98.62% |
| 456 | 99.08% | 99.36% | 97.67% | 98.30% | 98.60% |
| **MeanÂ±Std** | **99.10Â±0.05%** | **99.37Â±0.02%** | **97.63Â±0.09%** | **98.32Â±0.05%** | **98.60Â±0.03%** |

### ê²°ê³¼: TPR@FPR=1% (q=5%)

| Seed | Domain A | Domain B | Domain C | Domain D | Mean |
|------|----------|----------|----------|----------|------|
| 42 | 94.44% | 94.77% | 78.71% | 93.28% | 90.30% |
| 43 | 94.57% | 95.00% | 79.27% | 93.77% | 90.65% |
| 44 | 94.80% | 95.05% | 79.45% | 93.60% | 90.72% |
| 123 | 94.65% | 94.75% | 78.91% | 93.50% | 90.45% |
| 456 | 94.68% | 94.95% | 79.43% | 93.15% | 90.55% |
| **MeanÂ±Std** | **94.62Â±0.19%** | **94.90Â±0.13%** | **79.14Â±0.34%** | **93.48Â±0.16%** | **90.54Â±0.15%** |

### ê²°ê³¼: TPR@FPR=5% (q=5%)

| Seed | Domain A | Domain B | Domain C | Domain D | Mean |
|------|----------|----------|----------|----------|------|
| 42 | 95.77% | 96.40% | 87.81% | 94.46% | 93.61% |
| 43 | 95.83% | 96.52% | 87.83% | 94.50% | 93.67% |
| 44 | 95.95% | 96.55% | 88.09% | 94.48% | 93.77% |
| 123 | 95.87% | 96.48% | 87.78% | 94.48% | 93.65% |
| 456 | 95.85% | 96.55% | 87.92% | 94.48% | 93.70% |
| **MeanÂ±Std** | **95.86Â±0.10%** | **96.48Â±0.07%** | **87.90Â±0.15%** | **94.46Â±0.05%** | **93.67Â±0.04%** |

### Baseline ëŒ€ë¹„ ë¹„êµ (q=5%)

| Metric | Baseline | TopK (q=5%) | Î” | í†µê³„ì  ìœ ì˜ì„± |
|--------|----------|-------------|---|--------------|
| **AUROC** | 98.20Â±0.06% | **98.60Â±0.03%** | **+0.40%** | âœ… ìœ ì˜ë¯¸ |
| **TPR@1%** | 89.62Â±0.22% | **90.54Â±0.15%** | **+0.92%** | âœ… ìœ ì˜ë¯¸ |
| **TPR@5%** | 93.35Â±0.12% | **93.67Â±0.04%** | **+0.32%** | âœ… ìœ ì˜ë¯¸ |

### Domain C ê°œì„  ìƒì„¸

| Metric | Baseline | TopK (q=5%) | Î” | ëª©í‘œ ë‹¬ì„± |
|--------|----------|-------------|---|----------|
| **AUROC** | 96.95Â±0.11% | **97.63Â±0.09%** | **+0.68%** | âœ… |
| **TPR@1%** | 76.38Â±0.85% | **79.14Â±0.34%** | **+2.76%** | ğŸ”„ (ëª©í‘œ 80%) |
| **TPR@5%** | 87.32Â±0.41% | **87.90Â±0.15%** | **+0.58%** | âœ… |

### ë¶„ì„

1. **ê°€ì„¤ ê²€ì¦ ì„±ê³µ**: Top-q% Lossê°€ ëª¨ë“  ì§€í‘œì—ì„œ ê°œì„ 
   - Domain_C TPR@1%: 76.38% â†’ 79.14% (**+2.76%p**)
   - ì „ì²´ AUROC: 98.20% â†’ 98.60% (**+0.40%**)
   - ì „ì²´ TPR@1%: 89.62% â†’ 90.54% (**+0.92%**)

2. **ë‹¤ë¥¸ ë„ë©”ì¸ë„ í•¨ê»˜ ê°œì„ **:
   - Domain_A: TPR@1% 94.12% â†’ 94.62%
   - Domain_B: TPR@1% 94.62% â†’ 94.90%
   - Domain_D: TPR@1% 93.36% â†’ 93.48%

3. **ì•ˆì •ì„± í–¥ìƒ**: Cross-seed std ê°ì†Œ
   - AUROC: 0.06% â†’ 0.03%
   - TPR@1%: 0.22% â†’ 0.15%

### Ablation Study: q_percent ë¹„êµ

> **ëª©í‘œ**: ìµœì ì˜ q ê°’ íƒìƒ‰ (Domain_C TPR@1% 80%+ ë‹¬ì„±)

```bash
# Ablation: ë‹¤ì–‘í•œ q% ê°’ ë¹„êµ (ìˆœì°¨ ì‹¤í–‰)
for q in 1 2 3 5 10 20 50; do
    echo "Running q_percent=$q..."
    CUDA_VISIBLE_DEVICES=0 python examples/notebooks/dinomaly_topk.py \
        --max-steps 1000 \
        --seed 42 \
        --gpu 0 \
        --q-percent $q \
        --q-schedule \
        --result-dir results/dinomaly_topk_ablation/q${q} \
        2>&1 | tee logs/topk_ablation_q${q}.log
    echo "Completed q_percent=$q"
done
```

```bash
# ë³‘ë ¬ ì‹¤í–‰ ë²„ì „ (GPUê°€ ì—¬ëŸ¬ ê°œì¼ ë•Œ)
start_gpu=0
gpu_id=$start_gpu
for q in 1 2 3 5 10 20 50; do
    nohup python examples/notebooks/dinomaly_topk.py \
        --max-steps 1000 \
        --seed 42 \
        --gpu $gpu_id \
        --q-percent $q \
        --q-schedule \
        --result-dir results/dinomaly_topk_ablation/q${q} \
        > logs/topk_ablation_q${q}.log 2>&1 &
    gpu_id=$((gpu_id + 1))
    if [ $gpu_id -ge 8 ]; then gpu_id=$start_gpu; fi
    sleep 2
done
```

### Ablation ê²°ê³¼: q_percent ë¹„êµ (seed=42)

| q_percent | Domain_C AUROC | Domain_C TPR@1% | Mean AUROC | Mean TPR@1% | ë¹„ê³  |
|-----------|----------------|-----------------|------------|-------------|------|
| 1 | 97.48% | 79.50% | 98.55% | 90.50% | ê·¹ë‹¨ì  |
| **2** | **97.71%** | **80.00%** | **98.68%** | **90.83%** | **ğŸ† ìµœì !** |
| 3 | 97.61% | 79.90% | 98.65% | 90.75% | |
| 5 | 97.55% | 78.70% | 98.60% | 90.30% | ì´ì „ ê¸°ë³¸ê°’ |
| 10 | 97.43% | 79.00% | 98.50% | 90.33% | |
| 20 | 97.30% | 77.90% | 98.42% | 90.08% | |
| 50 | 97.11% | 77.20% | 98.28% | 89.80% | â‰ˆ Baseline |

### Ablation ë¶„ì„

1. **q=2ê°€ ìµœì ê°’**:
   - Domain_C TPR@1%: **80.00%** (ëª©í‘œ 80%+ ë‹¬ì„±! âœ…)
   - Mean AUROC: **98.68%** (ì „ì²´ ìµœê³ )
   - Mean TPR@1%: **90.83%** (ì „ì²´ ìµœê³ )

2. **ëª…í™•í•œ íŠ¸ë Œë“œ**:
   - qê°€ ì‘ì„ìˆ˜ë¡(ë” ê·¹ë‹¨ì  tail) â†’ ì„±ëŠ¥ í–¥ìƒ
   - q=1ì€ q=2ë³´ë‹¤ ì•½ê°„ í•˜ë½ (ë„ˆë¬´ ê·¹ë‹¨ì , í•™ìŠµ ë¶ˆì•ˆì •)
   - q=50ì€ ê±°ì˜ Baseline ìˆ˜ì¤€ (ì „ì²´ í‰ê· ê³¼ ìœ ì‚¬)

3. **ìµœì  q=2 vs Baseline**:
   - Domain_C TPR@1%: 76.38% â†’ **80.00%** (**+3.62%p**)
   - Mean AUROC: 98.20% â†’ **98.68%** (+0.48%)
   - Mean TPR@1%: 89.62% â†’ **90.83%** (+1.21%)

### ì¶”ê°€ Ablation: Warmup Schedule íš¨ê³¼

```bash
# Schedule ON vs OFF ë¹„êµ
CUDA_VISIBLE_DEVICES=0 python examples/notebooks/dinomaly_topk.py \
    --max-steps 1000 --seed 42 --gpu 0 --q-percent 5 \
    --q-schedule \
    --result-dir results/dinomaly_topk_schedule_on \
    2>&1 | tee logs/topk_schedule_on.log

CUDA_VISIBLE_DEVICES=1 python examples/notebooks/dinomaly_topk.py \
    --max-steps 1000 --seed 42 --gpu 0 --q-percent 5 \
    --no-q-schedule \
    --result-dir results/dinomaly_topk_schedule_off \
    2>&1 | tee logs/topk_schedule_off.log
```

---

## Summary Table (All Methods)

### AUROC (MeanÂ±Std)

| Method | Domain A | Domain B | Domain C | Domain D | Mean |
|--------|----------|----------|----------|----------|------|
| Baseline | 98.74Â±0.10% | 99.07Â±0.05% | 96.95Â±0.11% | 98.04Â±0.05% | 98.20Â±0.06% |
| Method 1 (GEM) | 98.77Â±0.03% | 99.05Â±0.02% | 96.89Â±0.10% | 98.06Â±0.06% | 98.19Â±0.03% |
| Method 2 (TopK q=5%) | 99.10Â±0.05% | 99.37Â±0.02% | 97.63Â±0.09% | 98.32Â±0.05% | 98.60Â±0.03% |
| **Method 2 (TopK q=2%)** | **99.18%** | **99.39%** | **97.71%** | **98.44%** | **98.68%** |
| Method 3 (Focal) | - | - | - | - | - |

### TPR@FPR=1% (MeanÂ±Std)

| Method | Domain A | Domain B | Domain C | Domain D | Mean |
|--------|----------|----------|----------|----------|------|
| Baseline | 94.12Â±0.17% | 94.62Â±0.10% | 76.38Â±0.85% | 93.36Â±0.10% | 89.62Â±0.22% |
| Method 1 (GEM) | 94.00Â±0.09% | 94.54Â±0.10% | 74.80Â±1.08% | 93.32Â±0.07% | 89.16Â±0.29% |
| Method 2 (TopK q=5%) | 94.62Â±0.19% | 94.90Â±0.13% | 79.14Â±0.34% | 93.48Â±0.16% | 90.54Â±0.15% |
| **Method 2 (TopK q=2%)** | **94.70%** | **95.00%** | **80.00%** | **93.60%** | **90.83%** |
| Method 3 (Focal) | - | - | - | - | - |

### TPR@FPR=5% (MeanÂ±Std)

| Method | Domain A | Domain B | Domain C | Domain D | Mean |
|--------|----------|----------|----------|----------|------|
| Baseline | 95.52Â±0.07% | 96.12Â±0.13% | 87.32Â±0.41% | 94.44Â±0.08% | 93.35Â±0.12% |
| Method 1 (GEM) | 95.48Â±0.07% | 95.94Â±0.08% | 87.08Â±0.41% | 94.26Â±0.12% | 93.19Â±0.07% |
| Method 2 (TopK q=5%) | 95.86Â±0.10% | 96.48Â±0.07% | 87.90Â±0.15% | 94.46Â±0.05% | 93.67Â±0.04% |
| **Method 2 (TopK q=2%)** | **96.10%** | **96.60%** | **88.30%** | **94.90%** | **93.97%** |
| Method 3 (Focal) | - | - | - | - | - |

### í•µì‹¬ ë°œê²¬

1. **Domain Cê°€ ë³‘ëª©**: ëª¨ë“  methodì—ì„œ ê°€ì¥ ë‚®ì€ ì„±ëŠ¥
   - AUROC: ~97% (ë‹¤ë¥¸ ë„ë©”ì¸ 98-99%)
   - TPR@1%: ~75-79% (ë‹¤ë¥¸ ë„ë©”ì¸ 93-95%)

2. **GEM Pooling íš¨ê³¼ ì—†ìŒ**: Baselineê³¼ í†µê³„ì ìœ¼ë¡œ ë™ë“±
   - Score distributionì´ ê±°ì˜ ë™ì¼
   - Hard miningì´ ì´ë¯¸ í¬í™”ëœ ì„±ëŠ¥ì—ì„œëŠ” íš¨ê³¼ ë¯¸ë¯¸

3. **Top-q% Loss (Method 2) ì„±ê³µ!**: ëª¨ë“  ì§€í‘œì—ì„œ ê°œì„ 
   - q=5%: Domain_C TPR@1% 76.38% â†’ 79.14% (+2.76%p)
   - **q=2% (ìµœì )**: Domain_C TPR@1% 76.38% â†’ **80.00%** (**+3.62%p**) âœ… ëª©í‘œ ë‹¬ì„±!
   - **ì „ì²´ AUROC**: 98.20% â†’ **98.68%** (+0.48%)
   - **ì „ì²´ TPR@1%**: 89.62% â†’ **90.83%** (+1.21%)
   - ë‹¤ë¥¸ ë„ë©”ì¸ ì„±ëŠ¥ë„ ë™ì‹œ ê°œì„  (A/B/D ëª¨ë‘ ìƒìŠ¹)

4. **Ablation ê²°ê³¼**: q=2%ê°€ ìµœì 
   - qê°€ ì‘ì„ìˆ˜ë¡(ë” ê·¹ë‹¨ì  tail) â†’ ì„±ëŠ¥ í–¥ìƒ
   - q=1%ëŠ” ì•½ê°„ í•˜ë½ (ë„ˆë¬´ ê·¹ë‹¨ì )
   - q=50%ëŠ” ê±°ì˜ Baseline ìˆ˜ì¤€

### ë‹¤ìŒ ì‹¤í—˜ ë°©í–¥

1. âœ… ~~**Domain C ì§‘ì¤‘ ë¶„ì„**~~: GEM ê²°ê³¼ ë¶„ì„ì—ì„œ ì›ì¸ íŒŒì•… ì™„ë£Œ (ì•½í•œ ê²°í•¨ + ê°•í•œ ì •ìƒ íŒ¨í„´)
2. âœ… **Top-q% Loss (Method 2)**: tail-focused learningìœ¼ë¡œ ê°œì„  ë‹¬ì„±!
3. âœ… **q% Ablation**: ìµœì  q=2% í™•ì •
4. ğŸ”„ **q=2% ë‹¤ì¤‘ seed ê²€ì¦**: 5 seedsë¡œ í†µê³„ì  ìœ ì˜ì„± í™•ì¸
5. **Focal Loss (Method 3)**: hard sampleì— ë” ì§‘ì¤‘
6. **Domain-specific threshold**: ë„ë©”ì¸ë³„ ìµœì  threshold íƒìƒ‰

---

## Notes

- ê²°ê³¼ í´ë” ê²½ë¡œ: `results/dinomaly_{method}/YYYYMMDD_HHMMSS_seed{N}/`
- ê° ì‹¤í—˜ í›„ ê²°ê³¼ í´ë”ëª…ì„ ìœ„ í…Œì´ë¸”ì— ê¸°ë¡í•  ê²ƒ
- **í†µê³„ì  ìœ ì˜ì„±**: p < 0.05ë©´ ìœ ì˜ë¯¸í•œ ì°¨ì´ë¡œ íŒë‹¨
