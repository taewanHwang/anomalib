#!/usr/bin/env python3
"""Multi-Domain Anomaly Detection í†µí•© í›ˆë ¨ í´ë˜ìŠ¤

ì´ ëª¨ë“ˆì€ multi-domain anomaly detection ì‹¤í—˜ì„ ìœ„í•œ í†µí•© í›ˆë ¨ í´ë˜ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
Single domainê³¼ ë‹¬ë¦¬ source domainì—ì„œ í›ˆë ¨í•˜ê³  multiple target domainsì—ì„œ í‰ê°€í•©ë‹ˆë‹¤.

ì£¼ìš” íŠ¹ì§•:
- MultiDomainHDMAPDataModule ì‚¬ìš©
- Source domain testê°€ validation ì—­í• 
- Target domains testê°€ í‰ê°€ ëŒ€ìƒ
- Source/Target ì‹œê°í™” í´ë” ë¶„ë¦¬
"""

import os
import logging
import torch
from pathlib import Path
from datetime import datetime
from typing import Dict, Any
from pytorch_lightning.loggers import TensorBoardLogger

# Anomalib imports
from anomalib.models.image.draem import Draem
from anomalib.models.image.dinomaly import Dinomaly
from anomalib.models.image.patchcore import Patchcore
from anomalib.engine import Engine
from anomalib.metrics import AUROC, Evaluator
from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint

# Experiment utilities import
import sys
sys.path.append(os.path.join(os.path.dirname(__file__), ".."))
from experiment_utils import (
    cleanup_gpu_memory,
    create_multi_domain_datamodule,
    evaluate_source_domain,
    evaluate_target_domains,
    extract_training_info,
    save_experiment_results,
    analyze_experiment_results
)


class MultiDomainAnomalyTrainer:
    """Multi-Domain Anomaly Detection ì „ìš© í›ˆë ¨ í´ë˜ìŠ¤"""

    def __init__(self, config: Dict[str, Any], experiment_name: str, session_timestamp: str, experiment_dir: str = None):
        """
        Args:
            config: ì‹¤í—˜ ì„¤ì • ë”•ì…”ë„ˆë¦¬ (model_type, source_domain, target_domains í¬í•¨)
            experiment_name: ì‹¤í—˜ ì´ë¦„
            session_timestamp: ì„¸ì…˜ timestamp
            experiment_dir: ì‹¤í—˜ ë””ë ‰í„°ë¦¬ ê²½ë¡œ (ì„ íƒì )
        """
        self.config = config
        self.experiment_name = experiment_name
        self.session_timestamp = session_timestamp
        self.external_experiment_dir = experiment_dir

        # Multi-domain ì„¤ì • ì¶”ì¶œ
        self.model_type = config["model_type"].lower()
        self.source_domain = config["source_domain"]
        self.target_domains = config["target_domains"]

        # ê²½ë¡œ ì„¤ì •
        self.setup_paths()

        # ë¡œê±° ì„¤ì •
        self.logger = logging.getLogger(f"multi_domain_{self.model_type}")

    def setup_paths(self):
        """ì‹¤í—˜ ê²½ë¡œ ì„¤ì •"""
        if self.external_experiment_dir:
            # bash ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ì „ë‹¬ë°›ì€ ë””ë ‰í„°ë¦¬ ì‚¬ìš©
            self.experiment_dir = Path(self.external_experiment_dir)
            self.results_dir = self.experiment_dir.parent
        else:
            # í˜¸í™˜ì„±ì„ ìœ„í•œ ê¸°ë³¸ ë°©ì‹ (ë‹¨ë… ì‹¤í–‰ ì‹œ)
            self.results_dir = Path("results") / self.session_timestamp
            self.experiment_dir = self.results_dir / f"{self.experiment_name}_{self.session_timestamp}"
            self.experiment_dir.mkdir(parents=True, exist_ok=True)
        
    def create_model(self):
        """Factory patternìœ¼ë¡œ ëª¨ë¸ ìƒì„± (multi-domain ìµœì í™”)"""
        if self.model_type == "draem":
            return self._create_draem_model()
        elif self.model_type == "draem_cutpaste":
            return self._create_draem_cutpaste_model()
        elif self.model_type == "draem_cutpaste_clf":
            return self._create_draem_cutpaste_clf_model()
        elif self.model_type == "dinomaly":
            return self._create_dinomaly_model()
        elif self.model_type == "patchcore":
            return self._create_patchcore_model()
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…: {self.model_type}")
    
    def _create_draem_model(self):
        """DRAEM ëª¨ë¸ ìƒì„±"""
        # ëª…ì‹œì ìœ¼ë¡œ test_image_AUROC ë©”íŠ¸ë¦­ ì„¤ì •
        val_auroc = AUROC(fields=["pred_score", "gt_label"], prefix="val_image_")
        test_auroc = AUROC(fields=["pred_score", "gt_label"], prefix="test_image_")
        evaluator = Evaluator(val_metrics=[val_auroc], test_metrics=[test_auroc])

        # DRAEM ëª¨ë¸ ìƒì„±
        model = Draem(evaluator=evaluator)

        # í•™ìŠµ ì„¤ì •ì„ _training_configì— ì €ì¥ (configure_optimizersì—ì„œ ì‚¬ìš©ë¨)
        model._training_config = {
            'learning_rate': self.config["learning_rate"],
            'optimizer': self.config["optimizer"],
            'weight_decay': self.config["weight_decay"],
            'max_epochs': self.config["max_epochs"],
            'scheduler': self.config.get("scheduler", None),  # ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • (ì„ íƒì‚¬í•­)
        }

        return model
    
    def _create_draem_cutpaste_model(self):
        """DRAEM CutPaste ëª¨ë¸ ìƒì„±"""
        # ëª…ì‹œì ìœ¼ë¡œ test_image_AUROC ë©”íŠ¸ë¦­ ì„¤ì •
        val_auroc = AUROC(fields=["pred_score", "gt_label"], prefix="val_image_")
        test_auroc = AUROC(fields=["pred_score", "gt_label"], prefix="test_image_")
        evaluator = Evaluator(val_metrics=[val_auroc], test_metrics=[test_auroc])

        # ëª¨ë¸ íŒŒë¼ë¯¸í„° ì„¤ì •
        model_params = {
            'evaluator': evaluator,
            'enable_sspcab': self.config.get("sspcab", False),
            'cut_w_range': tuple(self.config["cut_w_range"]),
            'cut_h_range': tuple(self.config["cut_h_range"]),
            'a_fault_start': self.config["a_fault_start"],
            'a_fault_range_end': self.config["a_fault_range_end"],
            'augment_probability': self.config["augment_probability"],
        }

        # DraemCutPaste ëª¨ë¸ ìƒì„±
        from anomalib.models.image.draem_cutpaste import DraemCutPaste
        model = DraemCutPaste(**model_params)

        # í•™ìŠµ ì„¤ì •ì„ _training_configì— ì €ì¥ (configure_optimizersì—ì„œ ì‚¬ìš©ë¨)
        model._training_config = {
            'learning_rate': self.config["learning_rate"],
            'optimizer': self.config["optimizer"],
            'weight_decay': self.config["weight_decay"],
            'max_epochs': self.config["max_epochs"],
            'scheduler': self.config.get("scheduler", None),  # ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • (ì„ íƒì‚¬í•­)
        }

        return model
    
    def _create_dinomaly_model(self):
        """Dinomaly ëª¨ë¸ ìƒì„±"""
        # DinomalyëŠ” ê¸°ë³¸ evaluatorë¥¼ ì‚¬ìš©í•˜ì—¬ trainable íŒŒë¼ë¯¸í„° ì„¤ì • ë¬¸ì œ íšŒí”¼
        model = Dinomaly(
            encoder_name=self.config["encoder_name"],
            target_layers=self.config["target_layers"],
            bottleneck_dropout=self.config["bottleneck_dropout"],
            decoder_depth=self.config["decoder_depth"],
            remove_class_token=self.config["remove_class_token"],
            evaluator=True  # ê¸°ë³¸ evaluator ì‚¬ìš©
        )
        # í•™ìŠµ ì„¤ì •ì„ _training_configì— ì €ì¥ (configure_optimizersì—ì„œ ì‚¬ìš©ë¨)
        model._training_config = {
            'learning_rate': self.config["learning_rate"],
            'weight_decay': self.config["weight_decay"],
        }
        return model
    
    def _create_patchcore_model(self):
        """Patchcore ëª¨ë¸ ìƒì„±"""
        return Patchcore(
            backbone=self.config["backbone"],
            layers=self.config["layers"],
            pre_trained=self.config["pre_trained"],
            coreset_sampling_ratio=self.config["coreset_sampling_ratio"],
            num_neighbors=self.config["num_neighbors"]
        )
    
    def _create_draem_cutpaste_clf_model(self):
        """DRAEM CutPaste Classification ëª¨ë¸ ìƒì„±"""
        # ëª…ì‹œì ìœ¼ë¡œ test_image_AUROC ë©”íŠ¸ë¦­ ì„¤ì •
        val_auroc = AUROC(fields=["pred_score", "gt_label"], prefix="val_image_")
        test_auroc = AUROC(fields=["pred_score", "gt_label"], prefix="test_image_")
        evaluator = Evaluator(val_metrics=[val_auroc], test_metrics=[test_auroc])

        # ëª¨ë¸ íŒŒë¼ë¯¸í„° ì„¤ì • - configì—ì„œë§Œ ê°’ í• ë‹¹
        model_params = {
            'evaluator': evaluator,
            'sspcab': self.config["sspcab"],
            'image_size': tuple(self.config["target_size"]),
            'severity_dropout': self.config["severity_dropout"],
            'severity_input_channels': self.config["severity_input_channels"],
            'cut_w_range': tuple(self.config["cut_w_range"]),
            'cut_h_range': tuple(self.config["cut_h_range"]),
            'a_fault_start': self.config["a_fault_start"],
            'a_fault_range_end': self.config["a_fault_range_end"],
            'augment_probability': self.config["augment_probability"],
            'clf_weight': self.config["clf_weight"],
        }

        # DraemCutPasteClf ëª¨ë¸ ìƒì„±
        from anomalib.models.image.draem_cutpaste_clf import DraemCutPasteClf
        model = DraemCutPasteClf(**model_params)

        # í•™ìŠµ ì„¤ì •ì„ _training_configì— ì €ì¥ (configure_optimizersì—ì„œ ì‚¬ìš©ë¨)
        model._training_config = {
            'learning_rate': self.config["learning_rate"],
            'optimizer': self.config["optimizer"],
            'weight_decay': self.config["weight_decay"],
            'max_epochs': self.config["max_epochs"],
            'scheduler': self.config.get("scheduler", None),  # ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • (ì„ íƒì‚¬í•­)
        }

        return model

    def create_callbacks(self):
        """ì½œë°± ì„¤ì • - ëª¨ë¸ë³„ ì ì ˆí•œ early stopping ë©”íŠ¸ë¦­ ì‚¬ìš©"""
        callbacks = []

        # ëª¨ë¸ë³„ EarlyStopping ì„¤ì •
        if self.model_type == "patchcore":
            # PatchCoreëŠ” ë‹¨ì¼ epoch í›ˆë ¨ì´ë¯€ë¡œ EarlyStoppingê³¼ ModelCheckpoint ëª¨ë‘ ë¶ˆí•„ìš”
            # Engineì—ì„œ ìë™ìœ¼ë¡œ ModelCheckpointë¥¼ ì¶”ê°€í•˜ë¯€ë¡œ ë³„ë„ ì¶”ê°€í•˜ì§€ ì•ŠìŒ
            print("   â„¹ï¸ PatchCore: EarlyStopping ë° ModelCheckpoint ë¹„í™œì„±í™” (ë‹¨ì¼ epoch í›ˆë ¨)")
            return []  # ë¹ˆ ì½œë°± ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

        else:
            # ëª¨ë¸ë³„ë¡œ ë‹¤ë¥¸ EarlyStopping monitor ì„¤ì •
            if self.model_type in ["draem", "draem_cutpaste", "draem_cutpaste_clf"]:
                # DRAEM: val_loss ê¸°ë°˜ EarlyStopping (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
                monitor_metric = "val_loss"
                monitor_mode = "min"
                print(f"   â„¹ï¸ {self.model_type.upper()}: EarlyStopping í™œì„±í™” (val_loss ëª¨ë‹ˆí„°ë§)")
            else:
                # Dinomaly: val_loss ê¸°ë°˜ EarlyStopping
                monitor_metric = "val_loss"
                monitor_mode = "min"
                print(f"   â„¹ï¸ {self.model_type.upper()}: EarlyStopping í™œì„±í™” (val_loss ëª¨ë‹ˆí„°ë§)")

            early_stopping = EarlyStopping(
                monitor=monitor_metric,
                patience=self.config["early_stopping_patience"],
                min_delta=self.config.get("early_stopping_min_delta", 0.001),
                mode=monitor_mode,
                verbose=True
            )
            callbacks.append(early_stopping)

            # Model Checkpoint
            if self.model_type in ["draem", "draem_cutpaste_clf"]:
                checkpoint = ModelCheckpoint(
                    filename=f"{self.model_type}_multi_domain_{self.source_domain}_to_targets_" + "{epoch:02d}_{val_loss:.4f}",
                    monitor="val_loss",
                    mode="min",
                    save_top_k=1,
                    verbose=True
                )
            else:
                checkpoint = ModelCheckpoint(
                    filename=f"{self.model_type}_multi_domain_{self.source_domain}_to_targets_" + "{epoch:02d}_{val_loss:.4f}",
                    monitor="val_loss",
                    mode="min",
                    save_top_k=1,
                    verbose=True
                )
            callbacks.append(checkpoint)

        return callbacks

    def configure_optimizer(self, model):
        """ì˜µí‹°ë§ˆì´ì € ì„¤ì • - ëª¨ë“  ëª¨ë¸ ê³µí†µ"""
        # PatchCoreëŠ” ì˜µí‹°ë§ˆì´ì €ê°€ í•„ìš”í•˜ì§€ ì•ŠìŒ
        if self.model_type == "patchcore":
            return

    def create_datamodule(self):
        """MultiDomainHDMAPDataModule ìƒì„±"""
        # dataset_root ìƒëŒ€ ê²½ë¡œ ì²˜ë¦¬
        dataset_root = self.config["dataset_root"]
        from pathlib import Path
        dataset_path = Path(dataset_root)
        if not dataset_path.is_absolute():
            # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì°¾ê¸° (anomalib ë””ë ‰í† ë¦¬ ê¸°ì¤€)
            current_file = Path(__file__).resolve()
            project_root = current_file.parent.parent.parent.parent  # 4ë‹¨ê³„ ìƒìœ„ = anomalib/
            dataset_root = str(project_root / dataset_root)
            print(f"   ğŸ“ ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜: {dataset_root}")
        
        return create_multi_domain_datamodule(
            source_domain=self.source_domain,
            target_domains=self.target_domains,
            dataset_root=dataset_root,
            batch_size=self.config["batch_size"],
            image_size=self.config["target_size"],
            resize_method=self.config["resize_method"],
            num_workers=self.config["num_workers"],
            seed=self.config["seed"],
            verbose=True
        )
    
    def train_model(self, model, datamodule, logger) -> tuple:
        """ëª¨ë¸ í›ˆë ¨ ìˆ˜í–‰"""
        print(f"\nğŸš€ {self.model_type.upper()} Multi-Domain ëª¨ë¸ í›ˆë ¨ ì‹œì‘")
        logger.info(f"ğŸš€ {self.model_type.upper()} Multi-Domain ëª¨ë¸ í›ˆë ¨ ì‹œì‘")

        # Config ì„¤ì • ì¶œë ¥
        print(f"   ğŸ”§ Config ì„¤ì •:")
        print(f"      Model Type: {self.model_type}")
        if self.model_type != "patchcore":
            print(f"      Max Epochs: {self.config['max_epochs']}")
            print(f"      Learning Rate: {self.config['learning_rate']}")
            print(f"      Early Stopping Patience: {self.config['early_stopping_patience']}")
        print(f"      Batch Size: {self.config['batch_size']}")

        # ì˜µí‹°ë§ˆì´ì € ì„¤ì •
        self.configure_optimizer(model)

        # ì½œë°± ì„¤ì •
        callbacks = self.create_callbacks()

        # TensorBoard ë¡œê±° ì„¤ì •
        self.tb_logger = TensorBoardLogger(
            save_dir=str(self.experiment_dir),
            name="tensorboard_logs",
            version=""
        )

        # Engine ì„¤ì •
        # PatchCoreì˜ ê²½ìš° max_epochsë¥¼ 1ë¡œ ê°•ì œ ì„¤ì •
        max_epochs = 1 if self.model_type == "patchcore" else self.config["max_epochs"]

        engine_kwargs = {
            "accelerator": "gpu" if torch.cuda.is_available() else "cpu",
            "devices": [0] if torch.cuda.is_available() else 1,
            "logger": self.tb_logger,
            "callbacks": callbacks,
            "enable_checkpointing": True,
            "log_every_n_steps": 10,
            "enable_model_summary": True,
            "default_root_dir": str(self.experiment_dir),
            "max_epochs": max_epochs,
            "check_val_every_n_epoch": 1,
            "num_sanity_val_steps": 0
        }

        if self.model_type == "patchcore":
            print(f"   â„¹ï¸ PatchCore: max_epochs ê°•ì œ ì„¤ì • (1 epoch)")
        else:
            print(f"   â„¹ï¸ {self.model_type.upper()}: max_epochs = {max_epochs}")

        engine = Engine(**engine_kwargs)

        print(f"   ğŸ”§ Engine ì„¤ì • ì™„ë£Œ")
        print(f"   ğŸ“ ê²°ê³¼ ì €ì¥ ê²½ë¡œ: {self.experiment_dir}")
        logger.info(f"ğŸ”§ Engine ì„¤ì • ì™„ë£Œ")
        logger.info(f"ğŸ“ ê²°ê³¼ ì €ì¥ ê²½ë¡œ: {self.experiment_dir}")

        # ëª¨ë¸ í›ˆë ¨
        print(f"   ğŸ¯ ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")
        logger.info("ğŸ¯ ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")

        engine.fit(model=model, datamodule=datamodule)

        # ìµœê³  ì²´í¬í¬ì¸íŠ¸ ì°¾ê¸°
        best_checkpoint = ""
        for callback in callbacks:
            if isinstance(callback, ModelCheckpoint) and hasattr(callback, 'best_model_path'):
                best_checkpoint = callback.best_model_path
                break

        print(f"   ğŸ† Best Checkpoint: {best_checkpoint}")
        logger.info(f"ğŸ† Best Checkpoint: {best_checkpoint}")

        # Best checkpoint ë¡œë“œ (PatchCore ì œì™¸)
        if best_checkpoint and os.path.exists(best_checkpoint) and self.model_type != "patchcore":
            print(f"   ğŸ“‚ Best checkpoint ë¡œë“œ ì¤‘...")
            checkpoint = torch.load(best_checkpoint, map_location='cuda' if torch.cuda.is_available() else 'cpu')

            # state_dict ë¡œë“œ
            model.load_state_dict(checkpoint['state_dict'])

            print(f"   âœ… Best checkpoint ë¡œë“œ ì™„ë£Œ!")
            logger.info(f"âœ… Best checkpoint ë¡œë“œ ì™„ë£Œ: {best_checkpoint}")
        elif self.model_type == "patchcore":
            print(f"   â„¹ï¸ PatchCore: Best checkpoint ë¡œë“œ ê±´ë„ˆëœ€ (ë‹¨ì¼ epoch ëª¨ë¸)")
        else:
            print(f"   âš ï¸ Best checkpoint íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {best_checkpoint}")
            logger.warning(f"Best checkpoint íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {best_checkpoint}")

        print(f"   âœ… ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!")
        logger.info("âœ… ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!")

        return model, engine, best_checkpoint
    
    def run_multi_domain_evaluation(self, model, datamodule, logger):
        """Multi-domain í‰ê°€ ìˆ˜í–‰ (source + targets)"""
        print(f"\nğŸ“Š Multi-Domain í‰ê°€ ì‹œì‘")
        logger.info("ğŸ“Š Multi-Domain í‰ê°€ ì‹œì‘")

        # ì‹œê°í™” ê¸°ë³¸ ë””ë ‰í„°ë¦¬ ì„¤ì •
        visualizations_dir = self.experiment_dir / "visualizations"

        # 1. Source Domain í‰ê°€ (validation ì—­í• )
        print(f"ğŸ“Š Source Domain ({self.source_domain}) í‰ê°€ ì‹œì‘")
        logger.info(f"ğŸ“Š Source Domain ({self.source_domain}) í‰ê°€ ì‹œì‘")

        # ì†ŒìŠ¤ ë„ë©”ì¸ ì‹œê°í™” ê²½ë¡œ: visualizations/source/domain_A/
        source_viz_dir = visualizations_dir / "source" / self.source_domain

        # Analysis ë””ë ‰í„°ë¦¬ ì„¤ì •
        analysis_dir = visualizations_dir.parent / "analysis"
        analysis_dir.mkdir(exist_ok=True)
        
        source_results = evaluate_source_domain(
            model=model,
            datamodule=datamodule,
            visualization_dir=source_viz_dir,
            model_type=self.model_type,
            max_visualization_batches=self.config.get("max_visualization_batches", 5),
            verbose=True,
            analysis_dir=analysis_dir
        )

        # 2. Target Domains í‰ê°€ (test ì—­í• )
        print(f"\nğŸ¯ Target Domains {self.target_domains} í‰ê°€ ì‹œì‘")
        logger.info(f"ğŸ¯ Target Domains {self.target_domains} í‰ê°€ ì‹œì‘")

        # íƒ€ê²Ÿ ë„ë©”ì¸ ì‹œê°í™” ê²½ë¡œ: visualizations/target/
        target_viz_base_dir = visualizations_dir / "target"

        # Source optimal threshold ì¶”ì¶œ (analysisê°€ í™œì„±í™”ëœ ê²½ìš°)
        source_optimal_threshold = source_results.get('optimal_threshold', None)
        
        target_results = evaluate_target_domains(
            model=model,
            datamodule=datamodule,
            visualization_base_dir=target_viz_base_dir,
            model_type=self.model_type,
            max_visualization_batches=self.config.get("max_visualization_batches", 5),
            verbose=True,
            analysis_base_dir=analysis_dir,
            source_optimal_threshold=source_optimal_threshold
        )

        return source_results, target_results

    def save_results(self, source_results, target_results, training_info, best_checkpoint, logger):
        """ì‹¤í—˜ ê²°ê³¼ ì €ì¥"""
        experiment_results = {
            "experiment_name": self.experiment_name,
            "description": f"{self.source_domain} to {self.target_domains} - {self.model_type.upper()} multi domain training",
            "source_domain": self.source_domain,
            "target_domains": self.target_domains,
            "config": self.config,
            "source_results": source_results,
            "target_results": target_results,
            "training_info": training_info,
            "timestamp": datetime.now().isoformat(),
            "checkpoint_path": best_checkpoint,
            "status": "success",
            "condition": {
                "name": self.experiment_name,
                "description": f"{self.source_domain} to {self.target_domains} - {self.model_type.upper()} multi domain training",
                "config": {
                    "source_domain": self.source_domain,
                    "target_domains": self.target_domains,
                    **self.config
                }
            }
        }

        # ê²°ê³¼ ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        # ì‹¤í—˜ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ì— ì§ì ‘ ì €ì¥
        result_filename = f"result_{timestamp}.json"

        results_file = save_experiment_results(
            result=experiment_results,
            result_filename=result_filename,
            log_dir=Path(self.experiment_dir),  # ì‹¤í—˜ ë£¨íŠ¸ ë””ë ‰í† ë¦¬
            logger=logger,
            model_type=self.model_type.upper()
        )
        print(f"ğŸ“„ ì‹¤í—˜ ê²°ê³¼ ì €ì¥ë¨: {results_file}")

        return experiment_results

    def run_experiment(self) -> Dict[str, Any]:
        """ì „ì²´ multi-domain ì‹¤í—˜ ì‹¤í–‰"""
        print(f"\n{'='*80}")
        print(f"ğŸ§ª Multi-Domain {self.model_type.upper()} ì‹¤í—˜ ì‹œì‘: {self.experiment_name}")
        print(f"ğŸŒ Source Domain: {self.source_domain}")
        print(f"ğŸ¯ Target Domains: {self.target_domains}")
        print(f"{'='*80}")
        
        self.logger.info(f"ğŸ§ª Multi-Domain {self.model_type.upper()} ì‹¤í—˜ ì‹œì‘: {self.experiment_name}")
        self.logger.info(f"ì‹¤í—˜ ì„¤ì •: {self.config}")
        
        try:
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            cleanup_gpu_memory()
            
            # ì‹¤í—˜ ë””ë ‰í„°ë¦¬ ìƒì„±
            self.experiment_dir.mkdir(parents=True, exist_ok=True)
            
            # ëª¨ë¸ ìƒì„±
            model = self.create_model()
            print(f"âœ… {self.model_type.upper()} ëª¨ë¸ ìƒì„± ì™„ë£Œ")
            
            # DataModule ìƒì„±
            datamodule = self.create_datamodule()
            print(f"âœ… Multi-Domain DataModule ìƒì„± ì™„ë£Œ")
            print(f"   ğŸŒ Source: {self.source_domain}")
            print(f"   ğŸ¯ Targets: {self.target_domains}")
            
            # ë¡œê¹… ì„¤ì •
            from experiment_utils import setup_experiment_logging
            log_file_path = self.experiment_dir / f"{self.source_domain}_multi_domain.log"
            logger = setup_experiment_logging(str(log_file_path), self.experiment_name)
            logger.info(f"ğŸš€ Multi-Domain ì‹¤í—˜ ì‹œì‘: {self.experiment_name}")

            # ëª¨ë¸ í›ˆë ¨
            model, engine, best_checkpoint = self.train_model(model, datamodule, logger)

            # Multi-domain í‰ê°€
            source_results, target_results = self.run_multi_domain_evaluation(
                model, datamodule, logger
            )
            
            # í›ˆë ¨ ì •ë³´ ì¶”ì¶œ
            training_info = extract_training_info(engine)
            
            # ê²°ê³¼ ë¶„ì„ ë° ì €ì¥
            analysis_path = self.experiment_dir / "analysis_results.json"
            analyze_experiment_results(
                source_results=source_results,
                target_results=target_results,
                training_info=training_info,
                experiment_config=self.config,
                save_path=analysis_path,
                verbose=True
            )

            # ì‹¤í—˜ ê²°ê³¼ ì €ì¥
            experiment_result = self.save_results(
                source_results, target_results, training_info, best_checkpoint, logger
            )
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            cleanup_gpu_memory()
            
            print(f"\nâœ… Multi-Domain ì‹¤í—˜ ì™„ë£Œ: {self.experiment_name}")
            logger.info(f"âœ… Multi-Domain ì‹¤í—˜ ì™„ë£Œ: {self.experiment_name}")
            
            return experiment_result
            
        except Exception as e:
            error_msg = f"Multi-Domain ì‹¤í—˜ ì‹¤íŒ¨: {e}"
            print(f"\nâŒ {error_msg}")
            if 'logger' in locals():
                logger.error(f"âŒ {error_msg}")
            else:
                self.logger.error(f"âŒ {error_msg}")
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬ (ì‹¤íŒ¨ ì‹œì—ë„)
            cleanup_gpu_memory()
            
            return {
                "status": "failed",
                "error": str(e),
                "experiment_name": self.experiment_name,
                "config": self.config
            }