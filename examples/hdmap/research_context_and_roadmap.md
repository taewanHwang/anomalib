# HDMap Anomaly Detection Research Context & Roadmap

## ì „ì²´ ì»¨í…ìŠ¤íŠ¸

### ì—°êµ¬ ë°°ê²½
- **ë„ë©”ì¸**: HDMap ê¸°ì–´ë°•ìŠ¤ ì§„ë™ ë°ì´í„° ì´ìƒê°ì§€
- **ë°ì´í„° íŠ¹ì„±**: 
  - ì •ìƒ â†’ ê³ ì¥ ì‹œ **additive pattern** ì¶”ê°€ (ì£¼ë¡œ ê°€ë¡œì„ )
  - Driving axis (Yì¶•) vs Driven tooth (Xì¶•) ì¡°í•©
  - ë¬¼ë¦¬ì  ê³ ì¥: íŠ¹ì • ì¶•ì—ì„œ ì „ì²´ ì§„ë™ ì¦ê°€ â†’ ë°©í–¥ì„± íŒ¨í„´

### í˜„ì¬ ì„±ëŠ¥ í˜„í™©
```
ğŸ“Š ì‹¤í—˜ ê²°ê³¼ ìš”ì•½ (AUROC):
- DINOMALY: 0.911 (ìµœê³  ì„±ëŠ¥) âœ…
- DRAEM SevNet: 0.6~0.8 (ì œí•œì  ì„±ëŠ¥) âŒ
- PatchCore: 0.8 ì •ë„
```

### ì—°êµ¬ ëª©í‘œ
**DINOMALYë¥¼ ê¸°ë°˜ìœ¼ë¡œ HDMap íŠ¹í™” ê°œì„ ì„ í†µí•´ ì„¸ê³„ì  ìˆ˜ì¤€ì˜ ë…¼ë¬¸ ì‘ì„±**

## í•µì‹¬ ì•„ì´ë””ì–´ ìš”ì•½

### ğŸ¯ Main Contribution: Directional Attention
**Physical-Informed Architectureë¡œ ê°€ì¥ ìœ ë§**

**ë°°ê²½**: ê¸°ì¡´ Linear Attentionì€ ì „ì—­ì ì´ë¼ ë°©í–¥ì„± ê³ ì¥ íŒ¨í„´ ë¬´ì‹œ  
**í•´ë²•**: Xì¶•/Yì¶• ë³„ë„ attention + í•™ìŠµ ê°€ëŠ¥í•œ ì¡°í•©

```python
# í•µì‹¬ ìˆ˜ì‹
out_y = DirectionalAttention_Y(q, k, v)  # ê°€ë¡œì„  íŠ¹í™”
out_x = DirectionalAttention_X(q, k, v)  # ì„¸ë¡œì„  íŠ¹í™”  
output = Î± * out_y + (1-Î±) * out_x      # í•™ìŠµ ê°€ëŠ¥í•œ ì¡°í•©
```

**ë…¼ë¬¸ ê¸°ì—¬ë„**: â˜…â˜…â˜…â˜…â˜…
- Novel architecture innovation
- Strong physical foundation
- Generalizable to other domains

### ğŸ¯ Secondary: Severity-Aware DINOMALY
**ì‹¤ìš©ì  ê°€ì¹˜ëŠ” ë†’ìœ¼ë‚˜ ê¸°ìˆ ì  ë…ì°½ì„± ì œí•œ**

**ë°°ê²½**: ì‚°ì—… í˜„ì¥ì—ì„œ anomaly ì‹¬ê°ë„ ì •ë³´ í•„ìš”  
**í•´ë²•**: Multi-task learning (detection + severity estimation)

**ë…¼ë¬¸ ê¸°ì—¬ë„**: â˜…â˜…â˜…
- Practical value ë†’ìŒ
- ê¸°ìˆ ì  novelty ì œí•œì 

## ìƒì„¸ ë¶„ì„ ê²°ê³¼

### DINOMALY ì•„í‚¤í…ì²˜ ë¶„ì„
```
ğŸ“ /anomalib/src/anomalib/models/image/dinomaly/
â”œâ”€â”€ README.md              # ì•„í‚¤í…ì²˜ ê°œìš”
â”œâ”€â”€ lightning_model.py     # PyTorch Lightning wrapper
â”œâ”€â”€ torch_model.py         # í•µì‹¬ ëª¨ë¸ êµ¬í˜„  
â””â”€â”€ components/
    â”œâ”€â”€ layers.py          # LinearAttention êµ¬í˜„ â­
    â”œâ”€â”€ loss.py           # CosineHardMiningLoss
    â””â”€â”€ vision_transformer.py
```

**í•µì‹¬ íŠ¹ì§•**:
1. **DINOv2 Encoder** (frozen) + **Bottleneck MLP** + **ViT Decoder**
2. **Linear Attention**: ELU ê¸°ë°˜ positive feature maps
3. **Hard Mining Loss**: Easy sample down-weighting
4. **Multi-scale Feature Fusion**: Layer groupë³„ reconstruction

### Linear Attention ìˆ˜ì‹ ë¶„ì„
```python
# í˜„ì¬ êµ¬í˜„ (layers.py:107-146)
Q' = ELU(Q) + 1.0
K' = ELU(K) + 1.0
KV = K'^T @ V                    # Global interaction
K_sum = sum(K', dim=seq)         # Normalization
Z = 1 / (Q' @ K_sum)
Output = (Q' @ KV) * Z
```

## Implementation Roadmap

### Phase 1: Directional Attention (4ì£¼)
```
Week 1-2: Core Implementation
- [ ] DirectionalLinearAttention ëª¨ë“ˆ êµ¬í˜„
- [ ] DINOMALY integration (decoder layers 2-3ê°œë§Œ êµì²´)
- [ ] Basic training pipeline êµ¬ì¶•

Week 3-4: Experimental Validation  
- [ ] Ablation studies (X-only, Y-only, Combined)
- [ ] Î± combination strategy ìµœì í™”
- [ ] HDMap ë°ì´í„°ì—ì„œ ì²« ì„±ëŠ¥ ê²€ì¦
```

### Phase 2: Advanced Experiments (4ì£¼)
```
Week 5-6: Deep Analysis
- [ ] Layer-wise application ìµœì í™” 
- [ ] Attention visualization ë° í•´ì„
- [ ] ë‹¤ì–‘í•œ spatial_shape ì‹¤í—˜

Week 7-8: Generalization
- [ ] ë‹¤ë¥¸ industrial dataset ê²€ì¦
- [ ] Medical/Satellite imagery í™•ì¥ ì‹¤í—˜
- [ ] Comparative analysis with baselines
```

### Phase 3: Paper Writing (4ì£¼)
```
Week 9-10: Core Writing
- [ ] Introduction & Related Work
- [ ] Method & Architecture ìƒì„¸ ê¸°ìˆ 
- [ ] Experimental setup ë° results

Week 11-12: Polish & Submit
- [ ] Discussion & Limitation
- [ ] Revision ë° proofreading  
- [ ] Target venue submission (ICLR/NeurIPS)
```

## íŒŒì¼ êµ¬ì¡° ê³„íš

### êµ¬í˜„ íŒŒì¼ êµ¬ì¡°
```
ğŸ“ anomalib/src/anomalib/models/image/dinomaly_directional/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ lightning_model.py          # DirectionalDINOMALY Lightning model
â”œâ”€â”€ torch_model.py             # Core model with DirectionalAttention
â””â”€â”€ components/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ directional_attention.py   # ğŸ†• DirectionalLinearAttention
    â”œâ”€â”€ layers.py                  # Extended from original
    â””â”€â”€ loss.py                    # ê¸°ì¡´ loss ì¬ì‚¬ìš©
```

### ì‹¤í—˜ ì„¤ì • íŒŒì¼
```
ğŸ“ examples/hdmap/configs/
â”œâ”€â”€ dinomaly_directional_base.yaml
â”œâ”€â”€ dinomaly_directional_ablation.yaml
â””â”€â”€ dinomaly_severity_aware.yaml
```

## í•µì‹¬ ì‹¤í—˜ ì„¤ê³„

### Ablation Studies Plan
1. **Direction Comparison**
   - Vanilla LinearAttention (baseline)
   - X-axis only attention  
   - Y-axis only attention
   - Combined (Î± learnable vs fixed)

2. **Architecture Variants**
   - ì–´ë–¤ decoder layerì— ì ìš©? (1-2ì¸µ vs 2-3ì¸µ vs ì „ì²´)
   - Spatial resolution ì˜í–¥? (16x16 vs 32x32)
   - Î± initialization strategy

3. **Generalization Tests**
   - Cross-domain: ë‹¤ë¥¸ ê¸°ê³„ ë°ì´í„°
   - Cross-resolution: ë‹¤ë¥¸ ì´ë¯¸ì§€ í¬ê¸°
   - Robustness: Noise, rotation ë“±

### Evaluation Metrics
- **Primary**: AUROC, AUPR (ê¸°ì¡´ anomaly detection metric)
- **Secondary**: F1-score, Precision@90%Recall
- **Analysis**: Attention map visualization, inference time

## ë…¼ë¬¸ ì „ëµ

### Target Venues (ìš°ì„ ìˆœìœ„)
1. **ICLR 2026** - Architecture innovation ì¸ì •
2. **NeurIPS 2025** - ML conference ìµœê³  ê¶Œìœ„
3. **AAAI 2026** - Practical application ì¤‘ì‹œ

### Paper Title í›„ë³´
- "Directional Attention for Structure-Aware Anomaly Detection"
- "Physical-Informed Attention Mechanisms in Vision Transformers"  
- "Structure-Aware Linear Attention for Industrial Anomaly Detection"

### Contribution Claims
1. **Novel Directional Attention**: ë¬¼ë¦¬ì  íŒ¨í„´ì— íŠ¹í™”ëœ attention mechanism
2. **Physical Foundation**: Domain knowledge â†’ Architecture design ì§ì ‘ ì—°ê²°
3. **Broad Applicability**: Industrial/Medical imaging ì „ë°˜ ì ìš© ê°€ëŠ¥
4. **Strong Empirical Results**: HDMap + ë‹¤ë¥¸ ë„ë©”ì¸ì—ì„œ consistent improvement

## ìœ„í—˜ ê´€ë¦¬

### Technical Risks
- **Gradient Flow**: Spatial reshape ê³¼ì •ì—ì„œ gradient vanishing/exploding
- **Memory Usage**: Directional attentionì˜ computational overhead  
- **Generalization**: HDMap íŠ¹í™”ê°€ ë‹¤ë¥¸ ë„ë©”ì¸ì—ì„œ ì„±ëŠ¥ ì €í•˜ ê°€ëŠ¥

### Mitigation Strategies
- **Progressive Implementation**: ë‹¨ê³„ë³„ êµ¬í˜„ìœ¼ë¡œ ë¬¸ì œ ì¡°ê¸° ë°œê²¬
- **Extensive Ablation**: ê° componentë³„ ì˜í–¥ ë¶„ì„
- **Multiple Baselines**: ì¶©ë¶„í•œ ë¹„êµêµ° í™•ë³´

## ë‹¤ìŒ ì„¸ì…˜ ì‹œì‘ ì‹œ ì°¸ê³ ì‚¬í•­

### ì¤‘ìš” ê²°ì •ì‚¬í•­
- âœ… **Main approach**: Directional Attention ì„ íƒ
- âœ… **Target**: ICLR 2026 submission  
- âœ… **Implementation priority**: DirectionalLinearAttention ë¨¼ì €

### í•µì‹¬ íŒŒì¼ ìœ„ì¹˜
- `/examples/hdmap/directional_attention_improvement.md` - ìƒì„¸ ê¸°ìˆ  ì„¤ê³„
- `/examples/hdmap/severity_aware_dinomaly.md` - ëŒ€ì•ˆ ì ‘ê·¼ë²•
- `/examples/hdmap/research_context_and_roadmap.md` - ì „ì²´ ì»¨í…ìŠ¤íŠ¸ (í˜„ì¬ íŒŒì¼)

### ë°”ë¡œ ì‹œì‘í•  ìˆ˜ ìˆëŠ” ì‘ì—…
1. **DirectionalLinearAttention ëª¨ë“ˆ êµ¬í˜„**
2. **ê¸°ì¡´ DINOMALY ì½”ë“œ ë¶„ì„ ë° integration point íŒŒì•…**  
3. **First prototype training on HDMap data**

---

**ğŸš€ Ready to Start Implementation!**