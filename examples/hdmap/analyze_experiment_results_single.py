#!/usr/bin/env python3
"""
Single Domain ì‹¤í—˜ ê²°ê³¼ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” Single-Domain ì‹¤í—˜ì˜ metrics_report.json íŒŒì¼ì„ ë¶„ì„í•˜ì—¬
ì„±ëŠ¥ ë©”íŠ¸ë¦­ì„ í…Œì´ë¸” í˜•íƒœë¡œ ì¶œë ¥í•©ë‹ˆë‹¤.

==============================================================================
ğŸš€ ì‚¬ìš©ë²•:
==============================================================================

.venv/bin/python examples/hdmap/analyze_experiment_results_single.py --results_dir results_100k_train

==============================================================================
ğŸ“ ëŒ€ìƒ í´ë” êµ¬ì¡°:
==============================================================================

results_100k_train/
â””â”€â”€ 20251011_054347/
    â”œâ”€â”€ exp-71.A_20251011_054347/
    â”‚   â””â”€â”€ analysis/
    â”‚       â””â”€â”€ metrics_report.json
    â”œâ”€â”€ exp-71.B_20251011_054347/
    â”‚   â””â”€â”€ analysis/
    â”‚       â””â”€â”€ metrics_report.json
    â””â”€â”€ ...

==============================================================================
ğŸ“Š ì¶œë ¥ ë‚´ìš©:
==============================================================================

ì‹¤í—˜ë³„ ìƒì„¸ ì„±ëŠ¥ í…Œì´ë¸”:
- ì‹¤í—˜ ì´ë¦„ (íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨)
- ë„ë©”ì¸ (A, B, C, D)
- AUROC
- Optimal Threshold
- Precision
- Recall
- F1 Score
- Accuracy (Confusion Matrix ê¸°ë°˜ ê³„ì‚°)

==============================================================================
ğŸ”§ Metrics JSON í˜•ì‹:
==============================================================================

{
  "auroc": 1.0,
  "optimal_threshold": 0.41473591327667236,
  "precision": 1.0,
  "recall": 0.9988888888888889,
  "f1_score": 0.9994441356309061,
  "confusion_matrix": [[900, 0], [1, 899]],
  "total_samples": 1800,
  "positive_samples": 900,
  "negative_samples": 900
}
"""

import argparse
import json
from pathlib import Path

import pandas as pd


def extract_experiment_info(exp_dir_name):
    """ì‹¤í—˜ ë””ë ‰í† ë¦¬ ì´ë¦„ì—ì„œ ì •ë³´ ì¶”ì¶œ

    ì˜ˆ: exp-71.A_20251011_054347
    -> experiment_name: exp-71.A
    -> domain: A
    -> timestamp: 20251011_054347
    """
    # íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ (ë§ˆì§€ë§‰ ë‘ ë¶€ë¶„: YYYYMMDD_HHMMSS)
    parts = exp_dir_name.split('_')

    timestamp = None
    exp_name_with_domain = exp_dir_name

    # ë§ˆì§€ë§‰ ë‘ ë¶€ë¶„ì´ timestampì¸ì§€ í™•ì¸
    if len(parts) >= 3:
        if (parts[-2].isdigit() and len(parts[-2]) == 8 and
            parts[-1].isdigit() and len(parts[-1]) == 6):
            timestamp = f"{parts[-2]}_{parts[-1]}"
            exp_name_with_domain = '_'.join(parts[:-2])

    # ë„ë©”ì¸ ì¶”ì¶œ (exp-71.A -> A)
    domain = None
    experiment_name = exp_name_with_domain

    if '.' in exp_name_with_domain:
        exp_parts = exp_name_with_domain.split('.')
        if len(exp_parts) >= 2:
            domain = exp_parts[1]  # A, B, C, D ë“±
            # experiment_nameì€ ë„ë©”ì¸ í¬í•¨í•œ ì „ì²´ ì´ë¦„
            experiment_name = exp_name_with_domain

    return {
        'full_name': exp_dir_name,
        'experiment_name': experiment_name,
        'domain': domain,
        'timestamp': timestamp
    }


def calculate_accuracy(confusion_matrix):
    """Confusion Matrixë¡œë¶€í„° Accuracy ê³„ì‚°

    confusion_matrix: [[TN, FP], [FN, TP]]
    """
    if confusion_matrix and len(confusion_matrix) == 2 and len(confusion_matrix[0]) == 2:
        tn, fp = confusion_matrix[0][0], confusion_matrix[0][1]
        fn, tp = confusion_matrix[1][0], confusion_matrix[1][1]
        total = tp + tn + fp + fn
        if total > 0:
            return (tp + tn) / total
    return None


def analyze_single_domain_experiments(results_dir: str):
    """Single-Domain ì‹¤í—˜ ê²°ê³¼ ë¶„ì„"""
    results_path = Path(results_dir)

    print(f"ğŸ” Single-Domain ì‹¤í—˜ ê²°ê³¼ ë¶„ì„ ì‹œì‘...")
    print(f"ğŸ“ ê¸°ë³¸ ë””ë ‰í† ë¦¬: {results_path}")

    # timestamp ë””ë ‰í† ë¦¬ ì°¾ê¸°
    timestamp_dirs = [d for d in results_path.iterdir() if d.is_dir()]

    if not timestamp_dirs:
        print(f"âŒ {results_path}ì—ì„œ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ëª¨ë“  ì‹¤í—˜ ë””ë ‰í† ë¦¬ ìˆ˜ì§‘
    all_experiment_dirs = []
    for timestamp_dir in timestamp_dirs:
        experiment_dirs = [d for d in timestamp_dir.iterdir() if d.is_dir()]
        all_experiment_dirs.extend(experiment_dirs)

    if not all_experiment_dirs:
        print(f"âŒ ì‹¤í—˜ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    print(f"ğŸ“Š ë°œê²¬ëœ ì‹¤í—˜ ë””ë ‰í† ë¦¬: {len(all_experiment_dirs)}ê°œ")

    # ê° ì‹¤í—˜ì˜ metrics_report.json ìˆ˜ì§‘
    experiment_results = []

    for exp_dir in all_experiment_dirs:
        metrics_report_path = exp_dir / "analysis" / "metrics_report.json"

        if not metrics_report_path.exists():
            continue

        try:
            with open(metrics_report_path, 'r', encoding='utf-8') as f:
                metrics_data = json.load(f)

            # ì‹¤í—˜ ì •ë³´ ì¶”ì¶œ
            exp_info = extract_experiment_info(exp_dir.name)

            # Accuracy ê³„ì‚°
            accuracy = calculate_accuracy(metrics_data.get('confusion_matrix'))
            val_metrics = metrics_data.get('validation_metrics')
            val_accuracy = (
                calculate_accuracy(val_metrics.get('confusion_matrix'))
                if val_metrics and val_metrics.get('confusion_matrix')
                else None
            )

            # Enhanced metrics ì¶”ì¶œ
            enhanced_metrics = metrics_data.get('enhanced_metrics')

            # ê²°ê³¼ ì €ì¥
            result_dict = {
                'experiment_name': exp_info['experiment_name'],
                'domain': exp_info['domain'],
                'timestamp': exp_info['timestamp'],
                'auroc': metrics_data.get('auroc'),
                'optimal_threshold': metrics_data.get('optimal_threshold'),
                'precision': metrics_data.get('precision'),
                'recall': metrics_data.get('recall'),
                'f1_score': metrics_data.get('f1_score'),
                'accuracy': accuracy,
                'val_auroc': val_metrics.get('auroc') if val_metrics else None,
                'val_precision': val_metrics.get('precision') if val_metrics else None,
                'val_recall': val_metrics.get('recall') if val_metrics else None,
                'val_f1_score': val_metrics.get('f1_score') if val_metrics else None,
                'val_accuracy': val_accuracy,
                'full_dir_name': exp_info['full_name']
            }

            # Enhanced metrics ì¶”ê°€ (ìˆëŠ” ê²½ìš°)
            if enhanced_metrics:
                result_dict.update({
                    'enhanced_auroc': enhanced_metrics.get('auroc'),
                    # 99.5 percentile metrics
                    'percentile_99_5_threshold': enhanced_metrics.get('percentile_99_5_threshold'),
                    'tpr_at_percentile_99_5': enhanced_metrics.get('tpr_at_percentile_99_5'),
                    'fpr_at_percentile_99_5': enhanced_metrics.get('fpr_at_percentile_99_5'),
                    'precision_at_percentile_99_5': enhanced_metrics.get('precision_at_percentile_99_5'),
                    # 99.9 percentile metrics
                    'percentile_99_9_threshold': enhanced_metrics.get('percentile_99_9_threshold'),
                    'tpr_at_percentile_99_9': enhanced_metrics.get('tpr_at_percentile_99_9'),
                    'fpr_at_percentile_99_9': enhanced_metrics.get('fpr_at_percentile_99_9'),
                    'precision_at_percentile_99_9': enhanced_metrics.get('precision_at_percentile_99_9'),
                    # Fixed threshold metrics
                    'tpr_at_fixed': enhanced_metrics.get('tpr_at_fixed'),
                    'fpr_at_fixed': enhanced_metrics.get('fpr_at_fixed'),
                    'precision_at_fixed': enhanced_metrics.get('precision_at_fixed'),
                    'val_normal_score_mean': enhanced_metrics.get('val_normal_score_mean'),
                    'val_normal_score_std': enhanced_metrics.get('val_normal_score_std'),
                })

            experiment_results.append(result_dict)

        except Exception as e:
            print(f"   âš ï¸ {exp_dir.name} metrics_report.json ë¡œë“œ ì‹¤íŒ¨: {e}")

    if not experiment_results:
        print("âŒ ë¶„ì„í•  ìˆ˜ ìˆëŠ” metrics_report.json íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    # DataFrame ìƒì„±
    df = pd.DataFrame(experiment_results)

    # ì •ë ¬: ì‹¤í—˜ëª… ì˜¤ë¦„ì°¨ìˆœ -> ë„ë©”ì¸ ì˜¤ë¦„ì°¨ìˆœ -> íƒ€ì„ìŠ¤íƒ¬í”„ ì˜¤ë¦„ì°¨ìˆœ
    df = df.sort_values(['experiment_name', 'domain', 'timestamp'], ascending=[True, True, True])

    # ê²°ê³¼ ì¶œë ¥
    print(f"\n{'='*120}")
    print(f"ğŸ¯ Single-Domain ì‹¤í—˜ ê²°ê³¼")
    print(f"{'='*120}")
    print(f"ì´ ì‹¤í—˜ ìˆ˜: {len(df)}")

    print(f"\nğŸ“Š ì‹¤í—˜ë³„ ìƒì„¸ ì„±ëŠ¥:")

    # ì¶œë ¥ìš© DataFrame ìƒì„±
    display_df = df.copy()
    display_df['ì‹¤í—˜ëª…'] = display_df['experiment_name']
    display_df['ë„ë©”ì¸'] = display_df['domain']
    display_df['íƒ€ì„ìŠ¤íƒ¬í”„'] = display_df['timestamp']
    display_df['AUROC'] = display_df['auroc'].apply(lambda x: f"{x:.4f}" if pd.notna(x) else 'N/A')
    display_df['Threshold'] = display_df['optimal_threshold'].apply(lambda x: f"{x:.4f}" if pd.notna(x) else 'N/A')
    display_df['Precision'] = display_df['precision'].apply(lambda x: f"{x:.4f}" if pd.notna(x) else 'N/A')
    display_df['Recall'] = display_df['recall'].apply(lambda x: f"{x:.4f}" if pd.notna(x) else 'N/A')
    display_df['F1 Score'] = display_df['f1_score'].apply(lambda x: f"{x:.4f}" if pd.notna(x) else 'N/A')
    display_df['Test Accuracy'] = display_df['accuracy'].apply(lambda x: f"{x:.4f}" if pd.notna(x) else 'N/A')
    display_df['Val AUROC'] = display_df['val_auroc'].apply(lambda x: f"{x:.4f}" if pd.notna(x) else 'N/A')
    display_df['Val Precision'] = display_df['val_precision'].apply(lambda x: f"{x:.4f}" if pd.notna(x) else 'N/A')
    display_df['Val Recall'] = display_df['val_recall'].apply(lambda x: f"{x:.4f}" if pd.notna(x) else 'N/A')
    display_df['Val F1 Score'] = display_df['val_f1_score'].apply(lambda x: f"{x:.4f}" if pd.notna(x) else 'N/A')
    display_df['Val Accuracy'] = display_df['val_accuracy'].apply(lambda x: f"{x:.4f}" if pd.notna(x) else 'N/A')

    # Enhanced metrics ì»¬ëŸ¼ ì¶”ê°€ (ìˆëŠ” ê²½ìš°)
    if 'tpr_at_percentile_99_5' in df.columns:
        # 99.5 percentile
        display_df['TPR@99.5%'] = df['tpr_at_percentile_99_5'].apply(lambda x: f"{x:.4f}" if pd.notna(x) else 'N/A')
        display_df['Prec@99.5%'] = df['precision_at_percentile_99_5'].apply(lambda x: f"{x:.4f}" if pd.notna(x) else 'N/A')
        display_df['Thresh@99.5%'] = df['percentile_99_5_threshold'].apply(lambda x: f"{x:.4f}" if pd.notna(x) else 'N/A')
        # 99.9 percentile
        display_df['TPR@99.9%'] = df['tpr_at_percentile_99_9'].apply(lambda x: f"{x:.4f}" if pd.notna(x) else 'N/A')
        display_df['Prec@99.9%'] = df['precision_at_percentile_99_9'].apply(lambda x: f"{x:.4f}" if pd.notna(x) else 'N/A')
        display_df['Thresh@99.9%'] = df['percentile_99_9_threshold'].apply(lambda x: f"{x:.4f}" if pd.notna(x) else 'N/A')
        # Fixed 0.5
        display_df['TPR@0.5'] = df['tpr_at_fixed'].apply(lambda x: f"{x:.4f}" if pd.notna(x) else 'N/A')
        display_df['Prec@0.5'] = df['precision_at_fixed'].apply(lambda x: f"{x:.4f}" if pd.notna(x) else 'N/A')

    # ì¶œë ¥í•  ì¹¼ëŸ¼ë§Œ ì„ íƒ
    output_columns = [
        'ì‹¤í—˜ëª…', 'ë„ë©”ì¸', 'íƒ€ì„ìŠ¤íƒ¬í”„',
        'AUROC', 'Threshold', 'Precision', 'Recall', 'F1 Score', 'Test Accuracy',
        'Val AUROC', 'Val Precision', 'Val Recall', 'Val F1 Score', 'Val Accuracy'
    ]

    # Enhanced metrics ì»¬ëŸ¼ ì¶”ê°€ (ìˆëŠ” ê²½ìš°)
    if 'TPR@99.5%' in display_df.columns:
        output_columns.extend([
            'TPR@99.5%', 'Prec@99.5%', 'Thresh@99.5%',
            'TPR@99.9%', 'Prec@99.9%', 'Thresh@99.9%',
            'TPR@0.5', 'Prec@0.5'
        ])

    display_df = display_df[output_columns]

    # CSV í˜•íƒœë¡œ ì¶œë ¥ (comma separated)
    print(display_df.to_csv(index=False, lineterminator='\n'))

    # CSV ì €ì¥
    output_path = results_path / "single_domain_analysis.csv"
    df.to_csv(output_path, index=False, encoding='utf-8-sig')
    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥ë¨: {output_path}")

    # ========================================================================
    # ë‘ ë²ˆì§¸ View: ì‹¤í—˜ë³„ ë„ë©”ì¸ ë°˜ë³µ íšŸìˆ˜
    # ========================================================================
    print(f"\n{'='*120}")
    print(f"ğŸ“Š ì‹¤í—˜ë³„ ë„ë©”ì¸ ë°˜ë³µ íšŸìˆ˜")
    print(f"{'='*120}")

    # ì‹¤í—˜ ë²ˆí˜¸ ì¶”ì¶œ (exp-71.A -> exp-71)
    def extract_experiment_number(exp_name):
        if '.' in exp_name and exp_name.startswith('exp-'):
            # exp-71.A -> exp-71
            return exp_name.split('.')[0]
        return exp_name

    df['experiment_number'] = df['experiment_name'].apply(extract_experiment_number)

    # ì‹¤í—˜ ë²ˆí˜¸ë³„, ë„ë©”ì¸ë³„ ë°˜ë³µ íšŸìˆ˜ ê³„ì‚°
    count_data = []

    for exp_num in sorted(df['experiment_number'].unique()):
        exp_data = df[df['experiment_number'] == exp_num]

        row_data = {'ì‹¤í—˜ëª…': exp_num}

        # ê° ë„ë©”ì¸ì— ëŒ€í•´ ë°˜ë³µ íšŸìˆ˜ ê³„ì‚°
        for domain in ['A', 'B', 'C', 'D']:
            domain_count = len(exp_data[exp_data['domain'] == domain])
            row_data[f'Domain_{domain}'] = domain_count

        count_data.append(row_data)

    # DataFrame ìƒì„±
    count_df = pd.DataFrame(count_data)

    # CSV í˜•íƒœë¡œ ì¶œë ¥
    print(count_df.to_csv(index=False, lineterminator='\n'))

    # CSV ì €ì¥
    count_output_path = results_path / "experiment_domain_count.csv"
    count_df.to_csv(count_output_path, index=False, encoding='utf-8-sig')
    print(f"\nğŸ’¾ ì‹¤í—˜ë³„ ë„ë©”ì¸ ë°˜ë³µ íšŸìˆ˜ ì €ì¥ë¨: {count_output_path}")

    # ========================================================================
    # ì„¸ ë²ˆì§¸ View: ì‹¤í—˜ë³„ ë„ë©”ì¸ Accuracy í‰ê· /í‘œì¤€í¸ì°¨ ë§¤íŠ¸ë¦­ìŠ¤
    # ========================================================================
    print(f"\n{'='*120}")
    print(f"ğŸ“Š ì‹¤í—˜ë³„ ë„ë©”ì¸ Accuracy í‰ê· /í‘œì¤€í¸ì°¨")
    print(f"{'='*120}")

    # ì‹¤í—˜ ë²ˆí˜¸ ì¶”ì¶œ (exp-71.A -> exp-71)
    def extract_experiment_number(exp_name):
        if '.' in exp_name and exp_name.startswith('exp-'):
            # exp-71.A -> exp-71
            return exp_name.split('.')[0]
        return exp_name

    df['experiment_number'] = df['experiment_name'].apply(extract_experiment_number)

    summary_rows = []
    val_summary_rows = []
    for exp_num in sorted(df['experiment_number'].unique()):
        exp_data = df[df['experiment_number'] == exp_num]
        row = {'ì‹¤í—˜ëª…': exp_num}
        val_row = {'ì‹¤í—˜ëª…': exp_num}

        for domain in ['A', 'B', 'C', 'D']:
            test_data = exp_data[exp_data['domain'] == domain]['accuracy']
            if len(test_data) > 0:
                mean_test = test_data.mean()
                std_test = test_data.std() if len(test_data) > 1 else 0.0
                row[f'Domain_{domain}_Mean'] = f"{mean_test:.4f}"
                row[f'Domain_{domain}_Std'] = f"{std_test:.4f}"
            else:
                row[f'Domain_{domain}_Mean'] = 'N/A'
                row[f'Domain_{domain}_Std'] = 'N/A'

            val_data = exp_data[exp_data['domain'] == domain]['val_accuracy']
            if len(val_data) > 0:
                mean_val = val_data.mean()
                std_val = val_data.std() if len(val_data) > 1 else 0.0
                val_row[f'Val_Domain_{domain}_Mean'] = f"{mean_val:.4f}"
                val_row[f'Val_Domain_{domain}_Std'] = f"{std_val:.4f}"
            else:
                val_row[f'Val_Domain_{domain}_Mean'] = 'N/A'
                val_row[f'Val_Domain_{domain}_Std'] = 'N/A'

        summary_rows.append(row)
        val_summary_rows.append(val_row)

    summary_df = pd.DataFrame(summary_rows)
    summary_df['Overall_Mean'] = df.groupby('experiment_number')['accuracy'].mean().apply(lambda x: f"{x:.4f}").values
    summary_df['Overall_Std'] = df.groupby('experiment_number')['accuracy'].std().fillna(0).apply(lambda x: f"{x:.4f}").values
    print(summary_df.to_csv(index=False, lineterminator='\n'))
    summary_output_path = results_path / "experiment_domain_accuracy_summary.csv"
    summary_df.to_csv(summary_output_path, index=False, encoding='utf-8-sig')
    print(f"\nğŸ’¾ ì‹¤í—˜ë³„ ë„ë©”ì¸ Test Accuracy ìš”ì•½ ì €ì¥ë¨: {summary_output_path}")

    val_summary_df = pd.DataFrame(val_summary_rows)
    val_summary_df['Val_Overall_Mean'] = df.groupby('experiment_number')['val_accuracy'].mean().apply(lambda x: f"{x:.4f}").values
    val_summary_df['Val_Overall_Std'] = df.groupby('experiment_number')['val_accuracy'].std().fillna(0).apply(lambda x: f"{x:.4f}").values
    print(val_summary_df.to_csv(index=False, lineterminator='\n'))
    val_summary_output_path = results_path / "experiment_domain_val_accuracy_summary.csv"
    val_summary_df.to_csv(val_summary_output_path, index=False, encoding='utf-8-sig')
    print(f"\nğŸ’¾ ì‹¤í—˜ë³„ ë„ë©”ì¸ Validation Accuracy ìš”ì•½ ì €ì¥ë¨: {val_summary_output_path}")

    # ========================================================================
    # ë„¤ ë²ˆì§¸ View: Enhanced Metrics ìš”ì•½ (TPR, Precision at different thresholds)
    # ========================================================================
    if 'tpr_at_percentile_99_5' in df.columns:
        print(f"\n{'='*120}")
        print(f"ğŸ“Š ì‹¤í—˜ë³„ Enhanced Metrics ìš”ì•½ (TPR & Precision @ Different Thresholds)")
        print(f"{'='*120}")

        enhanced_summary_rows = []
        for exp_num in sorted(df['experiment_number'].unique()):
            exp_data = df[df['experiment_number'] == exp_num]
            row = {'ì‹¤í—˜ëª…': exp_num}

            for domain in ['A', 'B', 'C', 'D']:
                domain_data = exp_data[exp_data['domain'] == domain]

                # TPR @ 99.5 percentile
                tpr_995_data = domain_data['tpr_at_percentile_99_5']
                if len(tpr_995_data) > 0:
                    mean_tpr = tpr_995_data.mean()
                    std_tpr = tpr_995_data.std() if len(tpr_995_data) > 1 else 0.0
                    row[f'Domain_{domain}_TPR@99.5%_Mean'] = f"{mean_tpr:.4f}"
                    row[f'Domain_{domain}_TPR@99.5%_Std'] = f"{std_tpr:.4f}"
                else:
                    row[f'Domain_{domain}_TPR@99.5%_Mean'] = 'N/A'
                    row[f'Domain_{domain}_TPR@99.5%_Std'] = 'N/A'

                # Precision @ 99.5 percentile
                prec_995_data = domain_data['precision_at_percentile_99_5']
                if len(prec_995_data) > 0:
                    mean_prec = prec_995_data.mean()
                    std_prec = prec_995_data.std() if len(prec_995_data) > 1 else 0.0
                    row[f'Domain_{domain}_Prec@99.5%_Mean'] = f"{mean_prec:.4f}"
                    row[f'Domain_{domain}_Prec@99.5%_Std'] = f"{std_prec:.4f}"
                else:
                    row[f'Domain_{domain}_Prec@99.5%_Mean'] = 'N/A'
                    row[f'Domain_{domain}_Prec@99.5%_Std'] = 'N/A'

                # TPR @ 99.9 percentile
                tpr_999_data = domain_data['tpr_at_percentile_99_9']
                if len(tpr_999_data) > 0:
                    mean_tpr = tpr_999_data.mean()
                    std_tpr = tpr_999_data.std() if len(tpr_999_data) > 1 else 0.0
                    row[f'Domain_{domain}_TPR@99.9%_Mean'] = f"{mean_tpr:.4f}"
                    row[f'Domain_{domain}_TPR@99.9%_Std'] = f"{std_tpr:.4f}"
                else:
                    row[f'Domain_{domain}_TPR@99.9%_Mean'] = 'N/A'
                    row[f'Domain_{domain}_TPR@99.9%_Std'] = 'N/A'

                # Precision @ 99.9 percentile
                prec_999_data = domain_data['precision_at_percentile_99_9']
                if len(prec_999_data) > 0:
                    mean_prec = prec_999_data.mean()
                    std_prec = prec_999_data.std() if len(prec_999_data) > 1 else 0.0
                    row[f'Domain_{domain}_Prec@99.9%_Mean'] = f"{mean_prec:.4f}"
                    row[f'Domain_{domain}_Prec@99.9%_Std'] = f"{std_prec:.4f}"
                else:
                    row[f'Domain_{domain}_Prec@99.9%_Mean'] = 'N/A'
                    row[f'Domain_{domain}_Prec@99.9%_Std'] = 'N/A'

                # TPR @ Fixed 0.5
                tpr_fixed_data = domain_data['tpr_at_fixed']
                if len(tpr_fixed_data) > 0:
                    mean_tpr_fixed = tpr_fixed_data.mean()
                    std_tpr_fixed = tpr_fixed_data.std() if len(tpr_fixed_data) > 1 else 0.0
                    row[f'Domain_{domain}_TPR@0.5_Mean'] = f"{mean_tpr_fixed:.4f}"
                    row[f'Domain_{domain}_TPR@0.5_Std'] = f"{std_tpr_fixed:.4f}"
                else:
                    row[f'Domain_{domain}_TPR@0.5_Mean'] = 'N/A'
                    row[f'Domain_{domain}_TPR@0.5_Std'] = 'N/A'

            enhanced_summary_rows.append(row)

        enhanced_summary_df = pd.DataFrame(enhanced_summary_rows)

        # Overall statistics
        enhanced_summary_df['TPR@99.5%_Overall_Mean'] = df.groupby('experiment_number')['tpr_at_percentile_99_5'].mean().apply(lambda x: f"{x:.4f}").values
        enhanced_summary_df['TPR@99.5%_Overall_Std'] = df.groupby('experiment_number')['tpr_at_percentile_99_5'].std().fillna(0).apply(lambda x: f"{x:.4f}").values
        enhanced_summary_df['Prec@99.5%_Overall_Mean'] = df.groupby('experiment_number')['precision_at_percentile_99_5'].mean().apply(lambda x: f"{x:.4f}").values
        enhanced_summary_df['Prec@99.5%_Overall_Std'] = df.groupby('experiment_number')['precision_at_percentile_99_5'].std().fillna(0).apply(lambda x: f"{x:.4f}").values
        enhanced_summary_df['TPR@99.9%_Overall_Mean'] = df.groupby('experiment_number')['tpr_at_percentile_99_9'].mean().apply(lambda x: f"{x:.4f}").values
        enhanced_summary_df['TPR@99.9%_Overall_Std'] = df.groupby('experiment_number')['tpr_at_percentile_99_9'].std().fillna(0).apply(lambda x: f"{x:.4f}").values
        enhanced_summary_df['Prec@99.9%_Overall_Mean'] = df.groupby('experiment_number')['precision_at_percentile_99_9'].mean().apply(lambda x: f"{x:.4f}").values
        enhanced_summary_df['Prec@99.9%_Overall_Std'] = df.groupby('experiment_number')['precision_at_percentile_99_9'].std().fillna(0).apply(lambda x: f"{x:.4f}").values
        enhanced_summary_df['TPR@0.5_Overall_Mean'] = df.groupby('experiment_number')['tpr_at_fixed'].mean().apply(lambda x: f"{x:.4f}").values
        enhanced_summary_df['TPR@0.5_Overall_Std'] = df.groupby('experiment_number')['tpr_at_fixed'].std().fillna(0).apply(lambda x: f"{x:.4f}").values

        print(enhanced_summary_df.to_csv(index=False, lineterminator='\n'))
        enhanced_summary_output_path = results_path / "experiment_enhanced_metrics_summary.csv"
        enhanced_summary_df.to_csv(enhanced_summary_output_path, index=False, encoding='utf-8-sig')
        print(f"\nğŸ’¾ ì‹¤í—˜ë³„ Enhanced Metrics ìš”ì•½ ì €ì¥ë¨: {enhanced_summary_output_path}")

    # ========================================================================
    # ë‹¤ì„¯ ë²ˆì§¸ View: Thresholdë³„ Test Accuracy ë¹„êµ (ë…¼ë¬¸ìš© - ë„ë©”ì¸ë³„)
    # ========================================================================
    if 'tpr_at_percentile_99_5' in df.columns and 'fpr_at_percentile_99_5' in df.columns:
        print(f"\n{'='*120}")
        print(f"ğŸ“Š ë…¼ë¬¸ìš©: Thresholdë³„ Test Accuracy ë¹„êµ (Balanced Test Set)")
        print(f"{'='*120}")

        # Accuracy ê³„ì‚° í•¨ìˆ˜
        def calculate_accuracy_from_tpr_fpr(tpr, fpr):
            """
            Balanced test setì—ì„œ TPRê³¼ FPRë¡œë¶€í„° Accuracy ê³„ì‚°
            Accuracy = (TP + TN) / Total = (TPR + (1-FPR)) / 2  (when P=N, balanced)
            """
            return (tpr + (1 - fpr)) / 2

        # ========== Table 1: Accuracy @ 99.5%-ile Threshold ==========
        print(f"\n{'='*120}")
        print(f"ğŸ“‹ Table 1: Test Accuracy @ 99.5%-ile Threshold (from Validation)")
        print(f"{'='*120}")

        acc_995_rows = []
        for exp_num in sorted(df['experiment_number'].unique()):
            exp_data = df[df['experiment_number'] == exp_num]
            row = {'ì‹¤í—˜ëª…': exp_num}

            for domain in ['A', 'B', 'C', 'D']:
                domain_data = exp_data[exp_data['domain'] == domain]
                if len(domain_data) > 0 and 'fpr_at_percentile_99_5' in domain_data.columns:
                    tpr = domain_data['tpr_at_percentile_99_5']
                    fpr = domain_data['fpr_at_percentile_99_5']
                    acc = (tpr + (1 - fpr)) / 2
                    mean_acc = acc.mean()
                    std_acc = acc.std() if len(acc) > 1 else 0.0
                    row[f'Domain_{domain}_Mean'] = f"{mean_acc:.4f}"
                    row[f'Domain_{domain}_Std'] = f"{std_acc:.4f}"
                else:
                    row[f'Domain_{domain}_Mean'] = 'N/A'
                    row[f'Domain_{domain}_Std'] = 'N/A'

            # Overall
            tpr_all = exp_data['tpr_at_percentile_99_5']
            fpr_all = exp_data['fpr_at_percentile_99_5']
            acc_all = (tpr_all + (1 - fpr_all)) / 2
            row['Overall_Mean'] = f"{acc_all.mean():.4f}"
            row['Overall_Std'] = f"{acc_all.std():.4f}" if len(acc_all) > 1 else "0.0000"

            acc_995_rows.append(row)

        acc_995_df = pd.DataFrame(acc_995_rows)
        print(acc_995_df.to_csv(index=False, lineterminator='\n'))
        acc_995_output_path = results_path / "accuracy_at_99_5_percentile.csv"
        acc_995_df.to_csv(acc_995_output_path, index=False, encoding='utf-8-sig')
        print(f"ğŸ’¾ ì €ì¥ë¨: {acc_995_output_path}")

        # ========== Table 2: Accuracy @ 99.9%-ile Threshold ==========
        print(f"\n{'='*120}")
        print(f"ğŸ“‹ Table 2: Test Accuracy @ 99.9%-ile Threshold (from Validation)")
        print(f"{'='*120}")

        acc_999_rows = []
        for exp_num in sorted(df['experiment_number'].unique()):
            exp_data = df[df['experiment_number'] == exp_num]
            row = {'ì‹¤í—˜ëª…': exp_num}

            for domain in ['A', 'B', 'C', 'D']:
                domain_data = exp_data[exp_data['domain'] == domain]
                if len(domain_data) > 0 and 'fpr_at_percentile_99_9' in domain_data.columns:
                    tpr = domain_data['tpr_at_percentile_99_9']
                    fpr = domain_data['fpr_at_percentile_99_9']
                    acc = (tpr + (1 - fpr)) / 2
                    mean_acc = acc.mean()
                    std_acc = acc.std() if len(acc) > 1 else 0.0
                    row[f'Domain_{domain}_Mean'] = f"{mean_acc:.4f}"
                    row[f'Domain_{domain}_Std'] = f"{std_acc:.4f}"
                else:
                    row[f'Domain_{domain}_Mean'] = 'N/A'
                    row[f'Domain_{domain}_Std'] = 'N/A'

            # Overall
            tpr_all = exp_data['tpr_at_percentile_99_9']
            fpr_all = exp_data['fpr_at_percentile_99_9']
            acc_all = (tpr_all + (1 - fpr_all)) / 2
            row['Overall_Mean'] = f"{acc_all.mean():.4f}"
            row['Overall_Std'] = f"{acc_all.std():.4f}" if len(acc_all) > 1 else "0.0000"

            acc_999_rows.append(row)

        acc_999_df = pd.DataFrame(acc_999_rows)
        print(acc_999_df.to_csv(index=False, lineterminator='\n'))
        acc_999_output_path = results_path / "accuracy_at_99_9_percentile.csv"
        acc_999_df.to_csv(acc_999_output_path, index=False, encoding='utf-8-sig')
        print(f"ğŸ’¾ ì €ì¥ë¨: {acc_999_output_path}")

        # ========== Table 3: Accuracy @ Fixed 0.5 Threshold ==========
        print(f"\n{'='*120}")
        print(f"ğŸ“‹ Table 3: Test Accuracy @ Fixed 0.5 Threshold (No Validation)")
        print(f"{'='*120}")

        acc_05_rows = []
        for exp_num in sorted(df['experiment_number'].unique()):
            exp_data = df[df['experiment_number'] == exp_num]
            row = {'ì‹¤í—˜ëª…': exp_num}

            for domain in ['A', 'B', 'C', 'D']:
                domain_data = exp_data[exp_data['domain'] == domain]
                if len(domain_data) > 0:
                    acc = domain_data['accuracy']
                    mean_acc = acc.mean()
                    std_acc = acc.std() if len(acc) > 1 else 0.0
                    row[f'Domain_{domain}_Mean'] = f"{mean_acc:.4f}"
                    row[f'Domain_{domain}_Std'] = f"{std_acc:.4f}"
                else:
                    row[f'Domain_{domain}_Mean'] = 'N/A'
                    row[f'Domain_{domain}_Std'] = 'N/A'

            # Overall
            acc_all = exp_data['accuracy']
            row['Overall_Mean'] = f"{acc_all.mean():.4f}"
            row['Overall_Std'] = f"{acc_all.std():.4f}" if len(acc_all) > 1 else "0.0000"

            acc_05_rows.append(row)

        acc_05_df = pd.DataFrame(acc_05_rows)
        print(acc_05_df.to_csv(index=False, lineterminator='\n'))
        acc_05_output_path = results_path / "accuracy_at_fixed_0_5.csv"
        acc_05_df.to_csv(acc_05_output_path, index=False, encoding='utf-8-sig')
        print(f"ğŸ’¾ ì €ì¥ë¨: {acc_05_output_path}")

    return df


def main():
    parser = argparse.ArgumentParser(description='Single-Domain ì‹¤í—˜ ê²°ê³¼ ë¶„ì„')
    parser.add_argument(
        '--results_dir',
        type=str,
        required=True,
        help='ì‹¤í—˜ ê²°ê³¼ ë””ë ‰í† ë¦¬ ê²½ë¡œ (ì˜ˆ: results_100k_train)'
    )

    args = parser.parse_args()

    # ë¶„ì„ ì‹¤í–‰
    analyze_single_domain_experiments(args.results_dir)


if __name__ == "__main__":
    main()
