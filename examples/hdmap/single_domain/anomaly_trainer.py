#!/usr/bin/env python3
"""
BaseAnomalyTrainer - í†µí•© Anomaly Detection ëª¨ë¸ í›ˆë ¨ì„ ìœ„í•œ ë² ì´ìŠ¤ í´ë˜ìŠ¤

ì´ ëª¨ë“ˆì€ ëª¨ë“  anomaly detection ëª¨ë¸ì˜ í›ˆë ¨ì„ í†µí•© ê´€ë¦¬í•©ë‹ˆë‹¤.

ì§€ì› ëª¨ë¸:
- DRAEM: Reconstruction + Anomaly Detection
- DRAEM CutPaste: DRAEM with CutPaste augmentation (without severity head)
- DRAEM CutPaste Clf: DRAEM with CutPaste augmentation + CNN classification
- Dinomaly: Vision Transformer ê¸°ë°˜ anomaly detection with DINOv2
- PatchCore: Memory bank ê¸°ë°˜ few-shot anomaly detection
"""

import torch
import os
import sys
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Tuple

# ê³µí†µ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ import
sys.path.append(str(Path(__file__).parent.parent))
from experiment_utils import (
    cleanup_gpu_memory,
    setup_experiment_logging,
    extract_training_info,
    save_experiment_results,
    create_single_domain_datamodule,
    analyze_test_data_distribution,
    unified_model_evaluation
)

# ëª¨ë¸ë³„ imports
from anomalib.models.image.draem import Draem
from anomalib.models.image.draem_cutpaste import DraemCutPaste
from anomalib.models.image.draem_cutpaste_clf import DraemCutPasteClf
from anomalib.models.image import Dinomaly, Patchcore, EfficientAd, Fastflow, Padim, Supersimplenet, ReverseDistillation, CutPasteClassifier
from anomalib.models.image.reverse_distillation.anomaly_map import AnomalyMapGenerationMode
from anomalib.engine import Engine
from pytorch_lightning.loggers import TensorBoardLogger
from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint
from anomalib.metrics import AUROC, Evaluator



class BaseAnomalyTrainer:
    """í†µí•© Anomaly Detection ëª¨ë¸ í›ˆë ¨ì„ ìœ„í•œ ë² ì´ìŠ¤ í´ë˜ìŠ¤"""
    
    def __init__(self, config: Dict[str, Any], experiment_name: str, session_timestamp: str, experiment_dir: str = None):
        """
        Args:
            config: ì‹¤í—˜ ì„¤ì • ë”•ì…”ë„ˆë¦¬ (model_type í¬í•¨)
            experiment_name: ì‹¤í—˜ ì´ë¦„
            session_timestamp: ì „ì²´ ì„¸ì…˜ì˜ timestamp
            experiment_dir: ì™¸ë¶€ì—ì„œ ì§€ì •í•œ ì‹¤í—˜ ë””ë ‰í„°ë¦¬ (ì„ íƒì )
        """
        self.config = config
        self.experiment_name = experiment_name
        self.session_timestamp = session_timestamp
        self.model_type = config.get("model_type", "").lower()
        self.external_experiment_dir = experiment_dir
        self.setup_paths()
        
    def setup_paths(self):
        """ì‹¤í—˜ ê²½ë¡œ ì„¤ì •"""
        if self.external_experiment_dir:
            # bash ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ì „ë‹¬ë°›ì€ ë””ë ‰í„°ë¦¬ ì‚¬ìš©
            self.experiment_dir = Path(self.external_experiment_dir)
            self.results_dir = self.experiment_dir.parent
        else:
            # í˜¸í™˜ì„±ì„ ìœ„í•œ ê¸°ë³¸ ë°©ì‹ (ë‹¨ë… ì‹¤í–‰ ì‹œ)
            self.results_dir = Path("results") / self.session_timestamp
            self.experiment_dir = self.results_dir / f"{self.experiment_name}_{self.session_timestamp}"
            self.experiment_dir.mkdir(parents=True, exist_ok=True)
        
    def create_model(self):
        """Factory patternìœ¼ë¡œ ëª¨ë¸ ìƒì„±"""
        if self.model_type == "draem":
            return self._create_draem_model()
        elif self.model_type == "draem_cutpaste":
            return self._create_draem_cutpaste_model()
        elif self.model_type == "draem_cutpaste_clf":
            return self._create_draem_cutpaste_clf_model()
        elif self.model_type == "cutpaste_clf":
            return self._create_cutpaste_clf_model()
        elif self.model_type == "dinomaly":
            return self._create_dinomaly_model()
        elif self.model_type == "patchcore":
            return self._create_patchcore_model()
        elif self.model_type == "efficient_ad":
            return self._create_efficient_ad_model()
        elif self.model_type == "fastflow":
            return self._create_fastflow_model()
        elif self.model_type == "padim":
            return self._create_padim_model()
        elif self.model_type == "supersimplenet":
            return self._create_supersimplenet_model()
        elif self.model_type == "reverse_distillation":
            return self._create_reverse_distillation_model()
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…: {self.model_type}")
    
    def _create_draem_model(self):
        """DRAEM ëª¨ë¸ ìƒì„±"""
        # ëª…ì‹œì ìœ¼ë¡œ test_image_AUROC ë©”íŠ¸ë¦­ ì„¤ì •
        val_auroc = AUROC(fields=["pred_score", "gt_label"], prefix="val_image_")
        test_auroc = AUROC(fields=["pred_score", "gt_label"], prefix="test_image_")
        evaluator = Evaluator(val_metrics=[val_auroc], test_metrics=[test_auroc])
        
        # DRAEM ëª¨ë¸ ìƒì„±
        model = Draem(evaluator=evaluator)

        # í•™ìŠµ ì„¤ì •ì„ _training_configì— ì €ì¥ (configure_optimizersì—ì„œ ì‚¬ìš©ë¨)
        model._training_config = {
            'learning_rate': self.config["learning_rate"],
            'optimizer': self.config["optimizer"],
            'weight_decay': self.config["weight_decay"],
            'max_epochs': self.config["max_epochs"],
            'scheduler': self.config.get("scheduler", None),  # ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • (ì„ íƒì‚¬í•­)
        }
        
        return model

    def _create_draem_cutpaste_model(self):
        """DRAEM CutPaste ëª¨ë¸ ìƒì„± (without severity head)"""
        # ëª…ì‹œì ìœ¼ë¡œ test_image_AUROC ë©”íŠ¸ë¦­ ì„¤ì •
        val_auroc = AUROC(fields=["pred_score", "gt_label"], prefix="val_image_")
        test_auroc = AUROC(fields=["pred_score", "gt_label"], prefix="test_image_")
        evaluator = Evaluator(val_metrics=[val_auroc], test_metrics=[test_auroc])

        # ëª¨ë¸ íŒŒë¼ë¯¸í„° ì„¤ì •
        model_params = {
            'evaluator': evaluator,
            'enable_sspcab': self.config.get("sspcab", False),
            'cut_w_range': tuple(self.config["cut_w_range"]),
            'cut_h_range': tuple(self.config["cut_h_range"]),
            'a_fault_start': self.config["a_fault_start"],
            'a_fault_range_end': self.config["a_fault_range_end"],
            'augment_probability': self.config["augment_probability"],
            'focal_alpha': self.config.get("focal_alpha", 0.9),
        }
        
        # DraemCutPaste ëª¨ë¸ ìƒì„±
        model = DraemCutPaste(**model_params)

        # í•™ìŠµ ì„¤ì •ì„ _training_configì— ì €ì¥ (configure_optimizersì—ì„œ ì‚¬ìš©ë¨)
        model._training_config = {
            'learning_rate': self.config["learning_rate"],
            'optimizer': self.config["optimizer"],
            'weight_decay': self.config["weight_decay"],
            'max_epochs': self.config["max_epochs"],
            'scheduler': self.config.get("scheduler", None),  # ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • (ì„ íƒì‚¬í•­)
        }

        return model

    def _create_draem_cutpaste_clf_model(self):
        """DRAEM CutPaste Classification ëª¨ë¸ ìƒì„±"""
        # ëª…ì‹œì ìœ¼ë¡œ test_image_AUROC ë©”íŠ¸ë¦­ ì„¤ì •
        val_auroc = AUROC(fields=["pred_score", "gt_label"], prefix="val_image_")
        test_auroc = AUROC(fields=["pred_score", "gt_label"], prefix="test_image_")
        evaluator = Evaluator(val_metrics=[val_auroc], test_metrics=[test_auroc])

        # ëª¨ë¸ íŒŒë¼ë¯¸í„° ì„¤ì • - configì—ì„œë§Œ ê°’ í• ë‹¹
        model_params = {
            'evaluator': evaluator,
            'sspcab': self.config["sspcab"],
            'image_size': tuple(self.config["image_size"]),
            'severity_dropout': self.config["severity_dropout"],
            'severity_input_channels': self.config["severity_input_channels"],
            'cut_w_range': tuple(self.config["cut_w_range"]),
            'cut_h_range': tuple(self.config["cut_h_range"]),
            'a_fault_start': self.config["a_fault_start"],
            'a_fault_range_end': self.config["a_fault_range_end"],
            'augment_probability': self.config["augment_probability"],
            'clf_weight': self.config["clf_weight"],
            'focal_alpha': self.config.get("focal_alpha", 0.9),
        }

        # DraemCutPasteClf ëª¨ë¸ ìƒì„±
        model = DraemCutPasteClf(**model_params)

        # í•™ìŠµ ì„¤ì •ì„ _training_configì— ì €ì¥ (configure_optimizersì—ì„œ ì‚¬ìš©ë¨)
        model._training_config = {
            'learning_rate': self.config["learning_rate"],
            'optimizer': self.config["optimizer"],
            'weight_decay': self.config["weight_decay"],
            'max_epochs': self.config["max_epochs"],
            'scheduler': self.config.get("scheduler", None),  # ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • (ì„ íƒì‚¬í•­)
        }

        return model

    def _create_dinomaly_model(self):
        """Dinomaly ëª¨ë¸ ìƒì„±"""
        # DinomalyëŠ” ê¸°ë³¸ evaluatorë¥¼ ì‚¬ìš©í•˜ì—¬ trainable íŒŒë¼ë¯¸í„° ì„¤ì • ë¬¸ì œ íšŒí”¼
        model = Dinomaly(
            encoder_name=self.config["encoder_name"],
            target_layers=self.config["target_layers"],
            bottleneck_dropout=self.config["bottleneck_dropout"],
            decoder_depth=self.config["decoder_depth"],
            remove_class_token=self.config["remove_class_token"],
            evaluator=True  # ê¸°ë³¸ evaluator ì‚¬ìš©
        )

        # í•™ìŠµ ì„¤ì •ì„ _training_configì— ì €ì¥ (configure_optimizersì—ì„œ ì‚¬ìš©ë¨)
        model._training_config = {
            'learning_rate': self.config["learning_rate"],
            'weight_decay': self.config["weight_decay"],
        }

        return model
    
    def _create_patchcore_model(self):
        """Patchcore ëª¨ë¸ ìƒì„±"""
        return Patchcore(
            backbone=self.config["backbone"],
            layers=self.config["layers"],
            pre_trained=self.config["pre_trained"],
            coreset_sampling_ratio=self.config["coreset_sampling_ratio"],
            num_neighbors=self.config["num_neighbors"]
        )
    
    def _create_efficient_ad_model(self):
        """EfficientAD ëª¨ë¸ ìƒì„±"""
        # ì ˆëŒ€ ê²½ë¡œë¡œ ImageNetteì™€ pretrained weights ê²½ë¡œ ì„¤ì •
        from pathlib import Path
        current_file = Path(__file__).resolve()
        project_root = current_file.parent.parent.parent.parent  # anomalib/ ë””ë ‰í† ë¦¬
        
        # HDMAP ë°ì´í„°ëŠ” gt_maskê°€ ì—†ìœ¼ë¯€ë¡œ image-level ë©”íŠ¸ë¦­ë§Œ ì‚¬ìš©
        val_auroc = AUROC(fields=["pred_score", "gt_label"], prefix="val_image_")
        test_auroc = AUROC(fields=["pred_score", "gt_label"], prefix="test_image_")
        evaluator = Evaluator(val_metrics=[val_auroc], test_metrics=[test_auroc])
        
        return EfficientAd(
            teacher_out_channels=self.config.get("teacher_out_channels", 384),
            model_size=self.config.get("model_size", "small"),
            lr=self.config.get("learning_rate", 0.0001),
            weight_decay=self.config.get("weight_decay", 0.00001),
            padding=self.config.get("padding", False),
            pad_maps=self.config.get("pad_maps", True),
            imagenet_dir=str(project_root / "datasets" / "imagenette"),
            evaluator=evaluator  # ì»¤ìŠ¤í…€ evaluator ì‚¬ìš©
        )
    
    def _create_fastflow_model(self):
        """FastFlow ëª¨ë¸ ìƒì„±"""
        # FastFlowëŠ” image-level ë©”íŠ¸ë¦­ë§Œ ì‚¬ìš© (gt_mask ì—†ìŒ)
        val_auroc = AUROC(fields=["pred_score", "gt_label"], prefix="val_image_")
        test_auroc = AUROC(fields=["pred_score", "gt_label"], prefix="test_image_")
        evaluator = Evaluator(val_metrics=[val_auroc], test_metrics=[test_auroc])

        model = Fastflow(
            backbone=self.config.get("backbone", "resnet18"),
            flow_steps=self.config.get("flow_steps", 8),
            conv3x3_only=self.config.get("conv3x3_only", False),
            hidden_ratio=self.config.get("hidden_ratio", 1.0),
            evaluator=evaluator
        )

        # í•™ìŠµ ì„¤ì •ì„ _training_configì— ì €ì¥
        model._training_config = {
            'learning_rate': self.config.get("learning_rate", 0.001),
            'weight_decay': self.config.get("weight_decay", 0.00001),
            'max_epochs': self.config["max_epochs"],
        }

        return model

    def _create_padim_model(self):
        """PaDiM ëª¨ë¸ ìƒì„± (Training-free, Memory Bank)"""
        # PaDiMì€ image-level ë©”íŠ¸ë¦­ë§Œ ì‚¬ìš© (gt_mask ì—†ìŒ)
        val_auroc = AUROC(fields=["pred_score", "gt_label"], prefix="val_image_")
        test_auroc = AUROC(fields=["pred_score", "gt_label"], prefix="test_image_")
        evaluator = Evaluator(val_metrics=[val_auroc], test_metrics=[test_auroc])

        layers = self.config.get("layers", ["layer1", "layer2", "layer3"])
        print(f"   ğŸ” PaDiM ëª¨ë¸ ìƒì„± - ìš”ì²­ëœ layers: {layers}")

        model = Padim(
            backbone=self.config.get("backbone", "resnet18"),
            layers=layers,
            pre_trained=self.config.get("pre_trained", True),
            n_features=self.config.get("n_features", None),  # None = use default (100 for resnet18)
            evaluator=evaluator
        )

        # ëª¨ë¸ ìƒì„± í›„ ì‹¤ì œ ì‚¬ìš©ë˜ëŠ” layers í™•ì¸
        if hasattr(model.model, 'layers'):
            print(f"   ğŸ” PaDiM ëª¨ë¸ ìƒì„± ì™„ë£Œ - ì‹¤ì œ layers: {model.model.layers}")
        if hasattr(model.model, 'feature_extractor') and hasattr(model.model.feature_extractor, 'layers'):
            print(f"   ğŸ” PaDiM feature_extractor layers: {model.model.feature_extractor.layers}")

        return model

    def _create_supersimplenet_model(self):
        """SuperSimpleNet ëª¨ë¸ ìƒì„±"""
        # SuperSimpleNetì€ image-level ë©”íŠ¸ë¦­ë§Œ ì‚¬ìš© (gt_mask ì—†ìŒ)
        val_auroc = AUROC(fields=["pred_score", "gt_label"], prefix="val_image_")
        test_auroc = AUROC(fields=["pred_score", "gt_label"], prefix="test_image_")
        evaluator = Evaluator(val_metrics=[val_auroc], test_metrics=[test_auroc])

        model = Supersimplenet(
            perlin_threshold=self.config.get("perlin_threshold", 0.2),
            backbone=self.config.get("backbone", "wide_resnet50_2.tv_in1k"),
            layers=self.config.get("layers", ["layer2", "layer3"]),
            supervised=self.config.get("supervised", False),
            evaluator=evaluator
        )

        # SuperSimpleNetì€ ë‚´ë¶€ì—ì„œ optimizerë¥¼ configureí•˜ë¯€ë¡œ ë³„ë„ ì„¤ì • ë¶ˆí•„ìš”
        # (AdamW with MultiStepLR scheduler)

        return model

    def _create_reverse_distillation_model(self):
        """Reverse Distillation ëª¨ë¸ ìƒì„±"""
        # Reverse Distillationì€ image-level ë©”íŠ¸ë¦­ë§Œ ì‚¬ìš© (gt_mask ì—†ìŒ)
        val_auroc = AUROC(fields=["pred_score", "gt_label"], prefix="val_image_")
        test_auroc = AUROC(fields=["pred_score", "gt_label"], prefix="test_image_")
        evaluator = Evaluator(val_metrics=[val_auroc], test_metrics=[test_auroc])

        model = ReverseDistillation(
            backbone=self.config.get("backbone", "wide_resnet50_2"),
            layers=self.config.get("layers", ["layer1", "layer2", "layer3"]),
            anomaly_map_mode=AnomalyMapGenerationMode.ADD,  # ê¸°ë³¸ê°’
            pre_trained=self.config.get("pre_trained", True),
            evaluator=evaluator
        )

        # Reverse Distillationì€ ë‚´ë¶€ì—ì„œ optimizerë¥¼ configureí•˜ë¯€ë¡œ ë³„ë„ ì„¤ì • ë¶ˆí•„ìš”
        # (Adam with lr=0.005, betas=(0.5, 0.99))

        return model

    def _create_cutpaste_clf_model(self):
        """CutPaste + Simple CNN Classifier ëª¨ë¸ ìƒì„± (Baseline)"""
        # CutPaste ClassifierëŠ” image-level ë©”íŠ¸ë¦­ë§Œ ì‚¬ìš© (gt_mask ì—†ìŒ)
        val_auroc = AUROC(fields=["pred_score", "gt_label"], prefix="val_image_")
        test_auroc = AUROC(fields=["pred_score", "gt_label"], prefix="test_image_")
        evaluator = Evaluator(val_metrics=[val_auroc], test_metrics=[test_auroc])

        model = CutPasteClassifier(
            image_size=tuple(self.config["image_size"]),
            cut_w_range=tuple(self.config["cut_w_range"]),
            cut_h_range=tuple(self.config["cut_h_range"]),
            a_fault_start=self.config["a_fault_start"],
            a_fault_range_end=self.config["a_fault_range_end"],
            augment_probability=self.config["augment_probability"],
            evaluator=evaluator,
            pre_processor=False  # HDMAPDataModuleì˜ target_sizeë¡œ ë¦¬ì‚¬ì´ì¦ˆë˜ë¯€ë¡œ PreProcessor ë¹„í™œì„±í™”
        )

        # í•™ìŠµ ì„¤ì •ì„ _training_configì— ì €ì¥
        model._training_config = {
            'learning_rate': self.config['learning_rate'],
            'optimizer': self.config['optimizer'],
            'weight_decay': self.config['weight_decay'],
            'max_epochs': self.config['max_epochs'],
            'scheduler': self.config.get('scheduler', None),
        }

        return model


    def create_datamodule(self):
        """ëª¨ë“  ëª¨ë¸ì— ê³µí†µìœ¼ë¡œ ì‚¬ìš©í•  ë°ì´í„° ëª¨ë“ˆ ìƒì„±"""
        # ë„ë©”ì¸ ì„¤ì • - ëª¨ë¸ë³„ë¡œ ë‹¤ë¥¸ í•„ë“œëª… ì²˜ë¦¬
        domain = self.config.get("source_domain") or self.config.get("domain")
        if not domain:
            raise ValueError("configì—ì„œ 'source_domain' ë˜ëŠ” 'domain' í•„ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # dataset_root í•„ìˆ˜ ì²´í¬ ë° ìƒëŒ€ ê²½ë¡œ ì²˜ë¦¬
        dataset_root = self.config.get("dataset_root")
        if not dataset_root:
            raise ValueError("configì—ì„œ 'dataset_root' í•„ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # ìƒëŒ€ ê²½ë¡œì¸ ê²½ìš° í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ë³€í™˜
        from pathlib import Path
        dataset_path = Path(dataset_root)
        if not dataset_path.is_absolute():
            # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì°¾ê¸° (anomalib ë””ë ‰í† ë¦¬ ê¸°ì¤€)
            current_file = Path(__file__).resolve()
            project_root = current_file.parent.parent.parent.parent  # 4ë‹¨ê³„ ìƒìœ„ = anomalib/
            dataset_root = str(project_root / dataset_root)
            print(f"   ğŸ“ ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜: {dataset_root}")
        
        # target_size ì„¤ì • (list -> tuple ë³€í™˜)
        target_size = self.config.get("target_size")
        print(f"\nğŸ” DEBUG create_datamodule:")
        print(f"   config.target_size = {self.config.get('target_size')}")
        print(f"   target_size variable = {target_size}")
        if target_size and isinstance(target_size, list) and len(target_size) == 2:
            target_size = tuple(target_size)
            print(f"   target_size after tuple conversion = {target_size}")
        elif target_size:
            raise ValueError("target_sizeëŠ” [height, width] í˜•íƒœì˜ ë¦¬ìŠ¤íŠ¸ì—¬ì•¼ í•©ë‹ˆë‹¤")

        val_split_mode = self.config.get("val_split_mode", "FROM_TEST")

        return create_single_domain_datamodule(
            domain=domain,
            dataset_root=dataset_root,
            batch_size=self.config["batch_size"],
            target_size=target_size,
            resize_method=self.config.get("resize_method", "resize"),
            val_split_ratio=self.config["val_split_ratio"],
            val_split_mode=val_split_mode,
            num_workers=self.config["num_workers"],
            seed=self.config["seed"]
        )
    
    def print_data_statistics(self, datamodule, logger):
        """ë°ì´í„°ì…‹ì˜ í†µê³„ëŸ‰ ì¶œë ¥"""
        print(f"\nğŸ“Š ë°ì´í„° ë¶„í¬ í†µê³„ëŸ‰:")
        logger.info("ğŸ“Š ë°ì´í„° ë¶„í¬ í†µê³„ëŸ‰:")

        import torch

        # Train ë°ì´í„° í†µê³„
        try:
            train_loader = datamodule.train_dataloader()
            train_batch = next(iter(train_loader))
            train_images = train_batch.image

            train_min = train_images.min().item()
            train_max = train_images.max().item()
            train_mean = train_images.mean().item()
            train_std = train_images.std().item()

            print(f"   ğŸ“ˆ Train ë°ì´í„°:")
            print(f"      - ë²”ìœ„: [{train_min:.6f}, {train_max:.6f}]")
            print(f"      - í‰ê· : {train_mean:.6f}")
            print(f"      - í‘œì¤€í¸ì°¨: {train_std:.6f}")
            logger.info(f"Train ë°ì´í„° - ë²”ìœ„: [{train_min:.6f}, {train_max:.6f}], í‰ê· : {train_mean:.6f}, í‘œì¤€í¸ì°¨: {train_std:.6f}")
        except Exception as e:
            print(f"   âš ï¸ Train ë°ì´í„° í†µê³„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            logger.warning(f"Train ë°ì´í„° í†µê³„ ê³„ì‚° ì‹¤íŒ¨: {e}")

        # Val ë°ì´í„° í†µê³„
        try:
            val_loader = datamodule.val_dataloader()
            val_batch = next(iter(val_loader))
            val_images = val_batch.image

            val_min = val_images.min().item()
            val_max = val_images.max().item()
            val_mean = val_images.mean().item()
            val_std = val_images.std().item()

            print(f"   ğŸ“Š Val ë°ì´í„°:")
            print(f"      - ë²”ìœ„: [{val_min:.6f}, {val_max:.6f}]")
            print(f"      - í‰ê· : {val_mean:.6f}")
            print(f"      - í‘œì¤€í¸ì°¨: {val_std:.6f}")
            logger.info(f"Val ë°ì´í„° - ë²”ìœ„: [{val_min:.6f}, {val_max:.6f}], í‰ê· : {val_mean:.6f}, í‘œì¤€í¸ì°¨: {val_std:.6f}")
        except Exception as e:
            print(f"   âš ï¸ Val ë°ì´í„° í†µê³„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            logger.warning(f"Val ë°ì´í„° í†µê³„ ê³„ì‚° ì‹¤íŒ¨: {e}")

        # Test ë°ì´í„° í†µê³„
        try:
            test_loader = datamodule.test_dataloader()
            test_batch = next(iter(test_loader))
            test_images = test_batch.image

            test_min = test_images.min().item()
            test_max = test_images.max().item()
            test_mean = test_images.mean().item()
            test_std = test_images.std().item()

            print(f"   ğŸ§ª Test ë°ì´í„°:")
            print(f"      - ë²”ìœ„: [{test_min:.6f}, {test_max:.6f}]")
            print(f"      - í‰ê· : {test_mean:.6f}")
            print(f"      - í‘œì¤€í¸ì°¨: {test_std:.6f}")
            logger.info(f"Test ë°ì´í„° - ë²”ìœ„: [{test_min:.6f}, {test_max:.6f}], í‰ê· : {test_mean:.6f}, í‘œì¤€í¸ì°¨: {test_std:.6f}")
        except Exception as e:
            print(f"   âš ï¸ Test ë°ì´í„° í†µê³„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            logger.warning(f"Test ë°ì´í„° í†µê³„ ê³„ì‚° ì‹¤íŒ¨: {e}")

        print()

    def create_callbacks(self):
        """ì½œë°± ì„¤ì • - ëª¨ë¸ë³„ ì ì ˆí•œ early stopping ë©”íŠ¸ë¦­ ì‚¬ìš©"""
        callbacks = []

        # ëª¨ë¸ë³„ EarlyStopping ì„¤ì •
        if self.model_type in ["patchcore", "padim"]:
            # PatchCoreì™€ PaDiMì€ training-free ë°©ì‹ì´ë¯€ë¡œ EarlyStoppingê³¼ ModelCheckpoint ë¶ˆí•„ìš”
            print(f"   â„¹ï¸ {self.model_type.upper()}: EarlyStopping ë° ModelCheckpoint ë¹„í™œì„±í™” (Training-free ë°©ì‹)")
            return []  # ë¹ˆ ì½œë°± ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

        else:
            # ëª¨ë¸ë³„ë¡œ ë‹¤ë¥¸ EarlyStopping monitor ì„¤ì •
            if self.model_type in ["draem", "draem_cutpaste", "draem_cutpaste_clf"]:
                # DRAEM ê³„ì—´: val_loss ê¸°ë°˜ EarlyStopping (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
                monitor_metric = "val_loss"
                monitor_mode = "min"
                print(f"   â„¹ï¸ {self.model_type.upper()}: EarlyStopping í™œì„±í™” (val_loss ëª¨ë‹ˆí„°ë§)")
            elif self.model_type in ["supersimplenet", "reverse_distillation"]:
                # SuperSimpleNet, Reverse Distillation: train_loss ê¸°ë°˜ EarlyStopping
                monitor_metric = "train_loss"
                monitor_mode = "min"
                print(f"   â„¹ï¸ {self.model_type.upper().replace('_', ' ')}: EarlyStopping í™œì„±í™” (train_loss ëª¨ë‹ˆí„°ë§)")
            elif self.model_type in ["efficient_ad", "fastflow", "cutpaste_clf"]:
                # EfficientAD, FastFlow, CutPaste Classifier: val_image_AUROC ê¸°ë°˜ EarlyStopping (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
                monitor_metric = "val_image_AUROC"
                monitor_mode = "max"
                print(f"   â„¹ï¸ {self.model_type.upper().replace('_', ' ')}: EarlyStopping í™œì„±í™” (val_image_AUROC ëª¨ë‹ˆí„°ë§)")
            else:
                # Dinomaly: val_loss ê¸°ë°˜ EarlyStopping
                monitor_metric = "val_loss"
                monitor_mode = "min"
                print(f"   â„¹ï¸ {self.model_type.upper()}: EarlyStopping í™œì„±í™” (val_loss ëª¨ë‹ˆí„°ë§)")

            early_stopping = EarlyStopping(
                monitor=monitor_metric,
                patience=self.config["early_stopping_patience"],
                mode=monitor_mode,
                verbose=True
            )
            callbacks.append(early_stopping)

            # Model Checkpoint
            domain = self.config.get("source_domain") or self.config.get("domain")

            if self.model_type in ["draem", "draem_cutpaste", "draem_cutpaste_clf"]:
                checkpoint = ModelCheckpoint(
                    filename=f"{self.model_type}_single_domain_{domain}_" + "{epoch:02d}_{val_loss:.4f}",
                    monitor="val_loss",
                    mode="min",
                    save_top_k=1,
                    verbose=True
                )
            elif self.model_type in ["supersimplenet", "reverse_distillation"]:
                checkpoint = ModelCheckpoint(
                    filename=f"{self.model_type}_single_domain_{domain}_" + "{epoch:02d}_{train_loss:.4f}",
                    monitor="train_loss",
                    mode="min",
                    save_top_k=1,
                    verbose=True
                )
            elif self.model_type in ["efficient_ad", "fastflow", "cutpaste_clf"]:
                checkpoint = ModelCheckpoint(
                    filename=f"{self.model_type}_single_domain_{domain}_" + "{epoch:02d}_{val_image_AUROC:.4f}",
                    monitor="val_image_AUROC",
                    mode="max",
                    save_top_k=1,
                    verbose=True
                )
            else:
                checkpoint = ModelCheckpoint(
                    filename=f"{self.model_type}_single_domain_{domain}_" + "{epoch:02d}_{val_loss:.4f}",
                    monitor="val_loss",
                    mode="min",
                    save_top_k=1,
                    verbose=True
                )
            callbacks.append(checkpoint)
        
        return callbacks
    
    def configure_optimizer(self, model):
        """ì˜µí‹°ë§ˆì´ì € ì„¤ì • - ëª¨ë“  ëª¨ë¸ ê³µí†µ"""
        # Training-free ëª¨ë¸ë“¤ì€ ì˜µí‹°ë§ˆì´ì €ê°€ í•„ìš”í•˜ì§€ ì•ŠìŒ
        if self.model_type in ["patchcore", "padim", "efficient_ad"]:
            return
                
    def train_model(self, model, datamodule, logger) -> Tuple[Any, Engine, str]:
        """ëª¨ë¸ í›ˆë ¨ ìˆ˜í–‰"""
        print(f"\nğŸš€ {self.model_type.upper()} ëª¨ë¸ í›ˆë ¨ ì‹œì‘")
        logger.info(f"ğŸš€ {self.model_type.upper()} ëª¨ë¸ í›ˆë ¨ ì‹œì‘")
        
        # Config ì„¤ì • ì¶œë ¥
        print(f"   ğŸ”§ Config ì„¤ì •:")
        print(f"      Model Type: {self.model_type}")

        # Training-free ëª¨ë¸ë“¤
        if self.model_type in ["patchcore", "padim"]:
            print(f"      Max Epochs: 1 (Training-free)")
        # ë‚´ë¶€ì—ì„œ optimizerë¥¼ configureí•˜ëŠ” ëª¨ë¸ë“¤ (learning_rateê°€ configì— ì—†ì„ ìˆ˜ ìˆìŒ)
        elif self.model_type in ["supersimplenet", "reverse_distillation"]:
            print(f"      Max Epochs: {self.config['max_epochs']}")
            print(f"      Early Stopping Patience: {self.config['early_stopping_patience']}")
            if self.model_type == "supersimplenet":
                print(f"      Learning Rate: AdamW with MultiStepLR (internal)")
            elif self.model_type == "reverse_distillation":
                print(f"      Learning Rate: 0.005 (internal, Adam)")
        # ì¼ë°˜ì ì¸ ëª¨ë¸ë“¤ (configì—ì„œ learning_rate ì‚¬ìš©)
        else:
            print(f"      Max Epochs: {self.config['max_epochs']}")
            print(f"      Learning Rate: {self.config['learning_rate']}")
            print(f"      Early Stopping Patience: {self.config['early_stopping_patience']}")

        print(f"      Batch Size: {self.config['batch_size']}")
        
        # ì˜µí‹°ë§ˆì´ì € ì„¤ì •
        self.configure_optimizer(model)
        
        # ì½œë°± ì„¤ì •
        callbacks = self.create_callbacks()
        
        analysis_dir = Path(self.experiment_dir) / "analysis"
        analysis_dir.mkdir(parents=True, exist_ok=True)

        if hasattr(model, "validation_analysis_dir"):
            model.validation_analysis_dir = analysis_dir

        self.tb_logger = TensorBoardLogger(
            save_dir=str(self.experiment_dir),
            name="tensorboard_logs",
            version=""
        )
        
        # Engine ì„¤ì •
        # Training-free ëª¨ë¸ë“¤ì€ 1 epochë§Œ í•„ìš”
        if self.model_type in ["patchcore", "padim"]:
            max_epochs = 1
        elif self.model_type == "efficient_ad":
            max_epochs = self.config["max_epochs"]
        else:
            max_epochs = self.config["max_epochs"]
        
        engine_kwargs = {
            "accelerator": "gpu" if torch.cuda.is_available() else "cpu",
            "devices": [0] if torch.cuda.is_available() else 1,
            "logger": self.tb_logger,
            "callbacks": callbacks,
            "enable_checkpointing": True,
            "log_every_n_steps": 10,
            "enable_model_summary": True,
            "default_root_dir": str(self.experiment_dir),
            "max_epochs": max_epochs,
            "check_val_every_n_epoch": 1,
            "num_sanity_val_steps": 0
        }
        
        if self.model_type in ["patchcore", "padim"]:
            print(f"   â„¹ï¸ {self.model_type.upper()}: max_epochs ê°•ì œ ì„¤ì • (1 epoch - Training-free)")
        elif self.model_type == "efficient_ad":
            print(f"   â„¹ï¸ EFFICIENT AD: max_epochs = {max_epochs} (íŠ¹ë³„í•œ í›ˆë ¨ ë°©ì‹)")
        else:
            print(f"   â„¹ï¸ {self.model_type.upper().replace('_', ' ')}: max_epochs = {max_epochs}")
        
        engine = Engine(**engine_kwargs)
        
        print(f"   ğŸ”§ Engine ì„¤ì • ì™„ë£Œ")
        print(f"   ğŸ“ ê²°ê³¼ ì €ì¥ ê²½ë¡œ: {self.experiment_dir}")
        logger.info(f"ğŸ”§ Engine ì„¤ì • ì™„ë£Œ")
        logger.info(f"ğŸ“ ê²°ê³¼ ì €ì¥ ê²½ë¡œ: {self.experiment_dir}")
        
        # ëª¨ë¸ í›ˆë ¨
        print(f"   ğŸ¯ ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")
        logger.info("ğŸ¯ ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")
        
        engine.fit(model=model, datamodule=datamodule)

        # ìµœê³  ì²´í¬í¬ì¸íŠ¸ ì°¾ê¸°
        best_checkpoint = ""
        for callback in callbacks:
            if isinstance(callback, ModelCheckpoint) and hasattr(callback, 'best_model_path'):
                best_checkpoint = callback.best_model_path
                break

        print(f"   ğŸ† Best Checkpoint: {best_checkpoint}")
        logger.info(f"ğŸ† Best Checkpoint: {best_checkpoint}")

        # Best checkpoint ì •ë³´ë§Œ ê¸°ë¡í•˜ê³  ë¡œë“œí•˜ì§€ ì•ŠìŒ (ë§ˆì§€ë§‰ epoch ëª¨ë¸ ì‚¬ìš©)
        if best_checkpoint and os.path.exists(best_checkpoint) and self.model_type not in ["patchcore", "padim", "efficient_ad"]:
            print(f"   ğŸ“‚ Best checkpoint ì €ì¥ë¨: {best_checkpoint}")
            print(f"   â„¹ï¸ í‰ê°€ëŠ” ë§ˆì§€ë§‰ epoch ëª¨ë¸ë¡œ ìˆ˜í–‰ë©ë‹ˆë‹¤.")
            logger.info(f"Best checkpoint ì €ì¥ë¨ (ì‚¬ìš©í•˜ì§€ ì•ŠìŒ): {best_checkpoint}")
        elif self.model_type in ["patchcore", "padim", "efficient_ad"]:
            print(f"   â„¹ï¸ {self.model_type.upper().replace('_', ' ')}: Best checkpoint ë¡œë“œ ê±´ë„ˆëœ€ (íŠ¹ë³„í•œ í›ˆë ¨ ë°©ì‹)")
        elif not best_checkpoint:
            print(f"   âš ï¸ Best checkpoint ì—†ìŒ - ë§ˆì§€ë§‰ epoch ëª¨ë¸ ì‚¬ìš©")
            logger.warning(f"Best checkpoint ì—†ìŒ - ë§ˆì§€ë§‰ epoch ëª¨ë¸ ì‚¬ìš©")

        print(f"   âœ… ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!")
        logger.info("âœ… ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!")

        return model, engine, best_checkpoint
    
    def evaluate_model(self, model, datamodule, logger) -> Dict[str, Any]:
        """ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ - í†µí•©ëœ ë‹¨ì¼ í‰ê°€"""
        domain = self.config.get("source_domain") or self.config.get("domain")
        
        print(f"\nğŸ“Š {domain} ë„ë©”ì¸ ì„±ëŠ¥ í‰ê°€ ì‹œì‘")
        logger.info(f"ğŸ“Š {domain} ë„ë©”ì¸ ì„±ëŠ¥ í‰ê°€ ì‹œì‘")
        
        try:
            # í†µí•©ëœ ì„±ëŠ¥ í‰ê°€ ìˆ˜í–‰
            evaluation_metrics = unified_model_evaluation(
                model, datamodule, self.experiment_dir, self.experiment_name, self.model_type, logger, visualization_interval=3
            )
            
            if evaluation_metrics:
                # TensorBoardì— test ë©”íŠ¸ë¦­ ë¡œê¹…
                if hasattr(self, 'tb_logger') and self.tb_logger is not None:
                    test_metrics = {
                        "test_auroc": evaluation_metrics["auroc"],
                        "test_f1_score": evaluation_metrics["f1_score"],
                        "test_accuracy": evaluation_metrics["accuracy"],
                        "test_precision": evaluation_metrics["precision"],
                        "test_recall": evaluation_metrics["recall"]
                    }
                    self.tb_logger.log_metrics(test_metrics, step=0)
                    print(f"   ğŸ“Š TensorBoardì— test ë©”íŠ¸ë¦­ ë¡œê¹… ì™„ë£Œ:")
                    print(f"      - test_auroc: {evaluation_metrics['auroc']:.4f}")
                    print(f"      - test_f1_score: {evaluation_metrics['f1_score']:.4f}")
                    logger.info(f"ğŸ“Š TensorBoardì— test ë©”íŠ¸ë¦­ ë¡œê¹… ì™„ë£Œ: AUROC={evaluation_metrics['auroc']:.4f}, F1={evaluation_metrics['f1_score']:.4f}")
                
                # ê²°ê³¼ ì •ë¦¬
                results = {
                    "domain": domain,
                    "image_AUROC": evaluation_metrics["auroc"],
                    "image_F1Score": evaluation_metrics["f1_score"],
                    "training_samples": len(datamodule.train_data),
                    "test_samples": len(datamodule.test_data),
                    "val_samples": len(datamodule.val_data) if datamodule.val_data else 0,
                    "evaluation_metrics": evaluation_metrics
                }
                
                # ìµœì¢… ê²°ê³¼ ì¶œë ¥
                print(f"   ğŸ¯ ìµœì¢… í‰ê°€ ê²°ê³¼:")
                print(f"      ğŸ“Š AUROC: {evaluation_metrics['auroc']:.4f}")
                print(f"      ğŸ¯ Accuracy: {evaluation_metrics['accuracy']:.4f}")
                print(f"      ğŸ“ˆ F1-Score: {evaluation_metrics['f1_score']:.4f}")
                print(f"      ğŸ” Precision: {evaluation_metrics['precision']:.4f}")
                print(f"      ğŸ“‰ Recall: {evaluation_metrics['recall']:.4f}")
                print(f"      ğŸ”¢ ì´ ìƒ˜í”Œ: {evaluation_metrics['total_samples']}ê°œ")

                # TensorBoardì— test_image_AUROC ë¡œê¹…
                if hasattr(logger, 'experiment') and hasattr(logger.experiment, 'add_scalar'):
                    logger.experiment.add_scalar(
                        'test_image_AUROC',
                        evaluation_metrics['auroc'],
                        global_step=0
                    )
                    print(f"      ğŸ“ TensorBoardì— test_image_AUROC={evaluation_metrics['auroc']:.4f} ë¡œê¹… ì™„ë£Œ")
                
                logger.info(f"âœ… {domain} í‰ê°€ ì™„ë£Œ: AUROC={evaluation_metrics['auroc']:.4f}")
                return results
            else:
                results = {"domain": domain, "error": "Evaluation failed"}
                logger.error(f"âŒ {domain} í‰ê°€ ì‹¤íŒ¨")
                return results
                
        except Exception as e:
            print(f"   âŒ í‰ê°€ ì‹¤íŒ¨: {str(e)}")
            logger.error(f"í‰ê°€ ì‹¤íŒ¨: {str(e)}")
            import traceback
            logger.error(f"Traceback: {traceback.format_exc()}")
            return {"domain": domain, "error": f"Evaluation failed: {str(e)}"}
        
    def save_results(self, results, training_info, best_checkpoint, logger):
        """ì‹¤í—˜ ê²°ê³¼ ì €ì¥"""
        domain = self.config.get("source_domain") or self.config.get("domain")
        
        experiment_results = {
            "experiment_name": self.experiment_name,
            "description": f"{domain} - {self.model_type.upper()} single domain training",
            "domain": domain,
            "config": self.config,
            "results": results,
            "training_info": training_info,
            "timestamp": datetime.now().isoformat(),
            "checkpoint_path": best_checkpoint,
            "status": "success",
            "condition": {
                "name": self.experiment_name,
                "description": f"{domain} - {self.model_type.upper()} single domain training",
                "config": {
                    "source_domain": domain,
                    **self.config
                }
            },
            "source_results": {
                "test_image_AUROC": results.get("image_AUROC", 0.0),
                "test_image_F1Score": results.get("image_F1Score", 0.0),
                "domain": domain
            },
            "target_results": {}  # Single domainì´ë¯€ë¡œ ë¹„ì›Œë‘ 
        }
        
        # ê²°ê³¼ ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        # ì‹¤í—˜ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ì— ì§ì ‘ ì €ì¥ (multi-domainê³¼ ë™ì¼)
        result_filename = f"result_{timestamp}.json"
        
        results_file = save_experiment_results(
            result=experiment_results,
            result_filename=result_filename,
            log_dir=Path(self.experiment_dir),  # ì‹¤í—˜ ë£¨íŠ¸ ë””ë ‰í† ë¦¬
            logger=logger,
            model_type=self.model_type.upper()
        )
        print(f"ğŸ“„ ì‹¤í—˜ ê²°ê³¼ ì €ì¥ë¨: {results_file}")
        
        return experiment_results
    
    
    def run_experiment(self) -> dict:
        """ì „ì²´ ì‹¤í—˜ ì‹¤í–‰"""
        domain = self.config.get("source_domain") or self.config.get("domain")

        print(f"ğŸ”¬ {self.model_type.upper()} Single Domain ì‹¤í—˜: {self.experiment_name}")

        try:
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            cleanup_gpu_memory()

            # ì™„ì „í•œ ì¬í˜„ì„± ì„¤ì • (ëª¨ë¸ ì´ˆê¸°í™”, augmentation, dataloader shuffle ì œì–´)
            seed = self.config.get("seed", 42)
            print(f"ğŸ² ì™„ì „í•œ ì¬í˜„ì„± ì„¤ì • (Seed: {seed})")

            import random
            import numpy as np
            import lightning.pytorch as pl

            # PyTorch Lightningì˜ seed_everythingìœ¼ë¡œ ëª¨ë“  ëœë¤ ìƒíƒœ ì œì–´
            pl.seed_everything(seed, workers=True)
            torch.manual_seed(seed)
            np.random.seed(seed)
            random.seed(seed)

            # GPU deterministic mode (ì¬í˜„ì„± ë³´ì¥, ì•½ê°„ì˜ ì„±ëŠ¥ ì €í•˜ ìˆì„ ìˆ˜ ìˆìŒ)
            if torch.cuda.is_available():
                torch.backends.cudnn.deterministic = True
                torch.backends.cudnn.benchmark = False

            print(f"   âœ… PyTorch, NumPy, Random ì‹œë“œ ì„¤ì • ì™„ë£Œ")
            print(f"   âœ… CUDA deterministic mode í™œì„±í™”")

            # ë¡œê¹… ì„¤ì •
            log_file_path = self.experiment_dir / f"{domain}_single.log"
            logger = setup_experiment_logging(str(log_file_path), self.experiment_name)
            logger.info(f"ğŸš€ {self.model_type.upper()} Single Domain ì‹¤í—˜ ì‹œì‘ (Seed: {seed})")
            
            # ëª¨ë¸ ìƒì„±
            model = self.create_model()
            
            # DataModule ìƒì„±
            datamodule = self.create_datamodule()
            
            # DataModule ë°ì´í„° ê°œìˆ˜ í™•ì¸
            print(f"\nğŸ“Š DataModule ì¤€ë¹„ ë° ì„¤ì • ì¤‘:")
            datamodule.prepare_data()
            datamodule.setup()
            
            train_size = len(datamodule.train_data) if datamodule.train_data else 0
            test_size = len(datamodule.test_data) if datamodule.test_data else 0
            val_size = len(datamodule.val_data) if datamodule.val_data else 0
            
            print(f"   ğŸ“ˆ í›ˆë ¨ ë°ì´í„°: {train_size:,}ê°œ | ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„°: {test_size:,}ê°œ | ğŸ“‹ ê²€ì¦ ë°ì´í„°: {val_size:,}ê°œ | ğŸ¯ ì´ ë°ì´í„°: {train_size + test_size + val_size:,}ê°œ")
            
            # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¼ë²¨ ë¶„í¬ í™•ì¸ (ì „ì²´ ë°ì´í„°)
            analyze_test_data_distribution(datamodule, test_size)

            # ë°ì´í„° ë¶„í¬ í†µê³„ëŸ‰ ì¶œë ¥
            self.print_data_statistics(datamodule, logger)

            # ëª¨ë¸ í›ˆë ¨
            trained_model, engine, best_checkpoint = self.train_model(model, datamodule, logger)
            
            # ì„±ëŠ¥ í‰ê°€
            results = self.evaluate_model(trained_model, datamodule, logger)
            
            # í›ˆë ¨ ì •ë³´ ì¶”ì¶œ
            training_info = extract_training_info(engine)
            
            # ê²°ê³¼ ì €ì¥
            experiment_results = self.save_results(results, training_info, best_checkpoint, logger)
                        
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            cleanup_gpu_memory()
            
            print(f"âœ… ì‹¤í—˜ ì™„ë£Œ: {self.experiment_name}")
            logger.info(f"âœ… ì‹¤í—˜ ì™„ë£Œ: {self.experiment_name}")
            
            return experiment_results
            
        except Exception as e:
            error_msg = f"âŒ ì‹¤í—˜ ì‹¤íŒ¨: {self.experiment_name} - {str(e)}"
            print(error_msg)
            if 'logger' in locals():
                logger.error(error_msg)
            
            cleanup_gpu_memory()
            
            return {
                "experiment_name": self.experiment_name,
                "error": str(e),
                "timestamp": datetime.now().isoformat(),
                "status": "failed"
            }
    
