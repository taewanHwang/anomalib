#!/usr/bin/env python3
"""
BaseAnomalyTrainer - í†µí•© Anomaly Detection ëª¨ë¸ í›ˆë ¨ì„ ìœ„í•œ ë² ì´ìŠ¤ í´ë˜ìŠ¤

ì´ ëª¨ë“ˆì€ ëª¨ë“  anomaly detection ëª¨ë¸ì˜ í›ˆë ¨ì„ í†µí•© ê´€ë¦¬í•©ë‹ˆë‹¤.

ì§€ì› ëª¨ë¸:
- DRAEM: Reconstruction + Anomaly Detection
- Dinomaly: Vision Transformer ê¸°ë°˜ anomaly detection with DINOv2
- PatchCore: Memory bank ê¸°ë°˜ few-shot anomaly detection  
- DRAEM-SevNet: Selective feature reconstruction
"""

import torch
import numpy as np
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Tuple

# ê³µí†µ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ import
from experiment_utils import (
    cleanup_gpu_memory,
    setup_experiment_logging,
    extract_training_info,
    save_experiment_results,
    create_experiment_visualization,
    create_single_domain_datamodule,
    save_detailed_test_results,
    plot_roc_curve,
    save_metrics_report,
    plot_score_distributions,
    save_extreme_samples,
    save_experiment_summary
)

# ëª¨ë¸ë³„ imports
from anomalib.models.image.draem import Draem
from anomalib.models.image.draem_sevnet import DraemSevNet
from anomalib.models.image import Dinomaly, Patchcore
from anomalib.engine import Engine
from pytorch_lightning.loggers import TensorBoardLogger
from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint
from anomalib.metrics import AUROC, Evaluator



class BaseAnomalyTrainer:
    """í†µí•© Anomaly Detection ëª¨ë¸ í›ˆë ¨ì„ ìœ„í•œ ë² ì´ìŠ¤ í´ë˜ìŠ¤"""
    
    def __init__(self, config: Dict[str, Any], experiment_name: str, session_timestamp: str, experiment_dir: str = None):
        """
        Args:
            config: ì‹¤í—˜ ì„¤ì • ë”•ì…”ë„ˆë¦¬ (model_type í¬í•¨)
            experiment_name: ì‹¤í—˜ ì´ë¦„
            session_timestamp: ì „ì²´ ì„¸ì…˜ì˜ timestamp
            experiment_dir: ì™¸ë¶€ì—ì„œ ì§€ì •í•œ ì‹¤í—˜ ë””ë ‰í„°ë¦¬ (ì„ íƒì )
        """
        self.config = config
        self.experiment_name = experiment_name
        self.session_timestamp = session_timestamp
        self.model_type = config.get("model_type", "").lower()
        self.external_experiment_dir = experiment_dir
        self.setup_paths()
        
    def setup_paths(self):
        """ì‹¤í—˜ ê²½ë¡œ ì„¤ì •"""
        if self.external_experiment_dir:
            # bash ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ì „ë‹¬ë°›ì€ ë””ë ‰í„°ë¦¬ ì‚¬ìš©
            self.experiment_dir = Path(self.external_experiment_dir)
            self.results_dir = self.experiment_dir.parent
        else:
            # í˜¸í™˜ì„±ì„ ìœ„í•œ ê¸°ë³¸ ë°©ì‹ (ë‹¨ë… ì‹¤í–‰ ì‹œ)
            self.results_dir = Path("results") / self.session_timestamp
            self.experiment_dir = self.results_dir / f"{self.experiment_name}_{self.session_timestamp}"
            self.experiment_dir.mkdir(parents=True, exist_ok=True)
        
    def create_model(self):
        """Factory patternìœ¼ë¡œ ëª¨ë¸ ìƒì„±"""
        if self.model_type == "draem":
            return self._create_draem_model()
        elif self.model_type == "dinomaly":
            return self._create_dinomaly_model()
        elif self.model_type == "patchcore":
            return self._create_patchcore_model()
        elif self.model_type == "draem_sevnet":
            return self._create_draem_sevnet_model()
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…: {self.model_type}")
    
    def _create_draem_model(self):
        """DRAEM ëª¨ë¸ ìƒì„±"""
        # ëª…ì‹œì ìœ¼ë¡œ test_image_AUROC ë©”íŠ¸ë¦­ ì„¤ì •
        val_auroc = AUROC(fields=["pred_score", "gt_label"], prefix="val_image_")
        test_auroc = AUROC(fields=["pred_score", "gt_label"], prefix="test_image_")
        evaluator = Evaluator(val_metrics=[val_auroc], test_metrics=[test_auroc])
        
        return Draem(evaluator=evaluator)
    
    def _create_dinomaly_model(self):
        """Dinomaly ëª¨ë¸ ìƒì„±"""
        # DinomalyëŠ” ê¸°ë³¸ evaluatorë¥¼ ì‚¬ìš©í•˜ì—¬ trainable íŒŒë¼ë¯¸í„° ì„¤ì • ë¬¸ì œ íšŒí”¼
        return Dinomaly(
            encoder_name=self.config["encoder_name"],
            target_layers=self.config["target_layers"],
            bottleneck_dropout=self.config["bottleneck_dropout"],
            decoder_depth=self.config["decoder_depth"],
            remove_class_token=self.config["remove_class_token"],
            evaluator=True  # ê¸°ë³¸ evaluator ì‚¬ìš©
        )
    
    def _create_patchcore_model(self):
        """Patchcore ëª¨ë¸ ìƒì„±"""
        return Patchcore(
            backbone=self.config["backbone"],
            layers=self.config["layers"],
            pre_trained=self.config["pre_trained"],
            coreset_sampling_ratio=self.config["coreset_sampling_ratio"],
            num_neighbors=self.config["num_neighbors"]
        )
    
    def _create_draem_sevnet_model(self):
        """DRAEM-SevNet ëª¨ë¸ ìƒì„±"""
        # ëª…ì‹œì ìœ¼ë¡œ test_image_AUROC ë©”íŠ¸ë¦­ ì„¤ì •
        val_auroc = AUROC(fields=["pred_score", "gt_label"], prefix="val_image_")
        test_auroc = AUROC(fields=["pred_score", "gt_label"], prefix="test_image_")
        evaluator = Evaluator(val_metrics=[val_auroc], test_metrics=[test_auroc])
        
        return DraemSevNet(
            severity_head_mode=self.config["severity_head_mode"],
            severity_head_hidden_dim=self.config["severity_head_hidden_dim"],
            score_combination=self.config["score_combination"],
            severity_weight_for_combination=self.config["severity_weight_for_combination"],
            severity_head_pooling_type=self.config["severity_head_pooling_type"],
            severity_head_spatial_size=self.config["severity_head_spatial_size"],
            severity_head_use_spatial_attention=self.config["severity_head_use_spatial_attention"],
            patch_ratio_range=self.config["patch_ratio_range"],
            patch_width_range=self.config["patch_width_range"],
            patch_count=self.config["patch_count"],
            anomaly_probability=self.config["anomaly_probability"],
            severity_weight=self.config["severity_weight"],
            severity_loss_type=self.config["severity_loss_type"],
            severity_max=self.config["severity_max"],
            evaluator=evaluator
        )
    
    def create_datamodule(self):
        """ëª¨ë“  ëª¨ë¸ì— ê³µí†µìœ¼ë¡œ ì‚¬ìš©í•  ë°ì´í„° ëª¨ë“ˆ ìƒì„±"""
        # ë„ë©”ì¸ ì„¤ì • - ëª¨ë¸ë³„ë¡œ ë‹¤ë¥¸ í•„ë“œëª… ì²˜ë¦¬
        domain = self.config.get("source_domain") or self.config.get("domain")
        if not domain:
            raise ValueError("configì—ì„œ 'source_domain' ë˜ëŠ” 'domain' í•„ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
        return create_single_domain_datamodule(
            domain=domain,
            batch_size=self.config["batch_size"],
            image_size=self.config["image_size"],
            val_split_ratio=self.config["val_split_ratio"],
            num_workers=self.config["num_workers"],
            seed=self.config["seed"]
        )
    
    def create_callbacks(self):
        """ì½œë°± ì„¤ì • - ëª¨ë¸ë³„ ì ì ˆí•œ early stopping ë©”íŠ¸ë¦­ ì‚¬ìš©"""
        callbacks = []
        
        # ëª¨ë¸ë³„ EarlyStopping ì„¤ì •
        if self.model_type == "patchcore":
            # PatchCoreëŠ” ë‹¨ì¼ epoch í›ˆë ¨ì´ë¯€ë¡œ EarlyStoppingê³¼ ModelCheckpoint ëª¨ë‘ ë¶ˆí•„ìš”
            # Engineì—ì„œ ìë™ìœ¼ë¡œ ModelCheckpointë¥¼ ì¶”ê°€í•˜ë¯€ë¡œ ë³„ë„ ì¶”ê°€í•˜ì§€ ì•ŠìŒ
            print("   â„¹ï¸ PatchCore: EarlyStopping ë° ModelCheckpoint ë¹„í™œì„±í™” (ë‹¨ì¼ epoch í›ˆë ¨)")
            return []  # ë¹ˆ ì½œë°± ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
            
        else:
            # DRAEM, DRAEM-SevNet, Dinomaly: val_loss ê¸°ë°˜ EarlyStopping
            early_stopping = EarlyStopping(
                monitor="val_loss",
                patience=self.config["early_stopping_patience"],
                mode="min",
                verbose=True
            )
            callbacks.append(early_stopping)
            
            # Model Checkpoint
            domain = self.config.get("source_domain") or self.config.get("domain")
            checkpoint = ModelCheckpoint(
                filename=f"{self.model_type}_single_domain_{domain}_" + "{epoch:02d}_{val_loss:.4f}",
                monitor="val_loss", 
                mode="min",
                save_top_k=1,
                verbose=True
            )
            callbacks.append(checkpoint)
            
            print(f"   â„¹ï¸ {self.model_type.upper()}: EarlyStopping í™œì„±í™” (val_loss ëª¨ë‹ˆí„°ë§)")
        
        return callbacks
    
    def configure_optimizer(self, model):
        """ì˜µí‹°ë§ˆì´ì € ì„¤ì • - ëª¨ë“  ëª¨ë¸ ê³µí†µ"""
        # PatchCoreëŠ” ì˜µí‹°ë§ˆì´ì €ê°€ í•„ìš”í•˜ì§€ ì•ŠìŒ
        if self.model_type == "patchcore":
            return
            
        # DRAEMê³¼ DRAEM-SevNetì€ ì´ë¯¸ ìì²´ configure_optimizersë¥¼ ê°€ì§€ê³  ìˆìŒ
        # Dinomalyë§Œ ê¸°ë³¸ ì„¤ì •ì„ ì‚¬ìš©
        # ë”°ë¼ì„œ ì—¬ê¸°ì„œëŠ” ë³„ë„ ì²˜ë¦¬ ë¶ˆí•„ìš”
    
    def train_model(self, model, datamodule, logger) -> Tuple[Any, Engine, str]:
        """ëª¨ë¸ í›ˆë ¨ ìˆ˜í–‰"""
        print(f"\nğŸš€ {self.model_type.upper()} ëª¨ë¸ í›ˆë ¨ ì‹œì‘")
        logger.info(f"ğŸš€ {self.model_type.upper()} ëª¨ë¸ í›ˆë ¨ ì‹œì‘")
        
        # Config ì„¤ì • ì¶œë ¥
        print(f"   ğŸ”§ Config ì„¤ì •:")
        print(f"      Model Type: {self.model_type}")
        if self.model_type != "patchcore":
            print(f"      Max Epochs: {self.config['max_epochs']}")
            print(f"      Learning Rate: {self.config['learning_rate']}")
            print(f"      Early Stopping Patience: {self.config['early_stopping_patience']}")
        print(f"      Batch Size: {self.config['batch_size']}")
        
        # ì˜µí‹°ë§ˆì´ì € ì„¤ì •
        self.configure_optimizer(model)
        
        # ì½œë°± ì„¤ì •
        callbacks = self.create_callbacks()
        
        # TensorBoard ë¡œê±° ì„¤ì •
        tb_logger = TensorBoardLogger(
            save_dir=str(self.experiment_dir),
            name="tensorboard_logs",
            version=""
        )
        
        # Engine ì„¤ì •
        # PatchCoreì˜ ê²½ìš° max_epochsë¥¼ 1ë¡œ ê°•ì œ ì„¤ì •
        max_epochs = 1 if self.model_type == "patchcore" else self.config["max_epochs"]
        
        engine_kwargs = {
            "accelerator": "gpu" if torch.cuda.is_available() else "cpu",
            "devices": [0] if torch.cuda.is_available() else 1,
            "logger": tb_logger,
            "callbacks": callbacks,
            "enable_checkpointing": True,
            "log_every_n_steps": 10,
            "enable_model_summary": True,
            "default_root_dir": str(self.experiment_dir),
            "max_epochs": max_epochs,
            "check_val_every_n_epoch": 1,
            "num_sanity_val_steps": 0
        }
        
        if self.model_type == "patchcore":
            print(f"   â„¹ï¸ PatchCore: max_epochs ê°•ì œ ì„¤ì • (1 epoch)")
        else:
            print(f"   â„¹ï¸ {self.model_type.upper()}: max_epochs = {max_epochs}")
        
        engine = Engine(**engine_kwargs)
        
        print(f"   ğŸ”§ Engine ì„¤ì • ì™„ë£Œ")
        print(f"   ğŸ“ ê²°ê³¼ ì €ì¥ ê²½ë¡œ: {self.experiment_dir}")
        logger.info(f"ğŸ”§ Engine ì„¤ì • ì™„ë£Œ")
        logger.info(f"ğŸ“ ê²°ê³¼ ì €ì¥ ê²½ë¡œ: {self.experiment_dir}")
        
        # ëª¨ë¸ í›ˆë ¨
        print(f"   ğŸ¯ ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")
        logger.info("ğŸ¯ ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")
        
        engine.fit(model=model, datamodule=datamodule)
        
        # ìµœê³  ì²´í¬í¬ì¸íŠ¸ ì°¾ê¸°
        best_checkpoint = ""
        for callback in callbacks:
            if isinstance(callback, ModelCheckpoint) and hasattr(callback, 'best_model_path'):
                best_checkpoint = callback.best_model_path
                break
        
        print(f"   ğŸ† Best Checkpoint: {best_checkpoint}")
        logger.info(f"ğŸ† Best Checkpoint: {best_checkpoint}")
        
        print(f"   âœ… ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!")
        logger.info("âœ… ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!")
        
        return model, engine, best_checkpoint
    
    def evaluate_model(self, model, engine, datamodule, logger) -> Dict[str, Any]:
        """ëª¨ë¸ ì„±ëŠ¥ í‰ê°€"""
        domain = self.config.get("source_domain") or self.config.get("domain")
        
        print(f"\nğŸ“Š {domain} ë„ë©”ì¸ ì„±ëŠ¥ í‰ê°€ ì‹œì‘")
        logger.info(f"ğŸ“Š {domain} ë„ë©”ì¸ ì„±ëŠ¥ í‰ê°€ ì‹œì‘")
        
        # ğŸ”§ FIX: Lightning Testìš© ìƒˆë¡œìš´ DataModule ìƒì„± (í›ˆë ¨ëœ DataModule ì¬ì‚¬ìš© ë°©ì§€)
        print(f"   ğŸ†• Lightning Test ì „ìš© DataModule ìƒì„± ì¤‘...")
        test_datamodule = self.create_datamodule()
        test_datamodule.prepare_data()
        test_datamodule.setup()
        
        # Lightning Testìš© DataModule ë°ì´í„° í™•ì¸
        test_train_size = len(test_datamodule.train_data) if test_datamodule.train_data else 0
        test_test_size = len(test_datamodule.test_data) if test_datamodule.test_data else 0
        test_val_size = len(test_datamodule.val_data) if test_datamodule.val_data else 0
        
        print(f"   ğŸ“Š Lightning Test DataModule: í›ˆë ¨={test_train_size}, í…ŒìŠ¤íŠ¸={test_test_size}, ê²€ì¦={test_val_size}")
        logger.info(f"Lightning Test DataModule - í›ˆë ¨: {test_train_size}, í…ŒìŠ¤íŠ¸: {test_test_size}, ê²€ì¦: {test_val_size}")
        
        # í…ŒìŠ¤íŠ¸ ìˆ˜í–‰ (ìƒˆë¡œìš´ DataModule ì‚¬ìš©)
        test_results = engine.test(model=model, datamodule=test_datamodule)
        
        # Lightning Confusion Matrix ê³„ì‚°
        print(f"   ğŸ§® Lightning Confusion Matrix ê³„ì‚° ì¤‘...")
        lightning_confusion_matrix = self._calculate_lightning_confusion_matrix(model, test_datamodule, logger)
        
        # ìƒì„¸ ë¶„ì„ ìˆ˜í–‰ (ëª¨ë“  ëª¨ë¸ íƒ€ì…)
        print(f"   ğŸ”¬ ìƒì„¸ ë¶„ì„ ì‹œì‘ - ì´ë¯¸ì§€ë³„ ì˜ˆì¸¡ ì ìˆ˜ ì¶”ì¶œ ({self.model_type})")
        logger.info(f"ğŸ”¬ ìƒì„¸ ë¶„ì„ ì‹œì‘ - ì´ë¯¸ì§€ë³„ ì˜ˆì¸¡ ì ìˆ˜ ì¶”ì¶œ ({self.model_type})")
        try:
            custom_metrics = self._generate_detailed_analysis(model, test_datamodule, logger)
            print(f"   âœ… ìƒì„¸ ë¶„ì„ ì™„ë£Œ")
            logger.info("âœ… ìƒì„¸ ë¶„ì„ ì™„ë£Œ")
        except Exception as e:
            print(f"   âš ï¸ ìƒì„¸ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            logger.error(f"ìƒì„¸ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            import traceback
            logger.error(f"Traceback: {traceback.format_exc()}")
            custom_metrics = None
        
        # ê²°ê³¼ ì •ë¦¬ - Custom Analysis ë©”íŠ¸ë¦­ ìš°ì„  ì‚¬ìš©
        if custom_metrics and custom_metrics is not None:
            # Custom Analysis ë©”íŠ¸ë¦­ì„ ë©”ì¸ ê²°ê³¼ë¡œ ì‚¬ìš©
            results = {
                "domain": domain,
                "image_AUROC": custom_metrics["custom_auroc"],
                "image_F1Score": custom_metrics["custom_f1_score"],
                "training_samples": len(test_datamodule.train_data),
                "test_samples": len(test_datamodule.test_data),
                "val_samples": len(test_datamodule.val_data) if test_datamodule.val_data else 0,
                "lightning_confusion_matrix": lightning_confusion_matrix,
                "custom_analysis_metrics": custom_metrics
            }
            
            print(f"   ğŸ¯ Using Custom Analysis Results:")
            print(f"      Custom AUROC: {custom_metrics['custom_auroc']:.4f}")
            print(f"      Custom F1: {custom_metrics['custom_f1_score']:.4f}")
            print(f"      Custom Accuracy: {custom_metrics['custom_accuracy']:.4f}")
            
        elif test_results and len(test_results) > 0:
            # Fallback to Lightning Test results if Custom Analysis fails
            test_metrics = test_results[0]
            image_auroc = test_metrics.get("test_image_AUROC", test_metrics.get("image_AUROC", 0.0))
            
            results = {
                "domain": domain,
                "image_AUROC": float(image_auroc),
                "image_F1Score": test_metrics.get("test_image_F1Score", test_metrics.get("image_F1Score", 0.0)),
                "training_samples": len(test_datamodule.train_data),
                "test_samples": len(test_datamodule.test_data),
                "val_samples": len(test_datamodule.val_data) if test_datamodule.val_data else 0,
                "lightning_confusion_matrix": lightning_confusion_matrix
            }
            
            print(f"   âš ï¸ Fallback to Lightning Test Results:")
            print(f"      Lightning AUROC: {results['image_AUROC']:.4f}")
            print(f"      Lightning F1: {results['image_F1Score']:.4f}")
        else:
            results = {"domain": domain, "error": "No test results available"}
            logger.error(f"âŒ {domain} í‰ê°€ ì‹¤íŒ¨")
        
        # Lightning vs Custom Analysis ë¹„êµ ì¶œë ¥
        if lightning_confusion_matrix and custom_metrics:
            lightning_auroc = lightning_confusion_matrix.get('auroc', 0.0)
            custom_auroc = custom_metrics.get('custom_auroc', 0.0)
            print(f"      Lightning CM AUROC: {lightning_auroc:.4f}")
            print(f"      Custom Analysis AUROC: {custom_auroc:.4f}")
            print(f"      ğŸ” Lightning vs Custom ì°¨ì´: {abs(lightning_auroc - custom_auroc):.4f}")
            
        logger.info(f"âœ… {domain} í‰ê°€ ì™„ë£Œ: Custom AUROC={results.get('image_AUROC', 0.0):.4f}")
        
        return results
    
    def _calculate_lightning_confusion_matrix(self, model, test_datamodule, logger):
        """Lightning ê²°ê³¼ì˜ confusion matrix ê³„ì‚°"""
        from sklearn.metrics import confusion_matrix, roc_auc_score
        import numpy as np
        
        print(f"   ğŸ”§ Lightning ì˜ˆì¸¡ ì ìˆ˜ ìˆ˜ì§‘ ì¤‘...")
        
        # ëª¨ë¸ì„ evaluation ëª¨ë“œë¡œ ì„¤ì •
        model.eval()
        
        # ëª¨ë¸ì„ ì ì ˆí•œ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™ (í•œ ë²ˆë§Œ ì‹¤í–‰)
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        model = model.to(device)
        print(f"   ğŸ–¥ï¸  Lightning CM: ëª¨ë¸ì„ {device} ë””ë°”ì´ìŠ¤ë¡œ ì´ë™")
        
        # ë°ì´í„° ìˆ˜ì§‘ì„ ìœ„í•œ ë¦¬ìŠ¤íŠ¸ë“¤
        all_predictions = []
        all_ground_truth = []
        all_scores = []
        
        test_dataloader = test_datamodule.test_dataloader()
        total_batches = len(test_dataloader)
        
        print(f"   ğŸ“Š Lightning CM: {total_batches}ê°œ ë°°ì¹˜ ì²˜ë¦¬ ì¤‘...")
        
        with torch.no_grad():
            for batch_idx, batch in enumerate(test_dataloader):
                if batch_idx % 10 == 0:  # ë§¤ 10ë²ˆì§¸ ë°°ì¹˜ë§ˆë‹¤ ì§„í–‰ë¥  ì¶œë ¥
                    print(f"   ğŸ“ Lightning CM: {batch_idx+1}/{total_batches} ë°°ì¹˜ ì²˜ë¦¬ ì¤‘...")
                
                # Ground truth ìˆ˜ì§‘
                if hasattr(batch, 'gt_label'):
                    gt_labels = batch.gt_label.cpu().numpy()
                    all_ground_truth.extend(gt_labels)
                
                # ëª¨ë¸ ì˜ˆì¸¡
                try:
                    # ì…ë ¥ ë°ì´í„°ë¥¼ ëª¨ë¸ê³¼ ê°™ì€ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
                    input_images = batch.image.to(device)
                    
                    # Lightning ëª¨ë¸ë¡œ ì§ì ‘ ì˜ˆì¸¡
                    outputs = model(input_images)
                    
                    # DraemSevNetì˜ ê²½ìš° final_score ì‚¬ìš©
                    if hasattr(outputs, 'final_score'):
                        scores = outputs.final_score.cpu().numpy()
                    elif hasattr(outputs, 'pred_score'):
                        scores = outputs.pred_score.cpu().numpy()
                    elif hasattr(outputs, 'anomaly_score'):
                        scores = outputs.anomaly_score.cpu().numpy()
                    else:
                        # InferenceBatchì˜ ê²½ìš°
                        scores = outputs.pred_score.cpu().numpy() if hasattr(outputs, 'pred_score') else np.zeros(len(gt_labels))
                    
                    all_scores.extend(scores)
                    
                except Exception as e:
                    print(f"   âš ï¸ ë°°ì¹˜ {batch_idx} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    logger.warning(f"Lightning CM ë°°ì¹˜ {batch_idx} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    # ì˜¤ë¥˜ ì‹œ ë”ë¯¸ ì ìˆ˜ ì¶”ê°€
                    dummy_scores = np.zeros(len(gt_labels))
                    all_scores.extend(dummy_scores)
        
        if len(all_ground_truth) == 0 or len(all_scores) == 0:
            print(f"   âŒ Lightning CM: ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")
            return None
        
        # ê¸¸ì´ ë§ì¶”ê¸°
        min_len = min(len(all_ground_truth), len(all_scores))
        all_ground_truth = all_ground_truth[:min_len]
        all_scores = all_scores[:min_len]
        
        print(f"   âœ… Lightning CM: {len(all_ground_truth)}ê°œ ìƒ˜í”Œ ìˆ˜ì§‘ ì™„ë£Œ")
        
        # ì ìˆ˜ ë¶„í¬ ë¶„ì„ ì¶”ê°€
        scores_array = np.array(all_scores)
        print(f"   ğŸ” Lightning ì ìˆ˜ ë¶„í¬:")
        print(f"      ìµœì†Œê°’: {scores_array.min():.4f}")
        print(f"      ìµœëŒ€ê°’: {scores_array.max():.4f}")
        print(f"      í‰ê· ê°’: {scores_array.mean():.4f}")
        print(f"      ì¤‘ê°„ê°’: {np.median(scores_array):.4f}")
        print(f"      í‘œì¤€í¸ì°¨: {scores_array.std():.4f}")
        
        # ë¼ë²¨ë³„ ì ìˆ˜ ë¶„í¬
        gt_array = np.array(all_ground_truth)
        normal_scores = scores_array[gt_array == 0]
        anomaly_scores = scores_array[gt_array == 1]
        
        print(f"   ğŸ“Š ë¼ë²¨ë³„ ì ìˆ˜ ë¶„í¬:")
        print(f"      Normal í‰ê· : {normal_scores.mean():.4f} (min: {normal_scores.min():.4f}, max: {normal_scores.max():.4f})")
        print(f"      Anomaly í‰ê· : {anomaly_scores.mean():.4f} (min: {anomaly_scores.min():.4f}, max: {anomaly_scores.max():.4f})")
        
        # AUROC ê³„ì‚°
        try:
            lightning_auroc = roc_auc_score(all_ground_truth, all_scores)
            print(f"   ğŸ“Š Lightning ì§ì ‘ ê³„ì‚° AUROC: {lightning_auroc:.4f}")
        except Exception as e:
            lightning_auroc = 0.0
            print(f"   âš ï¸ Lightning AUROC ê³„ì‚° ì˜¤ë¥˜: {e}")
        
        # Optimal threshold ì°¾ê¸° (Youden's J statistic)
        from sklearn.metrics import roc_curve
        try:
            fpr, tpr, thresholds = roc_curve(all_ground_truth, all_scores)
            optimal_idx = np.argmax(tpr - fpr)
            optimal_threshold = thresholds[optimal_idx]
            print(f"   ğŸ¯ Lightning ìµœì  ì„ê³„ê°’: {optimal_threshold:.4f}")
        except:
            optimal_threshold = np.median(all_scores)
            print(f"   ğŸ¯ Lightning ê¸°ë³¸ ì„ê³„ê°’ (median): {optimal_threshold:.4f}")
        
        # ì˜ˆì¸¡ ë¼ë²¨ ìƒì„±
        predictions = (np.array(all_scores) > optimal_threshold).astype(int)
        
        # Confusion Matrix ê³„ì‚°
        cm = confusion_matrix(all_ground_truth, predictions)
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"   ğŸ§® Lightning Confusion Matrix:")
        print(f"       ì‹¤ì œ\\ì˜ˆì¸¡    Normal  Anomaly")
        print(f"       Normal     {cm[0,0]:6d}  {cm[0,1]:6d}")
        print(f"       Anomaly    {cm[1,0]:6d}  {cm[1,1]:6d}")
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        tn, fp, fn, tp = cm.ravel()
        accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        
        print(f"   ğŸ“ˆ Lightning ë©”íŠ¸ë¦­:")
        print(f"      AUROC: {lightning_auroc:.4f}")
        print(f"      Accuracy: {accuracy:.4f}")
        print(f"      Precision: {precision:.4f}")
        print(f"      Recall: {recall:.4f}")
        print(f"      F1-Score: {f1:.4f}")
        
        lightning_cm_result = {
            "confusion_matrix": cm.tolist(),
            "auroc": float(lightning_auroc),
            "optimal_threshold": float(optimal_threshold),
            "accuracy": float(accuracy),
            "precision": float(precision),
            "recall": float(recall),
            "f1_score": float(f1),
            "total_samples": len(all_ground_truth),
            "positive_samples": int(np.sum(all_ground_truth)),
            "negative_samples": int(len(all_ground_truth) - np.sum(all_ground_truth))
        }
        
        logger.info(f"Lightning CM - AUROC: {lightning_auroc:.4f}, CM: {cm.tolist()}")
        
        return lightning_cm_result
    
    def save_results(self, results, training_info, best_checkpoint, logger):
        """ì‹¤í—˜ ê²°ê³¼ ì €ì¥"""
        domain = self.config.get("source_domain") or self.config.get("domain")
        
        experiment_results = {
            "experiment_name": self.experiment_name,
            "description": f"{domain} - {self.model_type.upper()} single domain training",
            "domain": domain,
            "config": self.config,
            "results": results,
            "training_info": training_info,
            "timestamp": datetime.now().isoformat(),
            "checkpoint_path": best_checkpoint,
            "status": "success",
            "condition": {
                "name": self.experiment_name,
                "description": f"{domain} - {self.model_type.upper()} single domain training",
                "config": {
                    "source_domain": domain,
                    **self.config
                }
            },
            "source_results": {
                "test_image_AUROC": results.get("image_AUROC", 0.0),
                "test_image_F1Score": results.get("image_F1Score", 0.0),
                "domain": domain
            },
            "target_results": {}  # Single domainì´ë¯€ë¡œ ë¹„ì›Œë‘ 
        }
        
        # ê²°ê³¼ ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        # ì‹¤í—˜ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ì— ì§ì ‘ ì €ì¥ (multi-domainê³¼ ë™ì¼)
        result_filename = f"result_{timestamp}.json"
        
        results_file = save_experiment_results(
            result=experiment_results,
            result_filename=result_filename,
            log_dir=Path(self.experiment_dir),  # ì‹¤í—˜ ë£¨íŠ¸ ë””ë ‰í† ë¦¬
            logger=logger,
            model_type=self.model_type.upper()
        )
        print(f"ğŸ“„ ì‹¤í—˜ ê²°ê³¼ ì €ì¥ë¨: {results_file}")
        
        return experiment_results
    
    def _generate_detailed_analysis(self, model, datamodule, logger):
        """ì´ë¯¸ì§€ë³„ ìƒì„¸ ì˜ˆì¸¡ ë¶„ì„ ìˆ˜í–‰ ë° Custom Analysis ë©”íŠ¸ë¦­ ë°˜í™˜"""
        print(f"   ğŸ“Š ìƒˆë¡œìš´ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œë” ìƒì„± ì¤‘...")
        
        # ëª¨ë¸ì„ evaluation ëª¨ë“œë¡œ ì„¤ì •
        model.eval()
        
        # PyTorch ëª¨ë¸ì— ì§ì ‘ ì ‘ê·¼
        if not hasattr(model, 'model'):
            raise AttributeError("ëª¨ë¸ì— 'model' ì†ì„±ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        torch_model = model.model
        torch_model.eval()
        
        # ëª¨ë¸ì„ GPUë¡œ ì´ë™ (CUDA ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        torch_model = torch_model.to(device)
        print(f"   ğŸ–¥ï¸ ëª¨ë¸ì„ {device}ë¡œ ì´ë™ ì™„ë£Œ")
        
        # ë°ì´í„° ìˆ˜ì§‘ì„ ìœ„í•œ ë¦¬ìŠ¤íŠ¸ë“¤
        all_image_paths = []
        all_ground_truth = []
        all_scores = []
        all_mask_scores = []
        all_severity_scores = []
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œë” ìƒì„± (ì´ë¯¸ evaluate_modelì—ì„œ ìƒˆë¡œìš´ DataModule ìƒì„±ë¨)
        test_dataloader = datamodule.test_dataloader()
        print(f"   âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œë” ìƒì„± ì™„ë£Œ")
        total_batches = len(test_dataloader)
        
        print(f"   ğŸ”„ {total_batches}ê°œ ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘...")
        
        # ë°°ì¹˜ë³„ë¡œ ì˜ˆì¸¡ ìˆ˜í–‰
        with torch.no_grad():
            for batch_idx, batch in enumerate(test_dataloader):
                print(f"   ğŸ“ ì²˜ë¦¬ ì¤‘: {batch_idx+1}/{total_batches} ë°°ì¹˜ (ì§„í–‰ë¥ : {100*(batch_idx+1)/total_batches:.1f}%)")
                
                try:
                    # ì´ë¯¸ì§€ ê²½ë¡œ ì¶”ì¶œ
                    if hasattr(batch, 'image_path'):
                        image_paths = batch.image_path
                        if not isinstance(image_paths, list):
                            image_paths = [image_paths]
                    else:
                        # ê²½ë¡œê°€ ì—†ëŠ” ê²½ìš° ë°°ì¹˜ í¬ê¸°ë§Œí¼ ë”ë¯¸ ê²½ë¡œ ìƒì„±
                        batch_size = batch.image.shape[0]
                        image_paths = [f"batch_{batch_idx}_sample_{i}.jpg" for i in range(batch_size)]
                    
                    # ì´ë¯¸ì§€ í…ì„œ ì¶”ì¶œ
                    image_tensor = batch.image
                    print(f"      ğŸ–¼ï¸  ì´ë¯¸ì§€ í…ì„œ í¬ê¸°: {image_tensor.shape}, ê²½ë¡œ ìˆ˜: {len(image_paths)}")
                    
                    # ì´ë¯¸ì§€ í…ì„œë¥¼ ëª¨ë¸ê³¼ ê°™ì€ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
                    image_tensor = image_tensor.to(device)
                    
                    # ëª¨ë¸ë¡œ ì§ì ‘ ì˜ˆì¸¡ ìˆ˜í–‰
                    model_output = torch_model(image_tensor)
                    print(f"      âœ… ëª¨ë¸ ì¶œë ¥ ì™„ë£Œ: {type(model_output)}")
                    
                    # ëª¨ë¸ë³„ ì¶œë ¥ì—ì„œ ì ìˆ˜ë“¤ ì¶”ì¶œ
                    final_scores, mask_scores, severity_scores = self._extract_scores_from_model_output(
                        model_output, image_tensor.shape[0], batch_idx
                    )
                        
                except Exception as e:
                    print(f"   âŒ ë°°ì¹˜ {batch_idx} ì „ì²´ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
                    import traceback
                    print(f"      ğŸ” ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
                    
                    # ê¸°ë³¸ê°’ìœ¼ë¡œ ê±´ë„ˆë›°ê¸°
                    batch_size = image_tensor.shape[0] if 'image_tensor' in locals() else 16
                    final_scores = [0.5] * batch_size
                    mask_scores = [0.0] * batch_size
                    severity_scores = [0.0] * batch_size
                    image_paths = [f"batch_{batch_idx}_sample_{i}.jpg" for i in range(batch_size)]
                
                # Ground truth ì¶”ì¶œ (ì´ë¯¸ì§€ ê²½ë¡œì—ì„œ)
                gt_labels = []
                for path in image_paths:
                    if isinstance(path, str):
                        if '/fault/' in path:
                            gt_labels.append(1)  # anomaly
                        elif '/good/' in path:
                            gt_labels.append(0)  # normal
                        else:
                            gt_labels.append(0)  # ê¸°ë³¸ê°’
                    else:
                        gt_labels.append(0)
                
                # ê²°ê³¼ ìˆ˜ì§‘
                all_image_paths.extend(image_paths)
                all_ground_truth.extend(gt_labels)
                all_scores.extend(final_scores.flatten() if hasattr(final_scores, 'flatten') else final_scores)
                all_mask_scores.extend(mask_scores.flatten() if hasattr(mask_scores, 'flatten') else mask_scores)
                all_severity_scores.extend(severity_scores.flatten() if hasattr(severity_scores, 'flatten') else severity_scores)
                
                print(f"      âœ… ë°°ì¹˜ {batch_idx+1} ì™„ë£Œ: {len(gt_labels)}ê°œ ìƒ˜í”Œ ì¶”ê°€")
        
        print(f"   âœ… ì´ {len(all_image_paths)}ê°œ ìƒ˜í”Œ ì²˜ë¦¬ ì™„ë£Œ")
        
        # ì˜ˆì¸¡ ë ˆì´ë¸” ìƒì„± (threshold 0.5)
        all_predictions = [1 if score > 0.5 else 0 for score in all_scores]
        
        # analysis í´ë” ìƒì„±
        analysis_dir = self.experiment_dir / "analysis"
        analysis_dir.mkdir(exist_ok=True)
        
        print(f"   ğŸ’¾ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì¤‘: {analysis_dir}")
        
        # ìƒì„¸ í…ŒìŠ¤íŠ¸ ê²°ê³¼ CSV ì €ì¥
        predictions_dict = {
            "pred_scores": all_scores,
            "mask_scores": all_mask_scores,
            "severity_scores": all_severity_scores
        }
        ground_truth_dict = {
            "labels": all_ground_truth
        }
        save_detailed_test_results(
            predictions_dict, ground_truth_dict, all_image_paths, 
            analysis_dir, self.model_type
        )
        
        # AUROC ê³„ì‚° ë° ROC curve ìƒì„±
        from sklearn.metrics import roc_auc_score, roc_curve
        try:
            auroc = roc_auc_score(all_ground_truth, all_scores)
            plot_roc_curve(all_ground_truth, all_scores, analysis_dir, self.experiment_name)
            
            # ì„ê³„ê°’ ê³„ì‚°
            fpr, tpr, thresholds = roc_curve(all_ground_truth, all_scores)
            optimal_idx = (tpr - fpr).argmax()
            optimal_threshold = thresholds[optimal_idx]
            
            # ë©”íŠ¸ë¦­ ë³´ê³ ì„œ ì €ì¥
            save_metrics_report(all_ground_truth, all_predictions, all_scores, analysis_dir, auroc, optimal_threshold)
            
            # ì ìˆ˜ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨ ìƒì„±
            normal_scores = [score for gt, score in zip(all_ground_truth, all_scores) if gt == 0]
            anomaly_scores = [score for gt, score in zip(all_ground_truth, all_scores) if gt == 1]
            plot_score_distributions(normal_scores, anomaly_scores, analysis_dir, self.experiment_name)
            
            # ê·¹ë‹¨ì  ì‹ ë¢°ë„ ìƒ˜í”Œ ì €ì¥
            save_extreme_samples(all_image_paths, all_ground_truth, all_scores, all_predictions, analysis_dir)
            
            # ì‹¤í—˜ ìš”ì•½ ì €ì¥
            save_experiment_summary(self.config, {"auroc": auroc}, analysis_dir)
            
            print(f"   ğŸ“ˆ AUROC: {auroc:.4f}, ìµœì  ì„ê³„ê°’: {optimal_threshold:.4f}")
            logger.info(f"ìƒì„¸ ë¶„ì„ ì™„ë£Œ: AUROC={auroc:.4f}, ìƒ˜í”Œìˆ˜={len(all_image_paths)}")
            
            # Custom Analysis ë©”íŠ¸ë¦­ ë°˜í™˜
            from sklearn.metrics import confusion_matrix
            predictions = (np.array(all_scores) > optimal_threshold).astype(int)
            cm = confusion_matrix(all_ground_truth, predictions)
            tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, len(all_ground_truth), 0)
            
            accuracy = (tp + tn) / len(all_ground_truth) if len(all_ground_truth) > 0 else 0
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0
            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
            
            custom_metrics = {
                "custom_auroc": float(auroc),
                "custom_f1_score": float(f1),
                "custom_accuracy": float(accuracy), 
                "custom_precision": float(precision),
                "custom_recall": float(recall),
                "custom_confusion_matrix": cm.tolist(),
                "custom_optimal_threshold": float(optimal_threshold),
                "custom_total_samples": len(all_ground_truth),
                "custom_positive_samples": int(np.sum(all_ground_truth)),
                "custom_negative_samples": int(len(all_ground_truth) - np.sum(all_ground_truth))
            }
            
            return custom_metrics
            
        except Exception as e:
            print(f"   âš ï¸ ë©”íŠ¸ë¦­ ê³„ì‚° ì‹¤íŒ¨: {str(e)}")
            logger.error(f"ë©”íŠ¸ë¦­ ê³„ì‚° ì‹¤íŒ¨: {str(e)}")
            return None
    
    def run_experiment(self) -> dict:
        """ì „ì²´ ì‹¤í—˜ ì‹¤í–‰"""
        domain = self.config.get("source_domain") or self.config.get("domain")
        
        print(f"\n{'='*80}")
        print(f"ğŸ”¬ {self.model_type.upper()} Single Domain ì‹¤í—˜: {self.experiment_name}")
        print(f"ğŸ¯ ë„ë©”ì¸: {domain}")
        print(f"{'='*80}")
        
        try:
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            cleanup_gpu_memory()
            
            # ë¡œê¹… ì„¤ì •
            log_file_path = self.experiment_dir / f"{domain}_single.log"
            logger = setup_experiment_logging(str(log_file_path), self.experiment_name)
            logger.info(f"ğŸš€ {self.model_type.upper()} Single Domain ì‹¤í—˜ ì‹œì‘")
            
            # ëª¨ë¸ ìƒì„±
            model = self.create_model()
            
            # DataModule ìƒì„±
            datamodule = self.create_datamodule()
            
            # DataModule ë°ì´í„° ê°œìˆ˜ í™•ì¸
            print(f"\nğŸ“Š DataModule ë°ì´í„° ê°œìˆ˜ í™•ì¸:")
            print(f"   ğŸ”§ DataModule ì¤€ë¹„ ë° ì„¤ì • ì¤‘...")
            datamodule.prepare_data()
            datamodule.setup()
            
            train_size = len(datamodule.train_data) if datamodule.train_data else 0
            test_size = len(datamodule.test_data) if datamodule.test_data else 0
            val_size = len(datamodule.val_data) if datamodule.val_data else 0
            
            print(f"   ğŸ“ˆ í›ˆë ¨ ë°ì´í„°: {train_size:,}ê°œ")
            print(f"   ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„°: {test_size:,}ê°œ")
            print(f"   ğŸ“‹ ê²€ì¦ ë°ì´í„°: {val_size:,}ê°œ")
            print(f"   ğŸ¯ ì´ ë°ì´í„°: {train_size + test_size + val_size:,}ê°œ")
            
            # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¼ë²¨ ë¶„í¬ í™•ì¸ (ì²˜ìŒ ëª‡ ë°°ì¹˜ë§Œ ìƒ˜í”Œë§)
            test_loader = datamodule.test_dataloader()
            fault_count = 0
            good_count = 0
            sampled_images = 0
            max_sample = min(5 * datamodule.eval_batch_size, test_size)  # ì²˜ìŒ 5ë°°ì¹˜ ë˜ëŠ” ì „ì²´
            
            print(f"   ğŸ” í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¼ë²¨ ë¶„í¬ í™•ì¸ ì¤‘ (ìƒ˜í”Œ: {max_sample}ê°œ)...")
            with torch.no_grad():
                for batch_idx, batch in enumerate(test_loader):
                    if hasattr(batch, 'gt_label'):
                        labels = batch.gt_label.numpy()
                        fault_count += (labels == 1).sum()
                        good_count += (labels == 0).sum()
                        sampled_images += len(labels)
                    
                    if batch_idx >= 4 or sampled_images >= max_sample:  # ì²˜ìŒ 5ë°°ì¹˜ë§Œ í™•ì¸
                        break
            
            print(f"   ğŸš¨ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ë¶„í¬ ({sampled_images}ê°œ): Fault={fault_count}, Good={good_count}")
            if sampled_images > 0:
                fault_ratio = fault_count / sampled_images * 100
                good_ratio = good_count / sampled_images * 100
                print(f"   ğŸ“Š í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ë¹„ìœ¨: Fault={fault_ratio:.1f}%, Good={good_ratio:.1f}%")
                
                # ë¶ˆê· í˜• ê²½ê³ 
                if fault_count == 0:
                    print(f"   âš ï¸  ê²½ê³ : Fault ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤! AUROC ê³„ì‚°ì— ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                elif good_count == 0:
                    print(f"   âš ï¸  ê²½ê³ : Good ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤! AUROC ê³„ì‚°ì— ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                elif abs(fault_count - good_count) > sampled_images * 0.3:
                    print(f"   âš ï¸  ê²½ê³ : ë¼ë²¨ ë¶„í¬ê°€ ë¶ˆê· í˜•í•©ë‹ˆë‹¤ (30% ì´ìƒ ì°¨ì´)")
                else:
                    print(f"   âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¼ë²¨ ë¶„í¬ ì •ìƒ")
            
            logger.info(f"DataModule - í›ˆë ¨: {train_size}, í…ŒìŠ¤íŠ¸: {test_size}, ê²€ì¦: {val_size}")
            logger.info(f"í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ë¶„í¬ - Fault: {fault_count}, Good: {good_count}")
            
            # ëª¨ë¸ í›ˆë ¨
            trained_model, engine, best_checkpoint = self.train_model(model, datamodule, logger)
            
            # ì„±ëŠ¥ í‰ê°€
            results = self.evaluate_model(trained_model, engine, datamodule, logger)
            
            # í›ˆë ¨ ì •ë³´ ì¶”ì¶œ
            training_info = extract_training_info(engine)
            
            # ê²°ê³¼ ì €ì¥
            experiment_results = self.save_results(results, training_info, best_checkpoint, logger)
            
            # ì‹œê°í™” ìƒì„±
            try:
                create_experiment_visualization(
                    experiment_name=self.experiment_name,
                    model_type=f"{self.model_type.upper()}_single_domain_{domain}",
                    results_base_dir=str(self.experiment_dir),
                    source_domain=domain,
                    source_results=experiment_results.get('results', {}),
                    single_domain=True
                )
                print(f"ğŸ“Š ê²°ê³¼ ì‹œê°í™” ìƒì„± ì™„ë£Œ")
            except Exception as viz_error:
                print(f"âš ï¸ ì‹œê°í™” ìƒì„± ì¤‘ ì˜¤ë¥˜: {viz_error}")
                logger.warning(f"ì‹œê°í™” ìƒì„± ì¤‘ ì˜¤ë¥˜: {viz_error}")
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            cleanup_gpu_memory()
            
            print(f"âœ… ì‹¤í—˜ ì™„ë£Œ: {self.experiment_name}")
            logger.info(f"âœ… ì‹¤í—˜ ì™„ë£Œ: {self.experiment_name}")
            
            return experiment_results
            
        except Exception as e:
            error_msg = f"âŒ ì‹¤í—˜ ì‹¤íŒ¨: {self.experiment_name} - {str(e)}"
            print(error_msg)
            if 'logger' in locals():
                logger.error(error_msg)
            
            cleanup_gpu_memory()
            
            return {
                "experiment_name": self.experiment_name,
                "error": str(e),
                "timestamp": datetime.now().isoformat(),
                "status": "failed"
            }
    
    def _extract_scores_from_model_output(self, model_output, batch_size, batch_idx):
        """
        ëª¨ë¸ë³„ ì¶œë ¥ì—ì„œ ì ìˆ˜ë“¤ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
        
        Args:
            model_output: ëª¨ë¸ ì¶œë ¥ ê°ì²´
            batch_size: ë°°ì¹˜ í¬ê¸°
            batch_idx: ë°°ì¹˜ ì¸ë±ìŠ¤
            
        Returns:
            tuple: (anomaly_scores, mask_scores, severity_scores)
        """
        model_type = self.model_type.lower()
        
        try:
            if model_type == "draem_sevnet":
                # DRAEM-SevNet: final_score, mask_score, severity_score ìˆìŒ
                if hasattr(model_output, 'final_score'):
                    final_scores = model_output.final_score.cpu().numpy()
                    mask_scores = model_output.mask_score.cpu().numpy()
                    severity_scores = model_output.severity_score.cpu().numpy()
                    print(f"      ğŸ“Š DRAEM-SevNet ì ìˆ˜ ì¶”ì¶œ: final={final_scores[0]:.4f}, mask={mask_scores[0]:.4f}, severity={severity_scores[0]:.4f}")
                else:
                    raise AttributeError("DraemSevNetOutput ì†ì„± ì—†ìŒ")
                    
            elif model_type == "draem":
                # DRAEM: pred_scoreë§Œ ìˆìŒ
                if hasattr(model_output, 'pred_score'):
                    final_scores = model_output.pred_score.cpu().numpy()
                    mask_scores = [0.0] * batch_size  # DRAEMì—ëŠ” mask_score ì—†ìŒ
                    severity_scores = [0.0] * batch_size  # DRAEMì—ëŠ” severity_score ì—†ìŒ
                    print(f"      ğŸ“Š DRAEM ì ìˆ˜ ì¶”ì¶œ: pred_score={final_scores[0]:.4f}")
                elif hasattr(model_output, 'anomaly_map'):
                    # anomaly_mapì—ì„œ ì ìˆ˜ ê³„ì‚°
                    anomaly_map = model_output.anomaly_map.cpu().numpy()
                    final_scores = [float(np.max(am)) for am in anomaly_map]
                    mask_scores = [0.0] * batch_size
                    severity_scores = [0.0] * batch_size
                    print(f"      ğŸ“Š DRAEM ì ìˆ˜ ì¶”ì¶œ (anomaly_map): max={final_scores[0]:.4f}")
                else:
                    raise AttributeError("DRAEM ì¶œë ¥ ì†ì„± ì—†ìŒ")
                    
            elif model_type == "patchcore":
                # PatchCore: pred_scoreë§Œ ìˆìŒ
                if hasattr(model_output, 'pred_score'):
                    final_scores = model_output.pred_score.cpu().numpy()
                    mask_scores = [0.0] * batch_size
                    severity_scores = [0.0] * batch_size
                    print(f"      ğŸ“Š PatchCore ì ìˆ˜ ì¶”ì¶œ: pred_score={final_scores[0]:.4f}")
                elif hasattr(model_output, 'anomaly_map'):
                    # anomaly_mapì—ì„œ ì ìˆ˜ ê³„ì‚°
                    anomaly_map = model_output.anomaly_map.cpu().numpy()
                    final_scores = [float(np.max(am)) for am in anomaly_map]
                    mask_scores = [0.0] * batch_size
                    severity_scores = [0.0] * batch_size
                    print(f"      ğŸ“Š PatchCore ì ìˆ˜ ì¶”ì¶œ (anomaly_map): max={final_scores[0]:.4f}")
                else:
                    raise AttributeError("PatchCore ì¶œë ¥ ì†ì„± ì—†ìŒ")
                    
            elif model_type == "dinomaly":
                # Dinomaly: pred_score ë˜ëŠ” anomaly_map
                if hasattr(model_output, 'pred_score'):
                    final_scores = model_output.pred_score.cpu().numpy()
                    mask_scores = [0.0] * batch_size
                    severity_scores = [0.0] * batch_size
                    print(f"      ğŸ“Š Dinomaly ì ìˆ˜ ì¶”ì¶œ: pred_score={final_scores[0]:.4f}")
                elif hasattr(model_output, 'anomaly_map'):
                    # anomaly_mapì—ì„œ ì ìˆ˜ ê³„ì‚°
                    anomaly_map = model_output.anomaly_map.cpu().numpy()
                    final_scores = [float(np.max(am)) for am in anomaly_map]
                    mask_scores = [0.0] * batch_size
                    severity_scores = [0.0] * batch_size
                    print(f"      ğŸ“Š Dinomaly ì ìˆ˜ ì¶”ì¶œ (anomaly_map): max={final_scores[0]:.4f}")
                else:
                    raise AttributeError("Dinomaly ì¶œë ¥ ì†ì„± ì—†ìŒ")
                    
            else:
                # ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸ íƒ€ì…: ê¸°ë³¸ ì²˜ë¦¬
                print(f"   âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸ íƒ€ì…: {model_type}, ì¼ë°˜ì ì¸ ì†ì„±ìœ¼ë¡œ ì‹œë„")
                if hasattr(model_output, 'pred_score'):
                    final_scores = model_output.pred_score.cpu().numpy()
                elif hasattr(model_output, 'final_score'):
                    final_scores = model_output.final_score.cpu().numpy()
                elif hasattr(model_output, 'anomaly_map'):
                    anomaly_map = model_output.anomaly_map.cpu().numpy()
                    final_scores = [float(np.max(am)) for am in anomaly_map]
                else:
                    raise AttributeError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ëª¨ë¸ ì¶œë ¥ í˜•ì‹: {type(model_output)}")
                    
                mask_scores = [0.0] * batch_size
                severity_scores = [0.0] * batch_size
                print(f"      ğŸ“Š ì¼ë°˜ ëª¨ë¸ ì ìˆ˜ ì¶”ì¶œ: anomaly_score={final_scores[0]:.4f}")
                
            return final_scores, mask_scores, severity_scores
            
        except Exception as e:
            # fallback: ë”ë¯¸ ì ìˆ˜ ì‚¬ìš©
            print(f"   âš ï¸ ë°°ì¹˜ {batch_idx}: {model_type} ì ìˆ˜ ì¶”ì¶œ ì‹¤íŒ¨ - {str(e)}, ë”ë¯¸ê°’ ì‚¬ìš©")
            final_scores = [0.5] * batch_size
            mask_scores = [0.0] * batch_size 
            severity_scores = [0.0] * batch_size
            return final_scores, mask_scores, severity_scores