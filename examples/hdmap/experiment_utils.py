#!/usr/bin/env python3
"""Anomaly Detection ì‹¤í—˜ì„ ìœ„í•œ ê³µí†µ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤.

ì´ ëª¨ë“ˆì€ DRAEM, PaDiM ë“± ë‹¤ì–‘í•œ Anomaly Detection ëª¨ë¸ ì‹¤í—˜ì—ì„œ 
ì¬ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ê³µí†µ í•¨ìˆ˜ë“¤ì„ ì œê³µí•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- GPU ë©”ëª¨ë¦¬ ê´€ë¦¬
- ì‹¤í—˜ ë¡œê¹… ì„¤ì •
- ê²°ê³¼ ì´ë¯¸ì§€ ì •ë¦¬ ë° ì‹œê°í™”
- Target domain í‰ê°€
- ì‹¤í—˜ ê²°ê³¼ ë¶„ì„ ë° ì €ì¥
"""

import os
import gc
import json
import shutil
import warnings
import logging
import torch
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List, Optional, Union, Tuple
from lightning.pytorch.callbacks import EarlyStopping

# Anomalib imports
from anomalib.engine import Engine


def load_experiment_conditions(json_filename: str) -> List[Dict[str, Any]]:
    """
    JSON íŒŒì¼ì—ì„œ ì‹¤í—˜ ì¡°ê±´ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    
    Args:
        json_filename: ë¡œë“œí•  JSON íŒŒì¼ëª… (í™•ì¥ì í¬í•¨)
        
    Returns:
        ì‹¤í—˜ ì¡°ê±´ ë¦¬ìŠ¤íŠ¸
        
    Raises:
        FileNotFoundError: JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš°
        json.JSONDecodeError: JSON íŒŒì‹± ì˜¤ë¥˜ê°€ ë°œìƒí•œ ê²½ìš°
    """
    # caller ìŠ¤í¬ë¦½íŠ¸ê°€ ìˆëŠ” ë””ë ‰í† ë¦¬ ê¸°ì¤€ìœ¼ë¡œ JSON íŒŒì¼ ê²½ë¡œ ìƒì„±
    import inspect
    caller_frame = inspect.stack()[1]
    caller_dir = os.path.dirname(os.path.abspath(caller_frame.filename))
    json_path = os.path.join(caller_dir, json_filename)
    
    if not os.path.exists(json_path):
        raise FileNotFoundError(f"ì‹¤í—˜ ì¡°ê±´ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {json_path}")
    
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except json.JSONDecodeError as e:
        raise json.JSONDecodeError(f"JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
    
    # JSONì—ì„œ ë¡œë“œí•œ ë°ì´í„°ì˜ ìœ íš¨ì„± ê²€ì‚¬
    if 'experiment_conditions' not in data:
        raise ValueError("JSON íŒŒì¼ì— 'experiment_conditions' í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    experiment_conditions = data['experiment_conditions']
    
    # JSONì—ì„œëŠ” tupleì´ listë¡œ ì €ì¥ë˜ë¯€ë¡œ, í•„ìš”í•œ í•„ë“œë“¤ì„ ë‹¤ì‹œ tupleë¡œ ë³€í™˜
    for condition in experiment_conditions:
        if 'config' not in condition:
            continue
            
        config = condition['config']
        
        # range íƒ€ì…ì˜ í•„ë“œë“¤ì„ tupleë¡œ ë³€í™˜
        range_fields = ['patch_width_range', 'patch_ratio_range']
        for field in range_fields:
            if field in config and isinstance(config[field], list):
                config[field] = tuple(config[field])
    
    return experiment_conditions

def create_experiment_visualization(
    experiment_name: str,
    model_type: str,
    results_base_dir: str,
    source_domain: str = None,
    target_domains: list = None,
    source_results: Dict[str, Any] = None,
    target_results: Dict[str, Dict[str, Any]] = None,
    single_domain: bool = False
) -> str:
    """ì‹¤í—˜ ê²°ê³¼ ì‹œê°í™” í´ë” êµ¬ì¡° ìƒì„± ë° ì‹¤í—˜ ì •ë³´ ì €ì¥.
    
    Args:
        experiment_name: ì‹¤í—˜ ì´ë¦„
        model_type: ëª¨ë¸ íƒ€ì… (ì˜ˆ: "DRAEM-SevNet", "PaDiM")
        results_base_dir: ê¸°ë³¸ ê²°ê³¼ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        source_domain: ì†ŒìŠ¤ ë„ë©”ì¸ ì´ë¦„ (single_domain=Trueì¼ ë•ŒëŠ” None ê°€ëŠ¥)
        target_domains: íƒ€ê²Ÿ ë„ë©”ì¸ ë¦¬ìŠ¤íŠ¸
        source_results: ì†ŒìŠ¤ ë„ë©”ì¸ í‰ê°€ ê²°ê³¼
        target_results: íƒ€ê²Ÿ ë„ë©”ì¸ë“¤ í‰ê°€ ê²°ê³¼
        single_domain: ë‹¨ì¼ ë„ë©”ì¸ ì‹¤í—˜ ì—¬ë¶€
        
    Returns:
        str: ìƒì„±ëœ visualize ë””ë ‰í† ë¦¬ ê²½ë¡œ
    """
    print(f"\nğŸ¨ {model_type} Visualization ìƒì„±")
    
    # ê¸°ë³¸ ê²½ë¡œë¥¼ ì‚¬ìš© (ì´ë¯¸ ì‹¤í—˜ë³„ ê³ ìœ  ê²½ë¡œì„)
    base_path = Path(results_base_dir)
    
    # ë¨¼ì € v* íŒ¨í„´ì˜ ë²„ì „ í´ë”ê°€ ìˆëŠ”ì§€ í™•ì¸
    version_dirs = [d for d in base_path.glob("v*") if d.is_dir() and d.name.startswith('v') and d.name[1:].isdigit()]
    if version_dirs:
        # v0, v1 ë“±ì˜ íŒ¨í„´ì´ ìˆìœ¼ë©´ ìµœì‹  ë²„ì „ ì‚¬ìš©
        latest_version_path = max(version_dirs, key=lambda x: int(x.name[1:]))
    else:
        # ë²„ì „ í´ë”ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©
        latest_version_path = base_path
    
    # visualize í´ë” ìƒì„±
    viz_path = latest_version_path / "visualize"
    viz_path.mkdir(exist_ok=True)
    
    # í´ë” êµ¬ì¡° ìƒì„± (single_domain ì‹¤í—˜ì—ì„œëŠ” target_domains í´ë” ìƒì„±í•˜ì§€ ì•ŠìŒ)
    if single_domain:
        folders_to_create = ["results"]
    else:
        folders_to_create = [
            "source_domain",
            "target_domains"
        ]
    
    for folder in folders_to_create:
        (viz_path / folder).mkdir(exist_ok=True)
    
    # íƒ€ê²Ÿ ë„ë©”ì¸ë³„ í•˜ìœ„ í´ë” ìƒì„± (multi-domain ì‹¤í—˜ì—ë§Œ ì ìš©)
    if not single_domain and target_domains:
        for domain in target_domains:
            (viz_path / "target_domains" / domain).mkdir(exist_ok=True)
    
    # ì‹¤í—˜ ì •ë³´ë¥¼ JSONìœ¼ë¡œ ì €ì¥
    experiment_info = {
        "experiment_name": experiment_name,
        "model_type": model_type,
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "results_path": str(latest_version_path),
        "source_domain": source_domain,
        "target_domains": target_domains or [],
        "results_summary": {
            "source_results": source_results or {},
            "target_results": target_results or {}
        }
    }
    
    # JSON íŒŒì¼ë¡œ ì €ì¥
    info_file = viz_path / "experiment_info.json"
    with open(info_file, 'w', encoding='utf-8') as f:
        json.dump(experiment_info, f, indent=2, ensure_ascii=False)
    
    # ì‹¤ì œ ìƒì„±ëœ ì´ë¯¸ì§€ íŒŒì¼ë“¤ì„ visualize/resultsë¡œ ë³µì‚¬ (single domainì˜ ê²½ìš°)
    if single_domain:
        try:
            # anomalib ëª¨ë¸ ê²°ê³¼ ì´ë¯¸ì§€ê°€ ì €ì¥ë˜ëŠ” íŒ¨í„´ íƒìƒ‰
            image_patterns = [
                latest_version_path / "*" / "*" / source_domain / "latest" / "images",  # ì¼ë°˜ì ì¸ íŒ¨í„´
                latest_version_path / "*" / source_domain / "latest" / "images",        # ì¶•ì•½ëœ íŒ¨í„´
                latest_version_path / "images",                                          # ì§ì ‘ images í´ë”
            ]
            
            images_found = False
            for pattern_path in image_patterns:
                # glob íŒ¨í„´ìœ¼ë¡œ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ ì°¾ê¸°
                for images_dir in Path(str(pattern_path).replace('*', '')).parent.glob('**/images'):
                    if images_dir.exists() and any(images_dir.iterdir()):
                        print(f"ğŸ“ ì´ë¯¸ì§€ ë°œê²¬: {images_dir}")
                        
                        # ì´ë¯¸ì§€ íŒŒì¼ë“¤ì„ visualize/resultsë¡œ ë³µì‚¬
                        results_dir = viz_path / "results"
                        
                        # ì„œë¸Œ ë””ë ‰í† ë¦¬ë³„ë¡œ ë³µì‚¬ (good, fault ë“±)
                        for subdir in images_dir.iterdir():
                            if subdir.is_dir():
                                target_subdir = results_dir / subdir.name
                                target_subdir.mkdir(exist_ok=True)
                                
                                # ëª¨ë“  ì´ë¯¸ì§€ íŒŒì¼ ë³µì‚¬
                                image_files = list(subdir.glob('*.png'))
                                for img_file in image_files:
                                    shutil.copy2(img_file, target_subdir / img_file.name)
                                
                                print(f"   ğŸ“¸ {len(image_files)}ê°œ ì´ë¯¸ì§€ë¥¼ {target_subdir}ì— ë³µì‚¬")
                        
                        images_found = True
                        break
                
                if images_found:
                    break
            
            if not images_found:
                print(f"âš ï¸ ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {latest_version_path}")
        
        except Exception as copy_error:
            print(f"âš ï¸ ì´ë¯¸ì§€ ë³µì‚¬ ì¤‘ ì˜¤ë¥˜: {copy_error}")
    
    print(f"âœ… {model_type} í´ë” êµ¬ì¡° ìƒì„± ì™„ë£Œ: {viz_path}")
    
    return str(viz_path)


def cleanup_gpu_memory():
    """GPU ë©”ëª¨ë¦¬ ì •ë¦¬ (ëª¨ë“  ëª¨ë¸ì—ì„œ ê³µí†µ ì‚¬ìš© ê°€ëŠ¥)."""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        gc.collect()


def setup_warnings_filter():
    """ì‹¤í—˜ ì‹œ ë¶ˆí•„ìš”í•œ ê²½ê³  ë©”ì‹œì§€ í•„í„°ë§ (ëª¨ë“  ëª¨ë¸ì—ì„œ ê³µí†µ ì‚¬ìš© ê°€ëŠ¥)."""
    warnings.filterwarnings("ignore", category=FutureWarning)
    warnings.filterwarnings("ignore", category=DeprecationWarning)
    warnings.filterwarnings("ignore", category=UserWarning)
    
    # ì‹œê°í™” ê´€ë ¨ íŠ¹ì • ê²½ê³  í•„í„°ë§ (ë” í¬ê´„ì )
    warnings.filterwarnings("ignore", message=".*Field.*gt_mask.*is None.*")
    warnings.filterwarnings("ignore", message=".*Skipping visualization.*")
    warnings.filterwarnings("ignore", message=".*gt_mask.*None.*")


def setup_experiment_logging(log_file_path: str, experiment_name: str) -> logging.Logger:
    """ì‹¤í—˜ ë¡œê¹… ì„¤ì • (ëª¨ë“  ëª¨ë¸ì—ì„œ ê³µí†µ ì‚¬ìš© ê°€ëŠ¥).
    
    Args:
        log_file_path: ë¡œê·¸ íŒŒì¼ ê²½ë¡œ
        experiment_name: ì‹¤í—˜ ì´ë¦„
        
    Returns:
        logging.Logger: ì„¤ì •ëœ ë¡œê±°
    """
    # ë¡œê±° ì„¤ì •
    logger = logging.getLogger(experiment_name)
    logger.setLevel(logging.INFO)
    
    # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±°
    for handler in logger.handlers[:]:
        logger.removeHandler(handler)
    
    # íŒŒì¼ í•¸ë“¤ëŸ¬ ì¶”ê°€
    file_handler = logging.FileHandler(log_file_path, encoding='utf-8')
    file_handler.setLevel(logging.INFO)
    
    # í¬ë§·í„° ì„¤ì •
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    file_handler.setFormatter(formatter)
    
    logger.addHandler(file_handler)
    
    return logger


def extract_training_info(engine: Engine) -> Dict[str, Any]:
    """PyTorch Lightning Engineì—ì„œ í•™ìŠµ ì •ë³´ ì¶”ì¶œ (ëª¨ë“  ëª¨ë¸ì—ì„œ ê³µí†µ ì‚¬ìš© ê°€ëŠ¥).
    
    Args:
        engine: Anomalib Engine ê°ì²´
        
    Returns:
        Dict[str, Any]: í•™ìŠµ ì •ë³´ ë”•ì…”ë„ˆë¦¬
    """
    import torch
    
    trainer = engine.trainer
    
    # Early Stopping ì½œë°± ì°¾ê¸°
    early_stopping_callback = None
    for callback in trainer.callbacks:
        if isinstance(callback, EarlyStopping):
            early_stopping_callback = callback
            break
    
    # ì‹¤ì œ í•™ìŠµ ì™„ë£Œ ì—í­ ê³„ì‚°
    if early_stopping_callback and hasattr(early_stopping_callback, 'stopped_epoch') and early_stopping_callback.stopped_epoch > 0:
        # Early stoppingì´ ë°œìƒí•œ ê²½ìš°: stopped_epochì´ ë§ˆì§€ë§‰ í•™ìŠµ ì—í­
        last_trained_epoch = early_stopping_callback.stopped_epoch + 1  # 0-based â†’ 1-based
        early_stopped = True
    else:
        # ì •ìƒ ì™„ë£Œ ë˜ëŠ” early stopping ì—†ìŒ
        last_trained_epoch = trainer.current_epoch + 1  # 0-based â†’ 1-based
        early_stopped = False
    
    # ê¸°ë³¸ ì •ë³´
    training_info = {
        "max_epochs_configured": trainer.max_epochs,
        "last_trained_epoch": last_trained_epoch,  # ì‹¤ì œ ë§ˆì§€ë§‰ìœ¼ë¡œ í•™ìŠµí•œ ì—í­ (1-based)
        "total_steps": trainer.global_step,
        "early_stopped": early_stopped,
        "early_stop_reason": None,
        "best_val_auroc": None,  # ìµœê³  validation AUROC
        "f1_threshold_issue": "F1ScoreëŠ” ê¸°ë³¸ thresholdë¡œ ê³„ì‚°ë¨. AUROC ëŒ€ë¹„ ë‚®ì„ ìˆ˜ ìˆìŒ."
    }
    
    # Early stopping ì„¸ë¶€ ì •ë³´ ì¶”ê°€
    if early_stopping_callback:
        if training_info["early_stopped"]:
            training_info["early_stop_reason"] = f"No improvement for {early_stopping_callback.patience} epochs"
        
        # ìµœê³  ì„±ëŠ¥ ê¸°ë¡
        if hasattr(early_stopping_callback, 'best_score') and early_stopping_callback.best_score is not None:
            best_score = early_stopping_callback.best_score
            if hasattr(best_score, 'cpu'):
                training_info["best_val_auroc"] = float(best_score.cpu())
            else:
                training_info["best_val_auroc"] = float(best_score)
    
    # ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ì •ë³´ì—ì„œë„ ìµœê³  ì„±ëŠ¥ ì¶”ì¶œ
    checkpoint_callback = None
    for callback in trainer.callbacks:
        if hasattr(callback, 'best_model_score'):
            checkpoint_callback = callback
            break
    
    if checkpoint_callback and hasattr(checkpoint_callback, 'best_model_score') and checkpoint_callback.best_model_score is not None:
        best_score = checkpoint_callback.best_model_score
        if hasattr(best_score, 'cpu'):
            training_info["best_val_auroc"] = float(best_score.cpu())
        else:
            training_info["best_val_auroc"] = float(best_score)
    
    # í•™ìŠµ ì™„ë£Œ ë°©ì‹ ê²°ì •
    if training_info["early_stopped"]:
        completion_type = "early_stopping"
        completion_description = f"ì—í­ {training_info['last_trained_epoch']}ì—ì„œ early stoppingìœ¼ë¡œ ì¤‘ë‹¨"
    elif training_info["last_trained_epoch"] >= training_info["max_epochs_configured"]:
        completion_type = "max_epochs_reached"
        completion_description = f"ìµœëŒ€ ì—í­ {training_info['max_epochs_configured']} ì™„ë£Œ"
    else:
        completion_type = "interrupted"
        completion_description = f"ì—í­ {training_info['last_trained_epoch']}ì—ì„œ ì¤‘ë‹¨ë¨"
    
    training_info["completion_type"] = completion_type
    training_info["completion_description"] = completion_description
    
    return training_info

def save_experiment_results(
    result: Dict[str, Any], 
    result_filename: str, 
    log_dir: Path, 
    logger: logging.Logger,
    model_type: str = "Model"
) -> Path:
    """ì‹¤í—˜ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥ (ëª¨ë“  ëª¨ë¸ì—ì„œ ê³µí†µ ì‚¬ìš© ê°€ëŠ¥).
    
    Args:
        result: ì‹¤í—˜ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        result_filename: ì €ì¥í•  íŒŒì¼ëª…
        log_dir: ë¡œê·¸ ë””ë ‰í† ë¦¬ (ì‹¤íŒ¨ ì‹œ ì‚¬ìš©)
        logger: ë¡œê±° ê°ì²´
        model_type: ëª¨ë¸ íƒ€ì… (ë¡œê¹…ìš©)
        
    Returns:
        Path: ì‹¤ì œ ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
    """
    # ì‹¤í—˜ë³„ ê²½ë¡œì— ì €ì¥í•˜ê±°ë‚˜, ì‹¤íŒ¨ ì‹œ log_dirì— ì €ì¥
    if result.get("experiment_path") and Path(result["experiment_path"]).exists():
        result_path = Path(result["experiment_path"]) / result_filename
        print(f"   ğŸ“ ê²°ê³¼ íŒŒì¼ì„ ì‹¤í—˜ í´ë”ì— ì €ì¥: {result_path}")
    else:
        result_path = log_dir / result_filename
        print(f"   ğŸ“ ê²°ê³¼ íŒŒì¼ì„ ë¡œê·¸ í´ë”ì— ì €ì¥: {result_path}")
    
    # ê²°ê³¼ JSON ì§ë ¬í™”ë¥¼ ìœ„í•œ ì²˜ë¦¬
    serializable_result = json.loads(json.dumps(result, default=str))
    
    with open(result_path, 'w', encoding='utf-8') as f:
        json.dump(serializable_result, f, indent=2, ensure_ascii=False)
    
    # ê²°ê³¼ ìš”ì•½ ë¡œê¹…
    if result["status"] == "success":
        logger.info("âœ… ì‹¤í—˜ ì„±ê³µ!")
        
        # AUROC ì •ë³´ ë¡œê¹… (multi-domain ë˜ëŠ” all-domainsì— ë”°ë¼ ë‹¤ë¥´ê²Œ ì²˜ë¦¬)
        if 'source_results' in result:
            # Multi-domain ì‹¤í—˜ì˜ ê²½ìš°
            source_auroc = result['source_results'].get('image_AUROC', None)
            if isinstance(source_auroc, (int, float)):
                logger.info(f"   Source Domain AUROC: {source_auroc:.4f}")
            else:
                logger.info(f"   Source Domain AUROC: {source_auroc or 'N/A'}")
            
            # Target Domains Avg AUROC ì•ˆì „í•œ í¬ë§·íŒ…
            avg_target_auroc = result.get('avg_target_auroc', None)
            if isinstance(avg_target_auroc, (int, float)):
                logger.info(f"   Target Domains Avg AUROC: {avg_target_auroc:.4f}")
            else:
                logger.info(f"   Target Domains Avg AUROC: {avg_target_auroc or 'N/A'}")
                
        elif 'all_domains_results' in result:
            # All-domains ì‹¤í—˜ì˜ ê²½ìš°
            all_domains_auroc = result['all_domains_results'].get('test_image_AUROC', None)
            if isinstance(all_domains_auroc, (int, float)):
                logger.info(f"   All Domains AUROC: {all_domains_auroc:.4f}")
            else:
                logger.info(f"   All Domains AUROC: {all_domains_auroc or 'N/A'}")
        else:
            logger.info("   AUROC ì •ë³´ ì—†ìŒ")
        
        logger.info(f"   ì²´í¬í¬ì¸íŠ¸: {result.get('best_checkpoint', 'N/A')}")
        
        # í•™ìŠµ ê³¼ì • ì •ë³´ ë¡œê¹…
        training_info = result.get('training_info', {})
        if training_info:
            logger.info("ğŸ“Š í•™ìŠµ ê³¼ì • ì •ë³´:")
            logger.info(f"   ì„¤ì •ëœ ìµœëŒ€ ì—í¬í¬: {training_info.get('max_epochs_configured', 'N/A')}")
            logger.info(f"   ì‹¤ì œ í•™ìŠµ ì—í¬í¬: {training_info.get('last_trained_epoch', 'N/A')}")
            logger.info(f"   ì´ í•™ìŠµ ìŠ¤í…: {training_info.get('total_steps', 'N/A')}")
            logger.info(f"   Early Stopping ì ìš©: {training_info.get('early_stopped', 'N/A')}")
            if training_info.get('early_stopped'):
                logger.info(f"   Early Stopping ì‚¬ìœ : {training_info.get('early_stop_reason', 'N/A')}")
            # ìµœê³  Validation AUROC ì•ˆì „í•œ í¬ë§·íŒ…
            best_val_auroc = training_info.get('best_val_auroc', None)
            if isinstance(best_val_auroc, (int, float)):
                logger.info(f"   ìµœê³  Validation AUROC: {best_val_auroc:.4f}")
            else:
                logger.info(f"   ìµœê³  Validation AUROC: {best_val_auroc or 'N/A'}")
            logger.info(f"   í•™ìŠµ ì™„ë£Œ ë°©ì‹: {training_info.get('completion_description', 'N/A')}")
        
        # Target Domainë³„ ìƒì„¸ ì„±ëŠ¥ ë¡œê¹…
        target_results = result.get('target_results', {})
        if target_results:
            logger.info("ğŸ¯ Target Domainë³„ ì„±ëŠ¥:")
            for domain, domain_result in target_results.items():
                domain_auroc = domain_result.get('image_AUROC', 'N/A')
                if isinstance(domain_auroc, (int, float)):
                    logger.info(f"   {domain}: {domain_auroc:.4f}")
                else:
                    logger.info(f"   {domain}: {domain_auroc}")
    else:
        logger.info("âŒ ì‹¤í—˜ ì‹¤íŒ¨!")
        logger.info(f"   ì˜¤ë¥˜: {result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
    
    logger.info(f"ğŸ“ ê²°ê³¼ íŒŒì¼: {result_path}")
    
    # ì‹¤í—˜ë³„ ê²½ë¡œì— ì €ì¥ëœ ê²½ìš° ì¶”ê°€ ì •ë³´ ë¡œê¹…
    if result.get("experiment_path"):
        logger.info(f"ğŸ“‚ ì‹¤í—˜ í´ë”: {result['experiment_path']}")
    
    return result_path


def create_single_domain_datamodule(
    domain: str,
    dataset_root: str,
    batch_size: int = 16,
    target_size: tuple[int, int] | None = None,
    resize_method: str = "resize",
    val_split_ratio: float = 0.2,
    val_split_mode: str = "FROM_TEST",
    num_workers: int = 4,
    seed: int = 42,
    verbose: bool = True,
):
    """Single Domainìš© HDMAPDataModule ìƒì„± ë° ì„¤ì •.
    
    Args:
        domain: ë‹¨ì¼ ë„ë©”ì¸ ì´ë¦„ (ì˜ˆ: "domain_A")
        dataset_root: ë°ì´í„°ì…‹ ë£¨íŠ¸ ê²½ë¡œ (í•„ìˆ˜)
        batch_size: ë°°ì¹˜ í¬ê¸°
        target_size: íƒ€ê²Ÿ ì´ë¯¸ì§€ í¬ê¸° (height, width). Noneì´ë©´ ë¦¬ì‚¬ì´ì¦ˆ ì•ˆ í•¨
        resize_method: ë¦¬ì‚¬ì´ì¦ˆ ë°©ë²• ("resize", "black_padding", "noise_padding")
        val_split_ratio: validation ë¶„í•  ë¹„ìœ¨
        val_split_mode: validation ë¶„í•  ëª¨ë“œ ("FROM_TEST", "NONE", "FROM_TRAIN")
        num_workers: ì›Œì»¤ ìˆ˜
        seed: ëœë¤ ì‹œë“œ
        verbose: ìƒì„¸ ë¡œê·¸ ì¶œë ¥ ì—¬ë¶€
        
    Returns:
        ì„¤ì •ëœ HDMAPDataModule
        
    Examples:
        # 256x256 ë¦¬ì‚¬ì´ì¦ˆ
        datamodule = create_single_domain_datamodule(
            domain="domain_A",
            dataset_root="/path/to/dataset",
            target_size=(256, 256),
            batch_size=8
        )
        
        # 224x224 ë¸”ë™ íŒ¨ë”©
        datamodule = create_single_domain_datamodule(
            domain="domain_B",
            dataset_root="/path/to/dataset",
            target_size=(224, 224),
            resize_method="black_padding"
        )
        
        # ì›ë³¸ í¬ê¸° ìœ ì§€
        datamodule = create_single_domain_datamodule(
            domain="domain_C",
            dataset_root="/path/to/dataset",
            target_size=None
        )
    """
    from anomalib.data.datamodules.image.hdmap import HDMAPDataModule
    from anomalib.data.utils import ValSplitMode
    from pathlib import Path
    
    # ValSplitMode ë¬¸ìì—´ì„ enumìœ¼ë¡œ ë³€í™˜
    split_mode_map = {
        "FROM_TEST": ValSplitMode.FROM_TEST,
        "NONE": ValSplitMode.NONE,
        "FROM_TRAIN": ValSplitMode.FROM_TRAIN
    }
    val_split_mode_enum = split_mode_map.get(val_split_mode, ValSplitMode.FROM_TEST)
    
    if verbose:
        print(f"\nğŸ“¦ HDMAPDataModule ìƒì„± ì¤‘...")
        print(f"   ğŸ¯ ë„ë©”ì¸: {domain}")
        if target_size:
            print(f"   ğŸ“ íƒ€ê²Ÿ í¬ê¸°: {target_size[0]}x{target_size[1]}")
            print(f"   ğŸ”§ ë¦¬ì‚¬ì´ì¦ˆ ë°©ë²•: {resize_method}")
        else:
            print(f"   ğŸ“ íƒ€ê²Ÿ í¬ê¸°: ì›ë³¸ í¬ê¸° ìœ ì§€")
        print(f"   ğŸ“Š ë°°ì¹˜ í¬ê¸°: {batch_size}")
        print(f"   ğŸ”„ Val ë¶„í• : {val_split_mode} (ë¹„ìœ¨: {val_split_ratio})")
    
    # Path ê°ì²´ë¡œ ë³€í™˜ ë° ê²€ì¦
    dataset_root = Path(dataset_root).resolve()
    
    if not dataset_root.exists():
        raise FileNotFoundError(f"ë°ì´í„°ì…‹ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {dataset_root}")
    
    if verbose:
        print(f"   ğŸ“ ë°ì´í„°ì…‹ ê²½ë¡œ: {dataset_root}")
        print(f"   ğŸ“ ë„ë©”ì¸ ê²½ë¡œ: {dataset_root / domain}")
    
    # HDMAPDataModule ìƒì„±
    datamodule = HDMAPDataModule(
        root=str(dataset_root),
        domain=domain,
        train_batch_size=batch_size,
        eval_batch_size=batch_size,
        num_workers=num_workers,
        val_split_mode=val_split_mode_enum,
        val_split_ratio=val_split_ratio,
        seed=seed,
        target_size=target_size,
        resize_method=resize_method
    )
    
    # ë°ì´í„° ì¤€ë¹„ ë° ì„¤ì •
    if verbose:
        print(f"   âš™ï¸  DataModule ì„¤ì • ì¤‘...")
    
    try:
        datamodule.prepare_data()
        datamodule.setup()
    except Exception as e:
        print(f"âŒ DataModule ì„¤ì • ì‹¤íŒ¨: {e}")
        raise
    
    # ë°ì´í„° í†µê³„ ì¶œë ¥
    if verbose:
        print(f"âœ… {domain} ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
        print(f"   í›ˆë ¨ ìƒ˜í”Œ: {len(datamodule.train_data):,}ê°œ")
        
        val_count = len(datamodule.val_data) if hasattr(datamodule, 'val_data') and datamodule.val_data else 0
        print(f"   ê²€ì¦ ìƒ˜í”Œ: {val_count:,}ê°œ")
        print(f"   í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ: {len(datamodule.test_data):,}ê°œ")
        
        # ì²« ë²ˆì§¸ ë°°ì¹˜ë¡œ ë°ì´í„° í˜•íƒœ í™•ì¸
        try:
            train_loader = datamodule.train_dataloader()
            sample_batch = next(iter(train_loader))
            print(f"   ğŸ“Š ì´ë¯¸ì§€ í˜•íƒœ: {sample_batch.image.shape}")
            print(f"   ğŸ“Š ë ˆì´ë¸” í˜•íƒœ: {sample_batch.gt_label.shape}")
            print(f"   ğŸ“Š ë°ì´í„° ë²”ìœ„: [{sample_batch.image.min():.3f}, {sample_batch.image.max():.3f}]")
        except Exception as e:
            print(f"   âš ï¸  ë°°ì¹˜ ì •ë³´ í™•ì¸ ì‹¤íŒ¨: {e}")
    
    return datamodule



# =============================================================================
# ìƒì„¸ ë¶„ì„ í•¨ìˆ˜ë“¤
# =============================================================================

def save_detailed_test_results(
    predictions: Dict[str, Any],
    ground_truth: Dict[str, Any], 
    image_paths: List[str],
    result_dir: Path,
    model_type: str = "unknown"
) -> None:
    """
    í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ ì´ë¯¸ì§€ë³„ë¡œ ìƒì„¸í•˜ê²Œ CSV íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        predictions: ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        ground_truth: ì‹¤ì œ ì •ë‹µ ë”•ì…”ë„ˆë¦¬
        image_paths: ì´ë¯¸ì§€ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        result_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        model_type: ëª¨ë¸ íƒ€ì… (draem_sevnet, patchcore ë“±)
    """
    import pandas as pd
    
    # result_dirì„ analysis_dirë¡œ ì§ì ‘ ì‚¬ìš© (ì¤‘ë³µ í´ë” ìƒì„± ë°©ì§€)
    analysis_dir = Path(result_dir)
    
    # ê²°ê³¼ ë°ì´í„° ìˆ˜ì§‘
    results_data = []
    
    for i, img_path in enumerate(image_paths):
        row = {
            "image_path": img_path,
            "ground_truth": ground_truth.get("labels", [0] * len(image_paths))[i] if isinstance(ground_truth.get("labels"), list) else ground_truth.get("label", [0])[i],
            "anomaly_score": predictions.get("pred_scores", [0] * len(image_paths))[i] if isinstance(predictions.get("pred_scores"), list) else 0,
        }
        
        
        # ì˜ˆì¸¡ ë ˆì´ë¸” ê³„ì‚° (ê¸°ë³¸ threshold 0.5 ì‚¬ìš©)
        row["predicted_label"] = 1 if row["anomaly_score"] > 0.5 else 0
        
        results_data.append(row)
    
    # DataFrame ìƒì„± ë° ì €ì¥
    df = pd.DataFrame(results_data)
    csv_path = analysis_dir / "test_results.csv"
    df.to_csv(csv_path, index=False)
    
    print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ CSV ì €ì¥: {csv_path}")


def plot_roc_curve(
    ground_truth: List[int],
    scores: List[float], 
    result_dir: Path,
    experiment_name: str = "Experiment"
) -> float:
    """
    ROC curveë¥¼ ê·¸ë¦¬ê³  AUROC ê°’ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Args:
        ground_truth: ì‹¤ì œ ì •ë‹µ ë¦¬ìŠ¤íŠ¸ (0 ë˜ëŠ” 1)
        scores: ì˜ˆì¸¡ ì ìˆ˜ ë¦¬ìŠ¤íŠ¸
        result_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        experiment_name: ì‹¤í—˜ ì´ë¦„
        
    Returns:
        AUROC ê°’
    """
    import matplotlib.pyplot as plt
    from sklearn.metrics import roc_curve, auc
    import numpy as np
    
    # result_dirì„ analysis_dirë¡œ ì§ì ‘ ì‚¬ìš© (ì¤‘ë³µ í´ë” ìƒì„± ë°©ì§€)
    analysis_dir = Path(result_dir)
    
    # ROC curve ê³„ì‚°
    fpr, tpr, thresholds = roc_curve(ground_truth, scores)
    auroc = auc(fpr, tpr)
    
    # ìµœì  threshold ê³„ì‚° (Youden's index)
    optimal_idx = np.argmax(tpr - fpr)
    optimal_threshold = thresholds[optimal_idx]
    
    # í”Œë¡¯ ìƒì„±
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUROC = {auroc:.3f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', alpha=0.6)
    plt.scatter(fpr[optimal_idx], tpr[optimal_idx], color='red', s=100, 
                label=f'Optimal threshold = {optimal_threshold:.3f}')
    
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'ROC Curve - {experiment_name}')
    plt.legend(loc="lower right")
    plt.grid(True, alpha=0.3)
    
    # ì €ì¥
    roc_path = analysis_dir / "roc_curve.png"
    plt.savefig(roc_path, dpi=150, bbox_inches='tight')
    plt.close()
    
    print(f"ğŸ“ˆ ROC Curve ì €ì¥: {roc_path}")
    return auroc


def save_metrics_report(
    ground_truth: List[int],
    predictions: List[int],
    scores: List[float],
    result_dir: Path,
    auroc: float,
    optimal_threshold: float = 0.5
) -> None:
    """
    ì„±ëŠ¥ ë©”íŠ¸ë¦­ì„ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        ground_truth: ì‹¤ì œ ì •ë‹µ ë¦¬ìŠ¤íŠ¸
        predictions: ì˜ˆì¸¡ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸  
        scores: ì˜ˆì¸¡ ì ìˆ˜ ë¦¬ìŠ¤íŠ¸
        result_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        auroc: AUROC ê°’
        optimal_threshold: ìµœì  threshold
    """
    from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix
    import json
    
    # result_dirì„ analysis_dirë¡œ ì§ì ‘ ì‚¬ìš© (ì¤‘ë³µ í´ë” ìƒì„± ë°©ì§€)
    analysis_dir = Path(result_dir)
    
    # ë©”íŠ¸ë¦­ ê³„ì‚°
    precision = precision_score(ground_truth, predictions, zero_division=0)
    recall = recall_score(ground_truth, predictions, zero_division=0)
    f1 = f1_score(ground_truth, predictions, zero_division=0)
    cm = confusion_matrix(ground_truth, predictions).tolist()
    
    # ë©”íŠ¸ë¦­ ë³´ê³ ì„œ ìƒì„±
    metrics_report = {
        "auroc": float(auroc),
        "optimal_threshold": float(optimal_threshold),
        "precision": float(precision),
        "recall": float(recall),
        "f1_score": float(f1),
        "confusion_matrix": cm,
        "total_samples": len(ground_truth),
        "positive_samples": sum(ground_truth),
        "negative_samples": len(ground_truth) - sum(ground_truth)
    }
    
    # JSON íŒŒì¼ë¡œ ì €ì¥
    metrics_path = analysis_dir / "metrics_report.json"
    with open(metrics_path, 'w', encoding='utf-8') as f:
        json.dump(metrics_report, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ“‹ ë©”íŠ¸ë¦­ ë³´ê³ ì„œ ì €ì¥: {metrics_path}")


def plot_score_distributions(
    normal_scores: List[float],
    anomaly_scores: List[float], 
    result_dir: Path,
    experiment_name: str = "Experiment"
) -> None:
    """
    ì •ìƒ/ì´ìƒ ìƒ˜í”Œì˜ ì ìˆ˜ ë¶„í¬ë¥¼ íˆìŠ¤í† ê·¸ë¨ìœ¼ë¡œ ì‹œê°í™”í•©ë‹ˆë‹¤.
    
    Args:
        normal_scores: ì •ìƒ ìƒ˜í”Œ ì ìˆ˜ ë¦¬ìŠ¤íŠ¸
        anomaly_scores: ì´ìƒ ìƒ˜í”Œ ì ìˆ˜ ë¦¬ìŠ¤íŠ¸
        result_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        experiment_name: ì‹¤í—˜ ì´ë¦„
    """
    import matplotlib.pyplot as plt
    import numpy as np
    
    # result_dirì„ analysis_dirë¡œ ì§ì ‘ ì‚¬ìš© (ì¤‘ë³µ í´ë” ìƒì„± ë°©ì§€)
    analysis_dir = Path(result_dir)
    
    # íˆìŠ¤í† ê·¸ë¨ ìƒì„±
    plt.figure(figsize=(10, 6))
    
    # ì •ìƒ ìƒ˜í”Œ ë¶„í¬
    plt.hist(normal_scores, bins=50, alpha=0.6, label=f'Normal (n={len(normal_scores)})', 
             color='blue', density=True)
    
    # ì´ìƒ ìƒ˜í”Œ ë¶„í¬  
    plt.hist(anomaly_scores, bins=50, alpha=0.6, label=f'Anomaly (n={len(anomaly_scores)})', 
             color='red', density=True)
    
    plt.xlabel('Anomaly Score')
    plt.ylabel('Density')
    plt.title(f'Score Distributions - {experiment_name}')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # í†µê³„ ì •ë³´ í…ìŠ¤íŠ¸ ì¶”ê°€
    normal_mean, normal_std = np.mean(normal_scores), np.std(normal_scores)
    anomaly_mean, anomaly_std = np.mean(anomaly_scores), np.std(anomaly_scores)
    
    stats_text = f'Normal: Î¼={normal_mean:.3f}, Ïƒ={normal_std:.3f}\\nAnomaly: Î¼={anomaly_mean:.3f}, Ïƒ={anomaly_std:.3f}'
    plt.text(0.02, 0.98, stats_text, transform=plt.gca().transAxes, 
             verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
    
    # ì €ì¥
    dist_path = analysis_dir / "score_distributions.png"
    plt.savefig(dist_path, dpi=150, bbox_inches='tight')
    plt.close()
    
    print(f"ğŸ“Š ì ìˆ˜ ë¶„í¬ ì €ì¥: {dist_path}")

def save_experiment_summary(
    experiment_config: Dict[str, Any],
    results: Dict[str, float],
    result_dir: Path,
    training_time: Optional[str] = None
) -> None:
    """
    ì‹¤í—˜ ì„¤ì •ê³¼ ê²°ê³¼ë¥¼ YAML íŒŒì¼ë¡œ ìš”ì•½ ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        experiment_config: ì‹¤í—˜ ì„¤ì • ë”•ì…”ë„ˆë¦¬
        results: ì‹¤í—˜ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        result_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        training_time: í•™ìŠµ ì‹œê°„ (ì„ íƒì )
    """
    import yaml
    from datetime import datetime
    
    # result_dirì„ analysis_dirë¡œ ì§ì ‘ ì‚¬ìš© (ì¤‘ë³µ í´ë” ìƒì„± ë°©ì§€)
    analysis_dir = Path(result_dir)
    
    # ìš”ì•½ ì •ë³´ ìƒì„±
    summary = {
        'experiment_info': {
            'name': experiment_config.get('name', 'unknown'),
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'training_time': training_time or 'N/A'
        },
        'hyperparameters': experiment_config,
        'results': results,
        'analysis_files': [
            'test_results.csv',
            'roc_curve.png', 
            'metrics_report.json',
            'score_distributions.png',
            'extreme_samples/'
        ]
    }
    
    # YAML íŒŒì¼ë¡œ ì €ì¥
    summary_path = analysis_dir / "experiment_summary.yaml"
    with open(summary_path, 'w', encoding='utf-8') as f:
        yaml.dump(summary, f, default_flow_style=False, allow_unicode=True, indent=2)
    
    print(f"ğŸ“„ ì‹¤í—˜ ìš”ì•½ ì €ì¥: {summary_path}")


def analyze_test_data_distribution(datamodule, test_size: int) -> Tuple[int, int]:
    """í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ë¼ë²¨ ë¶„í¬ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
    
    Args:
        datamodule: í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì œê³µí•˜ëŠ” ë°ì´í„°ëª¨ë“ˆ
        test_size: ì „ì²´ í…ŒìŠ¤íŠ¸ ë°ì´í„° í¬ê¸°
        
    Returns:
        Tuple[int, int]: (fault_count, good_count) 
    """
    import torch
    import numpy as np
    
    print(f"   ğŸ” í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¼ë²¨ ë¶„í¬ ì „ì²´ í™•ì¸ ì¤‘ (ì´ {test_size}ê°œ)...")
    
    test_loader = datamodule.test_dataloader()
    fault_count = 0
    good_count = 0
    total_processed = 0
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(test_loader):
            if hasattr(batch, 'gt_label'):
                labels = batch.gt_label.numpy()
                batch_fault_count = (labels == 1).sum()
                batch_good_count = (labels == 0).sum()
                
                fault_count += batch_fault_count
                good_count += batch_good_count
                total_processed += len(labels)
                
                # ì§„í–‰ë¥  í‘œì‹œ (100 ë°°ì¹˜ë§ˆë‹¤)
                if batch_idx % 100 == 0 and batch_idx > 0:
                    print(f"      ğŸ“Š ì§„í–‰ë¥ : {batch_idx+1} ë°°ì¹˜, {total_processed}ê°œ ì²˜ë¦¬ë¨")
    
    print(f"   âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„í¬ ë¶„ì„ ì™„ë£Œ - ì´ {total_processed}ê°œ ìƒ˜í”Œ")
    print(f"      ğŸš¨ ìµœì¢… ë¶„í¬: Fault={fault_count}, Good={good_count}")
    
    # ë¶„í¬ ë¹„ìœ¨ ê³„ì‚° ë° ê²½ê³ 
    if total_processed > 0:
        fault_ratio = fault_count / total_processed * 100
        good_ratio = good_count / total_processed * 100
        print(f"      ğŸ“ˆ ë¹„ìœ¨: Fault={fault_ratio:.1f}%, Good={good_ratio:.1f}%")
        
        # ë¶ˆê· í˜• ê²½ê³ 
        if fault_count == 0:
            print(f"      âš ï¸  ê²½ê³ : Fault ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤! AUROC ê³„ì‚°ì— ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        elif good_count == 0:
            print(f"      âš ï¸  ê²½ê³ : Good ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤! AUROC ê³„ì‚°ì— ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        elif abs(fault_count - good_count) > total_processed * 0.3:
            print(f"      âš ï¸  ê²½ê³ : ë¼ë²¨ ë¶„í¬ê°€ ë¶ˆê· í˜•í•©ë‹ˆë‹¤ (30% ì´ìƒ ì°¨ì´)")
        else:
            print(f"      âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¼ë²¨ ë¶„í¬ ì •ìƒ")
    
    return fault_count, good_count

def unified_model_evaluation(model, datamodule, experiment_dir, experiment_name, model_type, logger):
    """í†µí•©ëœ ëª¨ë¸ í‰ê°€ í•¨ìˆ˜
    
    Args:
        model: Lightning ëª¨ë¸
        datamodule: ë°ì´í„° ëª¨ë“ˆ
        experiment_dir: ì‹¤í—˜ ë””ë ‰í„°ë¦¬ ê²½ë¡œ
        experiment_name: ì‹¤í—˜ ì´ë¦„
        model_type: ëª¨ë¸ íƒ€ì… (ì†Œë¬¸ì)
        logger: ë¡œê±° ê°ì²´
                
    Returns:
        dict: AUROC, threshold, precision, recall, f1 score, confusion matrix ë“± í‰ê°€ ë©”íŠ¸ë¦­
    """
    import numpy as np
    from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix
    from PIL import Image
    from pathlib import Path
    
    print(f"   ğŸš€ í†µí•© ëª¨ë¸ í‰ê°€ ì‹œì‘...")
    
    # ëª¨ë¸ì„ evaluation ëª¨ë“œë¡œ ì„¤ì •
    model.eval()
    
    # PyTorch ëª¨ë¸ì— ì§ì ‘ ì ‘ê·¼
    torch_model = model.model
    torch_model.eval()
    
    # ëª¨ë¸ì„ GPUë¡œ ì´ë™ (CUDA ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    torch_model = torch_model.to(device)
    print(f"   ğŸ–¥ï¸ ëª¨ë¸ì„ {device}ë¡œ ì´ë™ ì™„ë£Œ")
    
    # ì‹œê°í™” ë””ë ‰í„°ë¦¬ ìƒì„±
    visualization_dir = Path(experiment_dir) / "visualizations"
    visualization_dir.mkdir(exist_ok=True)
    print(f"   ğŸ–¼ï¸ ì‹œê°í™” ì €ì¥ ê²½ë¡œ: {visualization_dir}")
    
    # ë°ì´í„° ìˆ˜ì§‘ì„ ìœ„í•œ ë¦¬ìŠ¤íŠ¸ë“¤
    all_image_paths = []
    all_ground_truth = []
    all_scores = []
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œë” ìƒì„±
    test_dataloader = datamodule.test_dataloader()
    print(f"   âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œë” ìƒì„± ì™„ë£Œ")
    total_batches = len(test_dataloader)
    
    print(f"   ğŸ”„ {total_batches}ê°œ ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘...")
    
    # ë°°ì¹˜ë³„ë¡œ ì˜ˆì¸¡ ìˆ˜í–‰
    with torch.no_grad():
        for batch_idx, batch in enumerate(test_dataloader):
            print(f"   ğŸ“ ì²˜ë¦¬ ì¤‘: {batch_idx+1}/{total_batches} ë°°ì¹˜ (ì§„í–‰ë¥ : {100*(batch_idx+1)/total_batches:.1f}%)")
            
            # ì´ë¯¸ì§€ ê²½ë¡œ ì¶”ì¶œ (í•„ìˆ˜)
            image_paths = batch.image_path
            if not isinstance(image_paths, list):
                image_paths = [image_paths]
            
            # ì´ë¯¸ì§€ í…ì„œ ì¶”ì¶œ ë° ë””ë°”ì´ìŠ¤ ì´ë™ì„ í•œ ë²ˆì— ì²˜ë¦¬
            image_tensor = batch.image.to(device)
            print(f"      ğŸ–¼ï¸  ì´ë¯¸ì§€ í…ì„œ í¬ê¸°: {image_tensor.shape}, ê²½ë¡œ ìˆ˜: {len(image_paths)}, min: {image_tensor.min().item():.4f}, q1: {image_tensor.quantile(0.25).item():.4f}, q2: {image_tensor.quantile(0.5).item():.4f}, q3: {image_tensor.quantile(0.75).item():.4f}, max: {image_tensor.max().item():.4f}")
            
            # ëª¨ë¸ë¡œ ì§ì ‘ ì˜ˆì¸¡ ìˆ˜í–‰
            model_output = torch_model(image_tensor)
            print(f"      âœ… ëª¨ë¸ ì¶œë ¥ ì™„ë£Œ: {type(model_output)}")
                        
            # ëª¨ë¸ë³„ ì¶œë ¥ì—ì„œ ì ìˆ˜ë“¤ ì¶”ì¶œ
            final_scores = extract_scores_from_model_output(
                model_output, image_tensor.shape[0], batch_idx, model_type
            )
            
            # ì‹œê°í™” ìƒì„± (ì „ì²´ ë°°ì¹˜)
            create_batch_visualizations(image_tensor, model_output, image_paths, visualization_dir, batch_idx)
            
            # Ground truth ì¶”ì¶œ (ì´ë¯¸ì§€ ê²½ë¡œì—ì„œ)
            gt_labels = []
            for path in image_paths:
                if '/fault/' in path:
                    gt_labels.append(1)  # anomaly
                elif '/good/' in path:
                    gt_labels.append(0)  # normal
                else:
                    gt_labels.append(0)  # ê¸°ë³¸ê°’
            
            # ê²°ê³¼ ìˆ˜ì§‘
            all_image_paths.extend(image_paths)
            all_ground_truth.extend(gt_labels)
            all_scores.extend(final_scores.flatten() if hasattr(final_scores, 'flatten') else final_scores)
            
            print(f"      âœ… ë°°ì¹˜ {batch_idx+1} ì™„ë£Œ: {len(gt_labels)}ê°œ ìƒ˜í”Œ ì¶”ê°€")
    
    print(f"   âœ… ì´ {len(all_image_paths)}ê°œ ìƒ˜í”Œ ì²˜ë¦¬ ì™„ë£Œ")
    
    # ê¸¸ì´ ë§ì¶”ê¸°
    min_len = min(len(all_ground_truth), len(all_scores))
    all_ground_truth = all_ground_truth[:min_len]
    all_scores = all_scores[:min_len]
    
    print(f"   âœ… í†µí•© í‰ê°€: {len(all_ground_truth)}ê°œ ìƒ˜í”Œë¡œ ë©”íŠ¸ë¦­ ê³„ì‚°")
    
    # NaN ê°’ í™•ì¸ ë° í•„í„°ë§
    import numpy as np
    scores_array = np.array(all_scores)
    gt_array = np.array(all_ground_truth)
    
    nan_mask = np.isnan(scores_array)
    if nan_mask.any():
        nan_count = nan_mask.sum()
        print(f"   âš ï¸  NaN ì ìˆ˜ {nan_count}ê°œ ë°œê²¬, ì œê±° í›„ ê³„ì†")
        
        # NaNì´ ì•„ë‹Œ ê°’ë“¤ë§Œ í•„í„°ë§
        valid_mask = ~nan_mask
        scores_array = scores_array[valid_mask]
        gt_array = gt_array[valid_mask]
        all_scores = scores_array.tolist()
        all_ground_truth = gt_array.tolist()
        
        print(f"   âœ… ìœ íš¨í•œ ìƒ˜í”Œ: {len(all_scores)}ê°œ")
    
    # AUROC ê³„ì‚° ë° ROC curve ìƒì„±
    try:
        auroc = roc_auc_score(all_ground_truth, all_scores)
    except ValueError as e:
        print(f"   âŒ í‰ê°€ ì‹¤íŒ¨: {e}")
        return None
    
    # ì„ê³„ê°’ ê³„ì‚° (Youden's J statistic)
    fpr, tpr, thresholds = roc_curve(all_ground_truth, all_scores)
    optimal_idx = np.argmax(tpr - fpr)
    optimal_threshold = thresholds[optimal_idx]
    
    print(f"   ğŸ“ˆ AUROC: {auroc:.4f}, ìµœì  ì„ê³„ê°’: {optimal_threshold:.4f}")
    
    # ì˜ˆì¸¡ ë¼ë²¨ ìƒì„±
    predictions = (np.array(all_scores) > optimal_threshold).astype(int)
    
    # Confusion Matrix ê³„ì‚°
    cm = confusion_matrix(all_ground_truth, predictions)
    
    # ë©”íŠ¸ë¦­ ê³„ì‚°
    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, len(all_ground_truth), 0)
    
    accuracy = (tp + tn) / len(all_ground_truth) if len(all_ground_truth) > 0 else 0
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"   ğŸ§® í†µí•© Confusion Matrix:")
    print(f"       ì‹¤ì œ\\ì˜ˆì¸¡    Normal  Anomaly")
    print(f"       Normal     {cm[0,0]:6d}  {cm[0,1]:6d}")
    print(f"       Anomaly    {cm[1,0]:6d}  {cm[1,1]:6d}")
    
    print(f"   ğŸ“ˆ í†µí•© ë©”íŠ¸ë¦­:")
    print(f"      AUROC: {auroc:.4f}")
    print(f"      Accuracy: {accuracy:.4f}")
    print(f"      Precision: {precision:.4f}")
    print(f"      Recall: {recall:.4f}")
    print(f"      F1-Score: {f1:.4f}")
    print(f"      Threshold: {optimal_threshold:.4f}")
    
    # ê¸°ë³¸ ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬
    unified_metrics = {
        "auroc": float(auroc),
        "accuracy": float(accuracy), 
        "precision": float(precision),
        "recall": float(recall),
        "f1_score": float(f1),
        "confusion_matrix": cm.tolist(),
        "optimal_threshold": float(optimal_threshold),
        "total_samples": len(all_ground_truth),
        "positive_samples": int(np.sum(all_ground_truth)),
        "negative_samples": int(len(all_ground_truth) - np.sum(all_ground_truth))
    }
    
    # analysis í´ë” ìƒì„± ë° ê²°ê³¼ ì €ì¥
    analysis_dir = Path(experiment_dir) / "analysis"
    analysis_dir.mkdir(exist_ok=True)
    print(f"   ğŸ’¾ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì¤‘: {analysis_dir}")
    
    # ìƒì„¸ í…ŒìŠ¤íŠ¸ ê²°ê³¼ CSV ì €ì¥
    predictions_dict = {
        "pred_scores": all_scores,
    }
    ground_truth_dict = {
        "labels": all_ground_truth
    }
    save_detailed_test_results(
        predictions_dict, ground_truth_dict, all_image_paths, 
        analysis_dir, model_type
    )
    
    # ROC curve ìƒì„±
    plot_roc_curve(all_ground_truth, all_scores, analysis_dir, experiment_name)
    
    # ë©”íŠ¸ë¦­ ë³´ê³ ì„œ ì €ì¥
    save_metrics_report(all_ground_truth, predictions, all_scores, analysis_dir, auroc, optimal_threshold)
    
    # ì ìˆ˜ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨ ìƒì„±
    normal_scores = [score for gt, score in zip(all_ground_truth, all_scores) if gt == 0]
    anomaly_scores = [score for gt, score in zip(all_ground_truth, all_scores) if gt == 1]
    plot_score_distributions(normal_scores, anomaly_scores, analysis_dir, experiment_name)
        
    # ì‹¤í—˜ ìš”ì•½ ì €ì¥
    save_experiment_summary({}, {"auroc": auroc}, analysis_dir)
    
    logger.info(f"í†µí•© í‰ê°€ ì™„ë£Œ: AUROC={auroc:.4f}, F1={f1:.4f}, ìƒ˜í”Œìˆ˜={len(all_image_paths)}")
    
    return unified_metrics


def extract_scores_from_model_output(model_output, batch_size, batch_idx, model_type):
    """
    ëª¨ë¸ë³„ ì¶œë ¥ì—ì„œ ì ìˆ˜ë“¤ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    Args:
        model_output: ëª¨ë¸ ì¶œë ¥ ê°ì²´
        batch_size: ë°°ì¹˜ í¬ê¸°
        batch_idx: ë°°ì¹˜ ì¸ë±ìŠ¤
        model_type: ëª¨ë¸ íƒ€ì… (ì†Œë¬¸ì)
        
    Returns:
        tuple: (anomaly_scores,)
    """
    import numpy as np
    
    model_type = model_type.lower()
    
    if model_type == "draem":
        # DRAEM: pred_scoreë§Œ ìˆìŒ
        if hasattr(model_output, 'pred_score'):
            final_scores = model_output.pred_score.cpu().numpy()
            
            # NaN ê°’ í™•ì¸ ë° ì²˜ë¦¬
            if np.isnan(final_scores).any():
                print(f"      âš ï¸  DRAEM pred_scoreì— NaN ë°œê²¬, 0.0ìœ¼ë¡œ ëŒ€ì²´")
                final_scores = np.nan_to_num(final_scores, nan=0.0)
            
            # ì—†ëŠ” ê°’ì€ 0 ìœ¼ë¡œ ì²˜ë¦¬ (DRAEMì—ëŠ” mask_score, severity_score, raw_severity_score, normalized_severity_score ì—†ìŒ)
            print(f"      ğŸ“Š DRAEM ì ìˆ˜ ì¶”ì¶œ: pred_score={final_scores[0]:.4f}")
        elif hasattr(model_output, 'anomaly_map'):
            # anomaly_mapì—ì„œ ì ìˆ˜ ê³„ì‚°
            anomaly_map = model_output.anomaly_map.cpu().numpy()
            final_scores = [float(np.max(am)) if am.size > 0 else 0.0 for am in anomaly_map]
            print(f"      ğŸ“Š DRAEM ì ìˆ˜ ì¶”ì¶œ (anomaly_map): max={final_scores[0]:.4f}")
        else:
            raise AttributeError("DRAEM ì¶œë ¥ ì†ì„± ì—†ìŒ")

    elif model_type == "draem_cutpaste_clf":
        # draem_cutpaste_clfì˜ ê²½ìš° pred_scoreì™€ anomaly_mapì´ ëª¨ë‘ ì¡´ì¬
        try:
            final_scores = model_output.pred_score.cpu().numpy()

            # NaN ê°’ í™•ì¸ ë° ì²˜ë¦¬
            if np.isnan(final_scores).any():
                raise ValueError(f"      âŒ DRAEM CutPaste Clf pred_scoreì— NaNì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. (ë°°ì¹˜ {batch_idx})")

            print(f"      ğŸ“Š DRAEM CutPaste Clf ì ìˆ˜ ì¶”ì¶œ: first pred_score={final_scores[0]:.4f}")
        except AttributeError:
            raise AttributeError("DRAEM CutPaste Clf ì¶œë ¥ ì†ì„± ì—†ìŒ")

    elif model_type == "patchcore":
        # PatchCore: pred_scoreë§Œ ìˆìŒ
        if hasattr(model_output, 'pred_score'):
            final_scores = model_output.pred_score.cpu().numpy()
            print(f"      ğŸ“Š PatchCore ì ìˆ˜ ì¶”ì¶œ: pred_score={final_scores[0]:.4f}")
        elif hasattr(model_output, 'anomaly_map'):
            # anomaly_mapì—ì„œ ì ìˆ˜ ê³„ì‚°
            anomaly_map = model_output.anomaly_map.cpu().numpy()
            final_scores = [float(np.max(am)) if am.size > 0 else 0.0 for am in anomaly_map]
            print(f"      ğŸ“Š PatchCore ì ìˆ˜ ì¶”ì¶œ (anomaly_map): max={final_scores[0]:.4f}")
        else:
            raise AttributeError("PatchCore ì¶œë ¥ ì†ì„± ì—†ìŒ")
            
    elif model_type == "dinomaly":
        # Dinomaly: pred_score ë˜ëŠ” anomaly_map
        if hasattr(model_output, 'pred_score'):
            final_scores = model_output.pred_score.cpu().numpy()
            print(f"      ğŸ“Š Dinomaly ì ìˆ˜ ì¶”ì¶œ: pred_score={final_scores[0]:.4f}")
        elif hasattr(model_output, 'anomaly_map'):
            # anomaly_mapì—ì„œ ì ìˆ˜ ê³„ì‚°
            anomaly_map = model_output.anomaly_map.cpu().numpy()
            final_scores = [float(np.max(am)) if am.size > 0 else 0.0 for am in anomaly_map]
            print(f"      ğŸ“Š Dinomaly ì ìˆ˜ ì¶”ì¶œ (anomaly_map): max={final_scores[0]:.4f}")
        else:
            raise AttributeError("Dinomaly ì¶œë ¥ ì†ì„± ì—†ìŒ")
            
    else:
        # ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸ íƒ€ì…: ì¼ë°˜ì ì¸ ì†ì„±ìœ¼ë¡œ ì‹œë„
        print(f"   âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸ íƒ€ì…: {model_type}, ì¼ë°˜ì ì¸ ì†ì„±ìœ¼ë¡œ ì‹œë„")
        if hasattr(model_output, 'pred_score'):
            final_scores = model_output.pred_score.cpu().numpy()
        elif hasattr(model_output, 'final_score'):
            final_scores = model_output.final_score.cpu().numpy()
        elif hasattr(model_output, 'anomaly_map'):
            anomaly_map = model_output.anomaly_map.cpu().numpy()
            final_scores = [float(np.max(am)) for am in anomaly_map]
        else:
            raise AttributeError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ëª¨ë¸ ì¶œë ¥ í˜•ì‹: {type(model_output)}")
            
        print(f"      ğŸ“Š ì¼ë°˜ ëª¨ë¸ ì ìˆ˜ ì¶”ì¶œ: anomaly_score={final_scores[0]:.4f}")
        
    return final_scores


def create_anomaly_heatmap_with_colorbar(anomaly_map_array, target_size, cmap='viridis', show_colorbar=False):
    """Anomaly heatmap ìƒì„± (colorbar ì˜µì…˜)
    
    Args:
        anomaly_map_array: numpy array [H, W]
        target_size: (width, height) ëª©í‘œ í¬ê¸°
        cmap: matplotlib colormap ì´ë¦„ (ê¸°ë³¸ê°’: 'viridis')
              - 'viridis': íŒŒë€ìƒ‰->ì´ˆë¡ìƒ‰->ë…¸ë€ìƒ‰ (ê¶Œì¥)
              - 'jet': íŒŒë€ìƒ‰->ì²­ë¡->ë…¸ë‘->ë¹¨ê°• (ê¸°ì¡´)
              - 'hot': ê²€ì •->ë¹¨ê°•->ë…¸ë‘->í°ìƒ‰
              - 'plasma': ë³´ë¼->ë¶„í™->ë…¸ë‘
              - 'inferno': ê²€ì •->ë³´ë¼->ë¹¨ê°•->ë…¸ë‘
              - 'turbo': íŒŒë€ìƒ‰->ì²­ë¡->ì´ˆë¡->ë…¸ë‘->ë¹¨ê°•
              - 'coolwarm': íŒŒë€ìƒ‰->í°ìƒ‰->ë¹¨ê°•
        show_colorbar: colorbar í‘œì‹œ ì—¬ë¶€ (ê¸°ë³¸ê°’: False)
        
    Returns:
        PIL.Image: heatmap ì´ë¯¸ì§€ (colorbar í¬í•¨/ì œì™¸)
    """
    import matplotlib.pyplot as plt
    import matplotlib.cm as cm
    from matplotlib.colors import Normalize
    import io
    from PIL import Image
    
    # ê°’ ë²”ìœ„ ê³„ì‚°
    vmin = anomaly_map_array.min()
    vmax = anomaly_map_array.max()
    
    # Figure í¬ê¸° ë° ë ˆì´ì•„ì›ƒ ì„¤ì •
    if show_colorbar:
        # Colorbar í¬í•¨ ë ˆì´ì•„ì›ƒ
        fig_width = 6
        fig_height = 4
        fig, (ax_img, ax_cbar) = plt.subplots(1, 2, figsize=(fig_width, fig_height), 
                                              gridspec_kw={'width_ratios': [4, 0.3]})
    else:
        # Colorbar ì—†ëŠ” ë ˆì´ì•„ì›ƒ
        fig_width = 4
        fig_height = 4
        fig, ax_img = plt.subplots(1, 1, figsize=(fig_width, fig_height))
    
    # Heatmap ìƒì„±
    norm = Normalize(vmin=vmin, vmax=vmax)
    im = ax_img.imshow(anomaly_map_array, cmap=cmap, norm=norm, aspect='auto')
    ax_img.axis('off')
    
    # Colorbar ìƒì„± (ì˜µì…˜ì— ë”°ë¼)
    if show_colorbar:
        cbar = plt.colorbar(im, cax=ax_cbar)
        cbar.set_label('Anomaly Score', rotation=270, labelpad=15, fontsize=9)
        cbar.ax.tick_params(labelsize=8)
    
    plt.tight_layout()
    
    # PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
    buf = io.BytesIO()
    plt.savefig(buf, format='png', dpi=100, bbox_inches='tight', pad_inches=0.05)
    buf.seek(0)
    heatmap_pil = Image.open(buf).convert('RGB')
    plt.close()
    
    return heatmap_pil


def create_batch_visualizations(image_tensor, model_output, image_paths, visualization_dir, batch_idx):
    """ë°°ì¹˜ì— ëŒ€í•œ ì‹œê°í™” ìƒì„± (ì›ë³¸ ì´ë¯¸ì§€ + anomaly map)
    
    Args:
        image_tensor: ì…ë ¥ ì´ë¯¸ì§€ í…ì„œ [B, C, H, W]
        model_output: ëª¨ë¸ ì¶œë ¥ ê°ì²´
        image_paths: ì´ë¯¸ì§€ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        visualization_dir: ì‹œê°í™” ì €ì¥ ë””ë ‰í„°ë¦¬
        batch_idx: ë°°ì¹˜ ì¸ë±ìŠ¤
    """
    import numpy as np
    from PIL import Image
    from pathlib import Path
    
    # anomalib ì‹œê°í™” í•¨ìˆ˜ë“¤ import
    from anomalib.visualization.image.functional import (
        overlay_images,
        create_image_grid,
        add_text_to_image
    )
    
    # ë°°ì¹˜ì—ì„œ anomaly map ì¶”ì¶œ
    anomaly_maps = None
    if hasattr(model_output, 'anomaly_map'):
        anomaly_maps = model_output.anomaly_map
    else:
        return
    
    # ë°°ì¹˜ í¬ê¸°
    batch_size = image_tensor.shape[0]
    
    # ê° ì´ë¯¸ì§€ì— ëŒ€í•´ ì‹œê°í™” ìƒì„±
    for i in range(batch_size):  # ì „ì²´ ë°°ì¹˜ ì‹œê°í™”
        try:
            # ì›ë³¸ ì´ë¯¸ì§€ ì¶”ì¶œ ë° ë³€í™˜
            original_img_tensor = image_tensor[i]  # [C, H, W]
            
            # í…ì„œë¥¼ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜ (ì •ê·œí™” í•´ì œ)
            # ì´ë¯¸ì§€ê°€ [0, 1] ë²”ìœ„ë¼ê³  ê°€ì •
            if original_img_tensor.max() <= 1.0:
                original_img_array = (original_img_tensor.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)
            else:
                original_img_array = original_img_tensor.permute(1, 2, 0).cpu().numpy().astype(np.uint8)
            
            # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ì¸ ê²½ìš° RGBë¡œ ë³€í™˜
            if original_img_array.shape[2] == 1:
                original_img_array = np.repeat(original_img_array, 3, axis=2)
            elif original_img_array.shape[2] > 3:
                original_img_array = original_img_array[:, :, :3]
                
            original_img_pil = Image.fromarray(original_img_array, mode='RGB')
            
            # Anomaly map ì¶”ì¶œ ë° ë³€í™˜
            anomaly_map_tensor = anomaly_maps[i]  # [H, W] ë˜ëŠ” [1, H, W]
            if len(anomaly_map_tensor.shape) == 3:
                anomaly_map_tensor = anomaly_map_tensor.squeeze(0)  # [H, W]
            
            # ì›ë³¸ ì´ë¯¸ì§€ì™€ anomaly map í¬ê¸° ë§ì¶”ê¸°
            target_size = original_img_pil.size
            
            # anomaly mapì„ matplotlibìœ¼ë¡œ ì‹œê°í™”
            # ë‹¤ë¥¸ colormap ì˜µì…˜: 'jet', 'hot', 'plasma', 'inferno', 'turbo', 'coolwarm'
            anomaly_map_vis = create_anomaly_heatmap_with_colorbar(
                anomaly_map_tensor.cpu().numpy(),
                target_size,
                cmap='hot',        # ì›í•˜ëŠ” colormapìœ¼ë¡œ ë³€ê²½ ê°€ëŠ¥
                show_colorbar=False    # colorbar í‘œì‹œ: True/False
            )
            
            # ì˜¤ë²„ë ˆì´ ìƒì„± (ì›ë³¸ + anomaly map)
            overlay_img = overlay_images(
                base=original_img_pil,
                overlays=anomaly_map_vis,
                alpha=0.5
            )
            
            # í…ìŠ¤íŠ¸ ì¶”ê°€
            original_with_text = add_text_to_image(
                original_img_pil.copy(), 
                "Original Image",
                font=None, size=10, color="white", background=(0, 0, 0, 128)
            )
            
            overlay_with_text = add_text_to_image(
                overlay_img.copy(), 
                "Original + Anomaly Map",
                font=None, size=10, color="white", background=(0, 0, 0, 128)
            )
            
            # 2ê°œ ì´ë¯¸ì§€ë¥¼ ê°€ë¡œë¡œ ë°°ì¹˜
            visualization_grid = create_image_grid(
                [original_with_text, overlay_with_text], 
                nrow=2
            )
            
            # íŒŒì¼ëª… ë° ë””ë ‰í„°ë¦¬ ìƒì„± (ë ˆì´ë¸”ë³„ë¡œ ë¶„ë¥˜)
            if i < len(image_paths):
                image_path = Path(image_paths[i])
                image_filename = image_path.stem
                
                # ê²½ë¡œì—ì„œ ë ˆì´ë¸” ì¶”ì¶œ (fault ë˜ëŠ” good)
                label = None
                if '/fault/' in str(image_path):
                    label = 'fault'
                elif '/good/' in str(image_path):
                    label = 'good'
                else:
                    label = 'unknown'
                
                # ë ˆì´ë¸”ë³„ ë””ë ‰í„°ë¦¬ ìƒì„±
                label_dir = visualization_dir / label
                label_dir.mkdir(exist_ok=True)
                
                # íŒŒì¼ëª…ì€ ì›ë³¸ ì´ë¯¸ì§€ ì´ë¦„ ì‚¬ìš©
                save_filename = f"{image_filename}.png"
                save_path = label_dir / save_filename
            else:
                # ì´ë¯¸ì§€ ê²½ë¡œê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ í˜•ì‹ ì‚¬ìš©
                save_filename = f"batch_{batch_idx:03d}_sample_{i:02d}.png"
                save_path = visualization_dir / save_filename
            
            # ì´ë¯¸ì§€ ì €ì¥
            visualization_grid.save(save_path)
            
        except Exception as e:
            print(f"âŒ ìƒ˜í”Œ {i} ì‹œê°í™” ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
