#!/usr/bin/env python3
"""Anomaly Detection ì‹¤í—˜ì„ ìœ„í•œ ê³µí†µ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤.

ì´ ëª¨ë“ˆì€ DRAEM, PaDiM ë“± ë‹¤ì–‘í•œ Anomaly Detection ëª¨ë¸ ì‹¤í—˜ì—ì„œ 
ì¬ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ê³µí†µ í•¨ìˆ˜ë“¤ì„ ì œê³µí•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- GPU ë©”ëª¨ë¦¬ ê´€ë¦¬
- ì‹¤í—˜ ë¡œê¹… ì„¤ì •
- ê²°ê³¼ ì´ë¯¸ì§€ ì •ë¦¬ ë° ì‹œê°í™”
- Target domain í‰ê°€
- ì‹¤í—˜ ê²°ê³¼ ë¶„ì„ ë° ì €ì¥
"""

import os
import gc
import json
import shutil
import warnings
import logging
import torch
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List, Optional, Union, Tuple
from lightning.pytorch.callbacks import EarlyStopping

# Anomalib imports
from anomalib.engine import Engine


def load_experiment_conditions(json_filename: str) -> List[Dict[str, Any]]:
    """
    JSON íŒŒì¼ì—ì„œ ì‹¤í—˜ ì¡°ê±´ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    
    Args:
        json_filename: ë¡œë“œí•  JSON íŒŒì¼ëª… (í™•ì¥ì í¬í•¨)
        
    Returns:
        ì‹¤í—˜ ì¡°ê±´ ë¦¬ìŠ¤íŠ¸
        
    Raises:
        FileNotFoundError: JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš°
        json.JSONDecodeError: JSON íŒŒì‹± ì˜¤ë¥˜ê°€ ë°œìƒí•œ ê²½ìš°
    """
    # caller ìŠ¤í¬ë¦½íŠ¸ê°€ ìˆëŠ” ë””ë ‰í† ë¦¬ ê¸°ì¤€ìœ¼ë¡œ JSON íŒŒì¼ ê²½ë¡œ ìƒì„±
    import inspect
    caller_frame = inspect.stack()[1]
    caller_dir = os.path.dirname(os.path.abspath(caller_frame.filename))
    json_path = os.path.join(caller_dir, json_filename)
    
    if not os.path.exists(json_path):
        raise FileNotFoundError(f"ì‹¤í—˜ ì¡°ê±´ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {json_path}")
    
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except json.JSONDecodeError as e:
        raise json.JSONDecodeError(f"JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
    
    # JSONì—ì„œ ë¡œë“œí•œ ë°ì´í„°ì˜ ìœ íš¨ì„± ê²€ì‚¬
    if 'experiment_conditions' not in data:
        raise ValueError("JSON íŒŒì¼ì— 'experiment_conditions' í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    experiment_conditions = data['experiment_conditions']
    
    # JSONì—ì„œëŠ” tupleì´ listë¡œ ì €ì¥ë˜ë¯€ë¡œ, í•„ìš”í•œ í•„ë“œë“¤ì„ ë‹¤ì‹œ tupleë¡œ ë³€í™˜
    for condition in experiment_conditions:
        if 'config' not in condition:
            continue
            
        config = condition['config']
        
        # range íƒ€ì…ì˜ í•„ë“œë“¤ì„ tupleë¡œ ë³€í™˜
        range_fields = ['patch_width_range', 'patch_ratio_range']
        for field in range_fields:
            if field in config and isinstance(config[field], list):
                config[field] = tuple(config[field])
    
    return experiment_conditions

def create_experiment_visualization(
    experiment_name: str,
    model_type: str,
    results_base_dir: str,
    source_domain: str = None,
    target_domains: list = None,
    source_results: Dict[str, Any] = None,
    target_results: Dict[str, Dict[str, Any]] = None,
    single_domain: bool = False
) -> str:
    """ì‹¤í—˜ ê²°ê³¼ ì‹œê°í™” í´ë” êµ¬ì¡° ìƒì„± ë° ì‹¤í—˜ ì •ë³´ ì €ì¥.
    
    Args:
        experiment_name: ì‹¤í—˜ ì´ë¦„
        model_type: ëª¨ë¸ íƒ€ì… (ì˜ˆ: "DRAEM-SevNet", "PaDiM")
        results_base_dir: ê¸°ë³¸ ê²°ê³¼ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        source_domain: ì†ŒìŠ¤ ë„ë©”ì¸ ì´ë¦„ (single_domain=Trueì¼ ë•ŒëŠ” None ê°€ëŠ¥)
        target_domains: íƒ€ê²Ÿ ë„ë©”ì¸ ë¦¬ìŠ¤íŠ¸
        source_results: ì†ŒìŠ¤ ë„ë©”ì¸ í‰ê°€ ê²°ê³¼
        target_results: íƒ€ê²Ÿ ë„ë©”ì¸ë“¤ í‰ê°€ ê²°ê³¼
        single_domain: ë‹¨ì¼ ë„ë©”ì¸ ì‹¤í—˜ ì—¬ë¶€
        
    Returns:
        str: ìƒì„±ëœ visualize ë””ë ‰í† ë¦¬ ê²½ë¡œ
    """
    print(f"\nğŸ¨ {model_type} Visualization ìƒì„±")
    
    # ê¸°ë³¸ ê²½ë¡œë¥¼ ì‚¬ìš© (ì´ë¯¸ ì‹¤í—˜ë³„ ê³ ìœ  ê²½ë¡œì„)
    base_path = Path(results_base_dir)
    
    # ë¨¼ì € v* íŒ¨í„´ì˜ ë²„ì „ í´ë”ê°€ ìˆëŠ”ì§€ í™•ì¸
    version_dirs = [d for d in base_path.glob("v*") if d.is_dir() and d.name.startswith('v') and d.name[1:].isdigit()]
    if version_dirs:
        # v0, v1 ë“±ì˜ íŒ¨í„´ì´ ìˆìœ¼ë©´ ìµœì‹  ë²„ì „ ì‚¬ìš©
        latest_version_path = max(version_dirs, key=lambda x: int(x.name[1:]))
    else:
        # ë²„ì „ í´ë”ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©
        latest_version_path = base_path
    
    # visualize í´ë” ìƒì„±
    viz_path = latest_version_path / "visualize"
    viz_path.mkdir(exist_ok=True)
    
    # í´ë” êµ¬ì¡° ìƒì„± (single_domain ì‹¤í—˜ì—ì„œëŠ” target_domains í´ë” ìƒì„±í•˜ì§€ ì•ŠìŒ)
    if single_domain:
        folders_to_create = ["results"]
    else:
        folders_to_create = [
            "source_domain",
            "target_domains"
        ]
    
    for folder in folders_to_create:
        (viz_path / folder).mkdir(exist_ok=True)
    
    # íƒ€ê²Ÿ ë„ë©”ì¸ë³„ í•˜ìœ„ í´ë” ìƒì„± (multi-domain ì‹¤í—˜ì—ë§Œ ì ìš©)
    if not single_domain and target_domains:
        for domain in target_domains:
            (viz_path / "target_domains" / domain).mkdir(exist_ok=True)
    
    # ì‹¤í—˜ ì •ë³´ë¥¼ JSONìœ¼ë¡œ ì €ì¥
    experiment_info = {
        "experiment_name": experiment_name,
        "model_type": model_type,
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "results_path": str(latest_version_path),
        "source_domain": source_domain,
        "target_domains": target_domains or [],
        "results_summary": {
            "source_results": source_results or {},
            "target_results": target_results or {}
        }
    }
    
    # JSON íŒŒì¼ë¡œ ì €ì¥
    info_file = viz_path / "experiment_info.json"
    with open(info_file, 'w', encoding='utf-8') as f:
        json.dump(experiment_info, f, indent=2, ensure_ascii=False)
    
    # ì‹¤ì œ ìƒì„±ëœ ì´ë¯¸ì§€ íŒŒì¼ë“¤ì„ visualize/resultsë¡œ ë³µì‚¬ (single domainì˜ ê²½ìš°)
    if single_domain:
        try:
            # anomalib ëª¨ë¸ ê²°ê³¼ ì´ë¯¸ì§€ê°€ ì €ì¥ë˜ëŠ” íŒ¨í„´ íƒìƒ‰
            image_patterns = [
                latest_version_path / "*" / "*" / source_domain / "latest" / "images",  # ì¼ë°˜ì ì¸ íŒ¨í„´
                latest_version_path / "*" / source_domain / "latest" / "images",        # ì¶•ì•½ëœ íŒ¨í„´
                latest_version_path / "images",                                          # ì§ì ‘ images í´ë”
            ]
            
            images_found = False
            for pattern_path in image_patterns:
                # glob íŒ¨í„´ìœ¼ë¡œ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ ì°¾ê¸°
                for images_dir in Path(str(pattern_path).replace('*', '')).parent.glob('**/images'):
                    if images_dir.exists() and any(images_dir.iterdir()):
                        print(f"ğŸ“ ì´ë¯¸ì§€ ë°œê²¬: {images_dir}")
                        
                        # ì´ë¯¸ì§€ íŒŒì¼ë“¤ì„ visualize/resultsë¡œ ë³µì‚¬
                        results_dir = viz_path / "results"
                        
                        # ì„œë¸Œ ë””ë ‰í† ë¦¬ë³„ë¡œ ë³µì‚¬ (good, fault ë“±)
                        for subdir in images_dir.iterdir():
                            if subdir.is_dir():
                                target_subdir = results_dir / subdir.name
                                target_subdir.mkdir(exist_ok=True)
                                
                                # ëª¨ë“  ì´ë¯¸ì§€ íŒŒì¼ ë³µì‚¬
                                image_files = list(subdir.glob('*.png'))
                                for img_file in image_files:
                                    shutil.copy2(img_file, target_subdir / img_file.name)
                                
                                print(f"   ğŸ“¸ {len(image_files)}ê°œ ì´ë¯¸ì§€ë¥¼ {target_subdir}ì— ë³µì‚¬")
                        
                        images_found = True
                        break
                
                if images_found:
                    break
            
            if not images_found:
                print(f"âš ï¸ ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {latest_version_path}")
        
        except Exception as copy_error:
            print(f"âš ï¸ ì´ë¯¸ì§€ ë³µì‚¬ ì¤‘ ì˜¤ë¥˜: {copy_error}")
    
    print(f"âœ… {model_type} í´ë” êµ¬ì¡° ìƒì„± ì™„ë£Œ: {viz_path}")
    
    return str(viz_path)


def cleanup_gpu_memory():
    """GPU ë©”ëª¨ë¦¬ ì •ë¦¬ (ëª¨ë“  ëª¨ë¸ì—ì„œ ê³µí†µ ì‚¬ìš© ê°€ëŠ¥)."""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        gc.collect()


def setup_warnings_filter():
    """ì‹¤í—˜ ì‹œ ë¶ˆí•„ìš”í•œ ê²½ê³  ë©”ì‹œì§€ í•„í„°ë§ (ëª¨ë“  ëª¨ë¸ì—ì„œ ê³µí†µ ì‚¬ìš© ê°€ëŠ¥)."""
    warnings.filterwarnings("ignore", category=FutureWarning)
    warnings.filterwarnings("ignore", category=DeprecationWarning)
    warnings.filterwarnings("ignore", category=UserWarning)
    
    # ì‹œê°í™” ê´€ë ¨ íŠ¹ì • ê²½ê³  í•„í„°ë§ (ë” í¬ê´„ì )
    warnings.filterwarnings("ignore", message=".*Field.*gt_mask.*is None.*")
    warnings.filterwarnings("ignore", message=".*Skipping visualization.*")
    warnings.filterwarnings("ignore", message=".*gt_mask.*None.*")


def setup_experiment_logging(log_file_path: str, experiment_name: str) -> logging.Logger:
    """ì‹¤í—˜ ë¡œê¹… ì„¤ì • (ëª¨ë“  ëª¨ë¸ì—ì„œ ê³µí†µ ì‚¬ìš© ê°€ëŠ¥).
    
    Args:
        log_file_path: ë¡œê·¸ íŒŒì¼ ê²½ë¡œ
        experiment_name: ì‹¤í—˜ ì´ë¦„
        
    Returns:
        logging.Logger: ì„¤ì •ëœ ë¡œê±°
    """
    # ë¡œê±° ì„¤ì •
    logger = logging.getLogger(experiment_name)
    logger.setLevel(logging.INFO)
    
    # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±°
    for handler in logger.handlers[:]:
        logger.removeHandler(handler)
    
    # íŒŒì¼ í•¸ë“¤ëŸ¬ ì¶”ê°€
    file_handler = logging.FileHandler(log_file_path, encoding='utf-8')
    file_handler.setLevel(logging.INFO)
    
    # í¬ë§·í„° ì„¤ì •
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    file_handler.setFormatter(formatter)
    
    logger.addHandler(file_handler)
    
    return logger


def extract_training_info(engine: Engine) -> Dict[str, Any]:
    """PyTorch Lightning Engineì—ì„œ í•™ìŠµ ì •ë³´ ì¶”ì¶œ (ëª¨ë“  ëª¨ë¸ì—ì„œ ê³µí†µ ì‚¬ìš© ê°€ëŠ¥).
    
    Args:
        engine: Anomalib Engine ê°ì²´
        
    Returns:
        Dict[str, Any]: í•™ìŠµ ì •ë³´ ë”•ì…”ë„ˆë¦¬
    """
    import torch
    
    trainer = engine.trainer
    
    # Early Stopping ì½œë°± ì°¾ê¸°
    early_stopping_callback = None
    for callback in trainer.callbacks:
        if isinstance(callback, EarlyStopping):
            early_stopping_callback = callback
            break
    
    # ì‹¤ì œ í•™ìŠµ ì™„ë£Œ ì—í­ ê³„ì‚°
    if early_stopping_callback and hasattr(early_stopping_callback, 'stopped_epoch') and early_stopping_callback.stopped_epoch > 0:
        # Early stoppingì´ ë°œìƒí•œ ê²½ìš°: stopped_epochì´ ë§ˆì§€ë§‰ í•™ìŠµ ì—í­
        last_trained_epoch = early_stopping_callback.stopped_epoch + 1  # 0-based â†’ 1-based
        early_stopped = True
    else:
        # ì •ìƒ ì™„ë£Œ ë˜ëŠ” early stopping ì—†ìŒ
        last_trained_epoch = trainer.current_epoch + 1  # 0-based â†’ 1-based
        early_stopped = False
    
    # ê¸°ë³¸ ì •ë³´
    training_info = {
        "max_epochs_configured": trainer.max_epochs,
        "last_trained_epoch": last_trained_epoch,  # ì‹¤ì œ ë§ˆì§€ë§‰ìœ¼ë¡œ í•™ìŠµí•œ ì—í­ (1-based)
        "total_steps": trainer.global_step,
        "early_stopped": early_stopped,
        "early_stop_reason": None,
        "best_val_auroc": None,  # ìµœê³  validation AUROC
        "f1_threshold_issue": "F1ScoreëŠ” ê¸°ë³¸ thresholdë¡œ ê³„ì‚°ë¨. AUROC ëŒ€ë¹„ ë‚®ì„ ìˆ˜ ìˆìŒ."
    }
    
    # Early stopping ì„¸ë¶€ ì •ë³´ ì¶”ê°€
    if early_stopping_callback:
        if training_info["early_stopped"]:
            training_info["early_stop_reason"] = f"No improvement for {early_stopping_callback.patience} epochs"
        
        # ìµœê³  ì„±ëŠ¥ ê¸°ë¡
        if hasattr(early_stopping_callback, 'best_score') and early_stopping_callback.best_score is not None:
            best_score = early_stopping_callback.best_score
            if hasattr(best_score, 'cpu'):
                training_info["best_val_auroc"] = float(best_score.cpu())
            else:
                training_info["best_val_auroc"] = float(best_score)
    
    # ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ì •ë³´ì—ì„œë„ ìµœê³  ì„±ëŠ¥ ì¶”ì¶œ
    checkpoint_callback = None
    for callback in trainer.callbacks:
        if hasattr(callback, 'best_model_score'):
            checkpoint_callback = callback
            break
    
    if checkpoint_callback and hasattr(checkpoint_callback, 'best_model_score') and checkpoint_callback.best_model_score is not None:
        best_score = checkpoint_callback.best_model_score
        if hasattr(best_score, 'cpu'):
            training_info["best_val_auroc"] = float(best_score.cpu())
        else:
            training_info["best_val_auroc"] = float(best_score)
    
    # í•™ìŠµ ì™„ë£Œ ë°©ì‹ ê²°ì •
    if training_info["early_stopped"]:
        completion_type = "early_stopping"
        completion_description = f"Early stopped at epoch {training_info['last_trained_epoch']}"
    elif training_info["last_trained_epoch"] >= training_info["max_epochs_configured"]:
        completion_type = "max_epochs_reached"
        completion_description = f"Completed max epochs {training_info['max_epochs_configured']}"
    else:
        completion_type = "interrupted"
        completion_description = f"Interrupted at epoch {training_info['last_trained_epoch']}"
    
    training_info["completion_type"] = completion_type
    training_info["completion_description"] = completion_description
    
    return training_info

def save_experiment_results(
    result: Dict[str, Any], 
    result_filename: str, 
    log_dir: Path, 
    logger: logging.Logger,
    model_type: str = "Model"
) -> Path:
    """ì‹¤í—˜ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥ (ëª¨ë“  ëª¨ë¸ì—ì„œ ê³µí†µ ì‚¬ìš© ê°€ëŠ¥).
    
    Args:
        result: ì‹¤í—˜ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        result_filename: ì €ì¥í•  íŒŒì¼ëª…
        log_dir: ë¡œê·¸ ë””ë ‰í† ë¦¬ (ì‹¤íŒ¨ ì‹œ ì‚¬ìš©)
        logger: ë¡œê±° ê°ì²´
        model_type: ëª¨ë¸ íƒ€ì… (ë¡œê¹…ìš©)
        
    Returns:
        Path: ì‹¤ì œ ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
    """
    # ì‹¤í—˜ë³„ ê²½ë¡œì— ì €ì¥í•˜ê±°ë‚˜, ì‹¤íŒ¨ ì‹œ log_dirì— ì €ì¥
    if result.get("experiment_path") and Path(result["experiment_path"]).exists():
        result_path = Path(result["experiment_path"]) / result_filename
        print(f"   ğŸ“ ê²°ê³¼ íŒŒì¼ì„ ì‹¤í—˜ í´ë”ì— ì €ì¥: {result_path}")
    else:
        result_path = log_dir / result_filename
        print(f"   ğŸ“ ê²°ê³¼ íŒŒì¼ì„ ë¡œê·¸ í´ë”ì— ì €ì¥: {result_path}")
    
    # ê²°ê³¼ JSON ì§ë ¬í™”ë¥¼ ìœ„í•œ ì²˜ë¦¬
    serializable_result = json.loads(json.dumps(result, default=str))
    
    with open(result_path, 'w', encoding='utf-8') as f:
        json.dump(serializable_result, f, indent=2, ensure_ascii=False)
    
    # ê²°ê³¼ ìš”ì•½ ë¡œê¹…
    if result["status"] == "success":
        logger.info("âœ… ì‹¤í—˜ ì„±ê³µ!")
        
        # AUROC ì •ë³´ ë¡œê¹… (multi-domain ë˜ëŠ” all-domainsì— ë”°ë¼ ë‹¤ë¥´ê²Œ ì²˜ë¦¬)
        if 'source_results' in result:
            # Multi-domain ì‹¤í—˜ì˜ ê²½ìš°
            source_auroc = result['source_results'].get('image_AUROC', None)
            if isinstance(source_auroc, (int, float)):
                logger.info(f"   Source Domain AUROC: {source_auroc:.4f}")
            else:
                logger.info(f"   Source Domain AUROC: {source_auroc or 'N/A'}")
            
            # Target Domains Avg AUROC ì•ˆì „í•œ í¬ë§·íŒ…
            avg_target_auroc = result.get('avg_target_auroc', None)
            if isinstance(avg_target_auroc, (int, float)):
                logger.info(f"   Target Domains Avg AUROC: {avg_target_auroc:.4f}")
            else:
                logger.info(f"   Target Domains Avg AUROC: {avg_target_auroc or 'N/A'}")
                
        elif 'all_domains_results' in result:
            # All-domains ì‹¤í—˜ì˜ ê²½ìš°
            all_domains_auroc = result['all_domains_results'].get('test_image_AUROC', None)
            if isinstance(all_domains_auroc, (int, float)):
                logger.info(f"   All Domains AUROC: {all_domains_auroc:.4f}")
            else:
                logger.info(f"   All Domains AUROC: {all_domains_auroc or 'N/A'}")
        else:
            logger.info("   AUROC ì •ë³´ ì—†ìŒ")
        
        logger.info(f"   ì²´í¬í¬ì¸íŠ¸: {result.get('best_checkpoint', 'N/A')}")
        
        # í•™ìŠµ ê³¼ì • ì •ë³´ ë¡œê¹…
        training_info = result.get('training_info', {})
        if training_info:
            logger.info("ğŸ“Š í•™ìŠµ ê³¼ì • ì •ë³´:")
            logger.info(f"   ì„¤ì •ëœ ìµœëŒ€ ì—í¬í¬: {training_info.get('max_epochs_configured', 'N/A')}")
            logger.info(f"   ì‹¤ì œ í•™ìŠµ ì—í¬í¬: {training_info.get('last_trained_epoch', 'N/A')}")
            logger.info(f"   ì´ í•™ìŠµ ìŠ¤í…: {training_info.get('total_steps', 'N/A')}")
            logger.info(f"   Early Stopping ì ìš©: {training_info.get('early_stopped', 'N/A')}")
            if training_info.get('early_stopped'):
                logger.info(f"   Early Stopping ì‚¬ìœ : {training_info.get('early_stop_reason', 'N/A')}")
            # ìµœê³  Validation AUROC ì•ˆì „í•œ í¬ë§·íŒ…
            best_val_auroc = training_info.get('best_val_auroc', None)
            if isinstance(best_val_auroc, (int, float)):
                logger.info(f"   ìµœê³  Validation AUROC: {best_val_auroc:.4f}")
            else:
                logger.info(f"   ìµœê³  Validation AUROC: {best_val_auroc or 'N/A'}")
            logger.info(f"   í•™ìŠµ ì™„ë£Œ ë°©ì‹: {training_info.get('completion_description', 'N/A')}")
        
        # Target Domainë³„ ìƒì„¸ ì„±ëŠ¥ ë¡œê¹…
        target_results = result.get('target_results', {})
        if target_results:
            logger.info("ğŸ¯ Target Domainë³„ ì„±ëŠ¥:")
            for domain, domain_result in target_results.items():
                domain_auroc = domain_result.get('image_AUROC', 'N/A')
                if isinstance(domain_auroc, (int, float)):
                    logger.info(f"   {domain}: {domain_auroc:.4f}")
                else:
                    logger.info(f"   {domain}: {domain_auroc}")
    else:
        logger.info("âŒ ì‹¤í—˜ ì‹¤íŒ¨!")
        logger.info(f"   Error: {result.get('error', 'Unknown error')}")
    
    logger.info(f"ğŸ“ ê²°ê³¼ íŒŒì¼: {result_path}")
    
    # ì‹¤í—˜ë³„ ê²½ë¡œì— ì €ì¥ëœ ê²½ìš° ì¶”ê°€ ì •ë³´ ë¡œê¹…
    if result.get("experiment_path"):
        logger.info(f"ğŸ“‚ ì‹¤í—˜ í´ë”: {result['experiment_path']}")
    
    return result_path


def create_single_domain_datamodule(
    domain: str,
    dataset_root: str,
    batch_size: int = 16,
    target_size: tuple[int, int] | None = None,
    resize_method: str = "resize",
    val_split_ratio: float = 0.2,
    val_split_mode: str = "FROM_TEST",
    num_workers: int = 4,
    seed: int = 42,
    verbose: bool = True,
):
    """Single Domainìš© HDMAPDataModule ìƒì„± ë° ì„¤ì •.
    
    Args:
        domain: ë‹¨ì¼ ë„ë©”ì¸ ì´ë¦„ (ì˜ˆ: "domain_A")
        dataset_root: ë°ì´í„°ì…‹ ë£¨íŠ¸ ê²½ë¡œ (í•„ìˆ˜)
        batch_size: ë°°ì¹˜ í¬ê¸°
        target_size: íƒ€ê²Ÿ ì´ë¯¸ì§€ í¬ê¸° (height, width). Noneì´ë©´ ë¦¬ì‚¬ì´ì¦ˆ ì•ˆ í•¨
        resize_method: ë¦¬ì‚¬ì´ì¦ˆ ë°©ë²• ("resize", "black_padding", "noise_padding")
        val_split_ratio: validation ë¶„í•  ë¹„ìœ¨
        val_split_mode: validation ë¶„í•  ëª¨ë“œ ("FROM_TEST", "NONE", "FROM_TRAIN")
        num_workers: ì›Œì»¤ ìˆ˜
        seed: ëœë¤ ì‹œë“œ
        verbose: ìƒì„¸ ë¡œê·¸ ì¶œë ¥ ì—¬ë¶€
        
    Returns:
        ì„¤ì •ëœ HDMAPDataModule
        
    Examples:
        # 256x256 ë¦¬ì‚¬ì´ì¦ˆ
        datamodule = create_single_domain_datamodule(
            domain="domain_A",
            dataset_root="/path/to/dataset",
            target_size=(256, 256),
            batch_size=8
        )
        
        # 224x224 ë¸”ë™ íŒ¨ë”©
        datamodule = create_single_domain_datamodule(
            domain="domain_B",
            dataset_root="/path/to/dataset",
            target_size=(224, 224),
            resize_method="black_padding"
        )
        
        # ì›ë³¸ í¬ê¸° ìœ ì§€
        datamodule = create_single_domain_datamodule(
            domain="domain_C",
            dataset_root="/path/to/dataset",
            target_size=None
        )
    """
    from anomalib.data.datamodules.image.hdmap import HDMAPDataModule
    from anomalib.data.utils import ValSplitMode
    from pathlib import Path
    
    # ValSplitMode ë¬¸ìì—´ì„ enumìœ¼ë¡œ ë³€í™˜
    split_mode_map = {
        "FROM_TEST": ValSplitMode.FROM_TEST,
        "NONE": ValSplitMode.NONE,
        "FROM_TRAIN": ValSplitMode.FROM_TRAIN
    }
    val_split_mode_enum = split_mode_map.get(val_split_mode, ValSplitMode.FROM_TEST)
    
    if verbose:
        print(f"\nğŸ“¦ HDMAPDataModule ìƒì„± ì¤‘...")
        print(f"   ğŸ¯ ë„ë©”ì¸: {domain}")
        if target_size:
            print(f"   ğŸ“ íƒ€ê²Ÿ í¬ê¸°: {target_size[0]}x{target_size[1]}")
            print(f"   ğŸ”§ ë¦¬ì‚¬ì´ì¦ˆ ë°©ë²•: {resize_method}")
        else:
            print(f"   ğŸ“ íƒ€ê²Ÿ í¬ê¸°: ì›ë³¸ í¬ê¸° ìœ ì§€")
        print(f"   ğŸ“Š ë°°ì¹˜ í¬ê¸°: {batch_size}")
        print(f"   ğŸ”„ Val ë¶„í• : {val_split_mode} (ë¹„ìœ¨: {val_split_ratio})")
    
    # Path ê°ì²´ë¡œ ë³€í™˜ ë° ê²€ì¦
    dataset_root = Path(dataset_root).resolve()
    
    if not dataset_root.exists():
        raise FileNotFoundError(f"ë°ì´í„°ì…‹ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {dataset_root}")
    
    if verbose:
        print(f"   ğŸ“ ë°ì´í„°ì…‹ ê²½ë¡œ: {dataset_root}")
        print(f"   ğŸ“ ë„ë©”ì¸ ê²½ë¡œ: {dataset_root / domain}")
    
    # HDMAPDataModule ìƒì„±
    datamodule = HDMAPDataModule(
        root=str(dataset_root),
        domain=domain,
        train_batch_size=batch_size,
        eval_batch_size=batch_size,
        num_workers=num_workers,
        val_split_mode=val_split_mode_enum,
        val_split_ratio=val_split_ratio,
        seed=seed,
        target_size=target_size,
        resize_method=resize_method
    )
    
    # ë°ì´í„° ì¤€ë¹„ ë° ì„¤ì •
    if verbose:
        print(f"   âš™ï¸  DataModule ì„¤ì • ì¤‘...")
    
    try:
        datamodule.prepare_data()
        datamodule.setup()
    except Exception as e:
        print(f"âŒ DataModule ì„¤ì • ì‹¤íŒ¨: {e}")
        raise
    
    # ë°ì´í„° í†µê³„ ì¶œë ¥
    if verbose:
        print(f"âœ… {domain} ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
        print(f"   í›ˆë ¨ ìƒ˜í”Œ: {len(datamodule.train_data):,}ê°œ")
        
        val_count = len(datamodule.val_data) if hasattr(datamodule, 'val_data') and datamodule.val_data else 0
        print(f"   ê²€ì¦ ìƒ˜í”Œ: {val_count:,}ê°œ")
        print(f"   í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ: {len(datamodule.test_data):,}ê°œ")
        
        # ì²« ë²ˆì§¸ ë°°ì¹˜ë¡œ ë°ì´í„° í˜•íƒœ í™•ì¸
        try:
            train_loader = datamodule.train_dataloader()
            sample_batch = next(iter(train_loader))
            print(f"   ğŸ“Š ì´ë¯¸ì§€ í˜•íƒœ: {sample_batch.image.shape}")
            print(f"   ğŸ“Š ë ˆì´ë¸” í˜•íƒœ: {sample_batch.gt_label.shape}")
            print(f"   ğŸ“Š ë°ì´í„° ë²”ìœ„: [{sample_batch.image.min():.3f}, {sample_batch.image.max():.3f}]")
        except Exception as e:
            print(f"   âš ï¸  ë°°ì¹˜ ì •ë³´ í™•ì¸ ì‹¤íŒ¨: {e}")
    
    return datamodule



# =============================================================================
# ìƒì„¸ ë¶„ì„ í•¨ìˆ˜ë“¤
# =============================================================================

def save_detailed_test_results(
    predictions: Dict[str, Any],
    ground_truth: Dict[str, Any], 
    image_paths: List[str],
    result_dir: Path,
    model_type: str = "unknown"
) -> None:
    """
    í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ ì´ë¯¸ì§€ë³„ë¡œ ìƒì„¸í•˜ê²Œ CSV íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        predictions: ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        ground_truth: ì‹¤ì œ ì •ë‹µ ë”•ì…”ë„ˆë¦¬
        image_paths: ì´ë¯¸ì§€ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        result_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        model_type: ëª¨ë¸ íƒ€ì… (draem_sevnet, patchcore ë“±)
    """
    import pandas as pd
    
    # result_dirì„ analysis_dirë¡œ ì§ì ‘ ì‚¬ìš© (ì¤‘ë³µ í´ë” ìƒì„± ë°©ì§€)
    analysis_dir = Path(result_dir)
    
    # ê²°ê³¼ ë°ì´í„° ìˆ˜ì§‘
    results_data = []
    
    # ì•ˆì „í•œ ë°ì´í„° ì ‘ê·¼ì„ ìœ„í•œ ë³€ìˆ˜ ì¤€ë¹„
    labels = ground_truth.get("labels", [0] * len(image_paths))
    pred_scores = predictions.get("pred_scores", [0] * len(image_paths))
    
    # None ì²´í¬ ë° ê¸°ë³¸ê°’ ì„¤ì •
    if labels is None:
        labels = [0] * len(image_paths)
    if pred_scores is None:
        pred_scores = [0] * len(image_paths)
    
    # ê¸¸ì´ í™•ì¸ ë° ì¡°ì •
    if len(image_paths) == 0:
        print("âš ï¸ Warning: No image paths provided")
        return
        
    min_len = min(len(image_paths), len(labels), len(pred_scores))
    
    if min_len != len(image_paths):
        print(f"âš ï¸ Warning: Length mismatch - paths: {len(image_paths)}, labels: {len(labels)}, scores: {len(pred_scores)}, using min: {min_len}")
    
    for i in range(min_len):
        img_path = image_paths[i]
        row = {
            "image_path": img_path,
            "ground_truth": labels[i] if i < len(labels) else 0,
            "anomaly_score": pred_scores[i] if i < len(pred_scores) else 0,
        }
        
        
        # ì˜ˆì¸¡ ë ˆì´ë¸” ê³„ì‚° (ê¸°ë³¸ threshold 0.5 ì‚¬ìš©)
        row["predicted_label"] = 1 if row["anomaly_score"] > 0.5 else 0
        
        results_data.append(row)
    
    # DataFrame ìƒì„± ë° ì €ì¥
    df = pd.DataFrame(results_data)
    csv_path = analysis_dir / "test_results.csv"
    df.to_csv(csv_path, index=False)
    
    print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ CSV ì €ì¥: {csv_path}")


def find_optimal_threshold(ground_truth: List[int], scores: List[float]) -> float:
    """Youden's J statisticì„ ì‚¬ìš©í•˜ì—¬ optimal threshold ì°¾ê¸°"""
    from sklearn.metrics import roc_curve
    import numpy as np
    
    fpr, tpr, thresholds = roc_curve(ground_truth, scores)
    j_scores = tpr - fpr  # Youden's J statistic = Sensitivity + Specificity - 1
    optimal_idx = np.argmax(j_scores)
    return float(thresholds[optimal_idx])

def evaluate_with_fixed_threshold(
    scores: List[float], 
    labels: List[int], 
    threshold: float
) -> Dict[str, float]:
    """ê³ ì •ëœ thresholdë¡œ ì„±ëŠ¥ í‰ê°€"""
    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
    import numpy as np
    
    predictions = (np.array(scores) > threshold).astype(int)
    
    return {
        'threshold': float(threshold),
        'accuracy': float(accuracy_score(labels, predictions)),
        'precision': float(precision_score(labels, predictions, zero_division=0)),
        'recall': float(recall_score(labels, predictions, zero_division=0)),
        'f1_score': float(f1_score(labels, predictions, zero_division=0))
    }

def plot_roc_curve(
    ground_truth: List[int],
    scores: List[float], 
    result_dir: Path,
    experiment_name: str = "Experiment"
) -> float:
    """
    ROC curveë¥¼ ê·¸ë¦¬ê³  AUROC ê°’ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Args:
        ground_truth: ì‹¤ì œ ì •ë‹µ ë¦¬ìŠ¤íŠ¸ (0 ë˜ëŠ” 1)
        scores: ì˜ˆì¸¡ ì ìˆ˜ ë¦¬ìŠ¤íŠ¸
        result_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        experiment_name: ì‹¤í—˜ ì´ë¦„
        
    Returns:
        AUROC ê°’
    """
    import matplotlib.pyplot as plt
    from sklearn.metrics import roc_curve, auc
    import numpy as np
    
    # result_dirì„ analysis_dirë¡œ ì§ì ‘ ì‚¬ìš© (ì¤‘ë³µ í´ë” ìƒì„± ë°©ì§€)
    analysis_dir = Path(result_dir)
    
    # ROC curve ê³„ì‚°
    fpr, tpr, thresholds = roc_curve(ground_truth, scores)
    auroc = auc(fpr, tpr)
    
    # ìµœì  threshold ê³„ì‚° (Youden's index)
    optimal_idx = np.argmax(tpr - fpr)
    optimal_threshold = thresholds[optimal_idx]
    
    # í”Œë¡¯ ìƒì„±
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUROC = {auroc:.3f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', alpha=0.6)
    plt.scatter(fpr[optimal_idx], tpr[optimal_idx], color='red', s=100, 
                label=f'Optimal threshold = {optimal_threshold:.3f}')
    
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'ROC Curve - {experiment_name}')
    plt.legend(loc="lower right")
    plt.grid(True, alpha=0.3)
    
    # ì €ì¥
    roc_path = analysis_dir / "roc_curve.png"
    plt.savefig(roc_path, dpi=150, bbox_inches='tight')
    plt.close()
    
    print(f"ğŸ“ˆ ROC Curve ì €ì¥: {roc_path}")
    return auroc


def save_metrics_report(
    ground_truth: List[int],
    predictions: List[int],
    scores: List[float],
    result_dir: Path,
    auroc: float,
    optimal_threshold: float = 0.5
) -> None:
    """
    ì„±ëŠ¥ ë©”íŠ¸ë¦­ì„ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        ground_truth: ì‹¤ì œ ì •ë‹µ ë¦¬ìŠ¤íŠ¸
        predictions: ì˜ˆì¸¡ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸  
        scores: ì˜ˆì¸¡ ì ìˆ˜ ë¦¬ìŠ¤íŠ¸
        result_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        auroc: AUROC ê°’
        optimal_threshold: ìµœì  threshold
    """
    from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix
    import json
    
    # result_dirì„ analysis_dirë¡œ ì§ì ‘ ì‚¬ìš© (ì¤‘ë³µ í´ë” ìƒì„± ë°©ì§€)
    analysis_dir = Path(result_dir)
    
    # ë©”íŠ¸ë¦­ ê³„ì‚°
    precision = precision_score(ground_truth, predictions, zero_division=0)
    recall = recall_score(ground_truth, predictions, zero_division=0)
    f1 = f1_score(ground_truth, predictions, zero_division=0)
    cm = confusion_matrix(ground_truth, predictions).tolist()
    
    # ë©”íŠ¸ë¦­ ë³´ê³ ì„œ ìƒì„±
    metrics_report = {
        "auroc": float(auroc),
        "optimal_threshold": float(optimal_threshold),
        "precision": float(precision),
        "recall": float(recall),
        "f1_score": float(f1),
        "confusion_matrix": cm,
        "total_samples": int(len(ground_truth)),
        "positive_samples": int(sum(ground_truth)),
        "negative_samples": int(len(ground_truth) - sum(ground_truth))
    }
    
    # JSON íŒŒì¼ë¡œ ì €ì¥
    metrics_path = analysis_dir / "metrics_report.json"
    with open(metrics_path, 'w', encoding='utf-8') as f:
        json.dump(metrics_report, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ“‹ ë©”íŠ¸ë¦­ ë³´ê³ ì„œ ì €ì¥: {metrics_path}")


def plot_score_distributions(
    normal_scores: List[float],
    anomaly_scores: List[float], 
    result_dir: Path,
    experiment_name: str = "Experiment"
) -> None:
    """
    ì •ìƒ/ì´ìƒ ìƒ˜í”Œì˜ ì ìˆ˜ ë¶„í¬ë¥¼ íˆìŠ¤í† ê·¸ë¨ìœ¼ë¡œ ì‹œê°í™”í•©ë‹ˆë‹¤.
    
    Args:
        normal_scores: ì •ìƒ ìƒ˜í”Œ ì ìˆ˜ ë¦¬ìŠ¤íŠ¸
        anomaly_scores: ì´ìƒ ìƒ˜í”Œ ì ìˆ˜ ë¦¬ìŠ¤íŠ¸
        result_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        experiment_name: ì‹¤í—˜ ì´ë¦„
    """
    import matplotlib.pyplot as plt
    import numpy as np
    
    # result_dirì„ analysis_dirë¡œ ì§ì ‘ ì‚¬ìš© (ì¤‘ë³µ í´ë” ìƒì„± ë°©ì§€)
    analysis_dir = Path(result_dir)
    
    # íˆìŠ¤í† ê·¸ë¨ ìƒì„±
    plt.figure(figsize=(10, 6))
    
    # ì •ìƒ ìƒ˜í”Œ ë¶„í¬
    plt.hist(normal_scores, bins=50, alpha=0.6, label=f'Normal (n={len(normal_scores)})', 
             color='blue', density=True)
    
    # ì´ìƒ ìƒ˜í”Œ ë¶„í¬  
    plt.hist(anomaly_scores, bins=50, alpha=0.6, label=f'Anomaly (n={len(anomaly_scores)})', 
             color='red', density=True)
    
    plt.xlabel('Anomaly Score')
    plt.ylabel('Density')
    plt.title(f'Score Distributions - {experiment_name}')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # í†µê³„ ì •ë³´ í…ìŠ¤íŠ¸ ì¶”ê°€
    normal_mean, normal_std = np.mean(normal_scores), np.std(normal_scores)
    anomaly_mean, anomaly_std = np.mean(anomaly_scores), np.std(anomaly_scores)
    
    stats_text = f'Normal: Î¼={normal_mean:.3f}, Ïƒ={normal_std:.3f}\\nAnomaly: Î¼={anomaly_mean:.3f}, Ïƒ={anomaly_std:.3f}'
    plt.text(0.02, 0.98, stats_text, transform=plt.gca().transAxes, 
             verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
    
    # ì €ì¥
    dist_path = analysis_dir / "score_distributions.png"
    plt.savefig(dist_path, dpi=150, bbox_inches='tight')
    plt.close()
    
    print(f"ğŸ“Š ì ìˆ˜ ë¶„í¬ ì €ì¥: {dist_path}")

def save_experiment_summary(
    experiment_config: Dict[str, Any],
    results: Dict[str, float],
    result_dir: Path,
    training_time: Optional[str] = None
) -> None:
    """
    ì‹¤í—˜ ì„¤ì •ê³¼ ê²°ê³¼ë¥¼ YAML íŒŒì¼ë¡œ ìš”ì•½ ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        experiment_config: ì‹¤í—˜ ì„¤ì • ë”•ì…”ë„ˆë¦¬
        results: ì‹¤í—˜ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        result_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        training_time: í•™ìŠµ ì‹œê°„ (ì„ íƒì )
    """
    import yaml
    from datetime import datetime
    
    # result_dirì„ analysis_dirë¡œ ì§ì ‘ ì‚¬ìš© (ì¤‘ë³µ í´ë” ìƒì„± ë°©ì§€)
    analysis_dir = Path(result_dir)
    
    # ìš”ì•½ ì •ë³´ ìƒì„±
    summary = {
        'experiment_info': {
            'name': experiment_config.get('name', 'unknown'),
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'training_time': training_time or 'N/A'
        },
        'hyperparameters': experiment_config,
        'results': results,
        'analysis_files': [
            'test_results.csv',
            'roc_curve.png', 
            'metrics_report.json',
            'score_distributions.png',
            'extreme_samples/'
        ]
    }
    
    # YAML íŒŒì¼ë¡œ ì €ì¥
    summary_path = analysis_dir / "experiment_summary.yaml"
    with open(summary_path, 'w', encoding='utf-8') as f:
        yaml.dump(summary, f, default_flow_style=False, allow_unicode=True, indent=2)
    
    print(f"ğŸ“„ ì‹¤í—˜ ìš”ì•½ ì €ì¥: {summary_path}")


def analyze_test_data_distribution(datamodule, test_size: int) -> Tuple[int, int]:
    """í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ë¼ë²¨ ë¶„í¬ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
    
    Args:
        datamodule: í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì œê³µí•˜ëŠ” ë°ì´í„°ëª¨ë“ˆ
        test_size: ì „ì²´ í…ŒìŠ¤íŠ¸ ë°ì´í„° í¬ê¸°
        
    Returns:
        Tuple[int, int]: (fault_count, good_count) 
    """
    import torch
    import numpy as np
    
    print(f"   ğŸ” í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¼ë²¨ ë¶„í¬ ì „ì²´ í™•ì¸ ì¤‘ (ì´ {test_size}ê°œ)...")
    
    test_loader = datamodule.test_dataloader()
    fault_count = 0
    good_count = 0
    total_processed = 0
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(test_loader):
            if hasattr(batch, 'gt_label'):
                labels = batch.gt_label.numpy()
                batch_fault_count = (labels == 1).sum()
                batch_good_count = (labels == 0).sum()
                
                fault_count += batch_fault_count
                good_count += batch_good_count
                total_processed += len(labels)
                
                # ì§„í–‰ë¥  í‘œì‹œ (100 ë°°ì¹˜ë§ˆë‹¤)
                if batch_idx % 100 == 0 and batch_idx > 0:
                    print(f"      ğŸ“Š ì§„í–‰ë¥ : {batch_idx+1} ë°°ì¹˜, {total_processed}ê°œ ì²˜ë¦¬ë¨")
    
    print(f"   âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„í¬ ë¶„ì„ ì™„ë£Œ - ì´ {total_processed}ê°œ ìƒ˜í”Œ")
    print(f"      ğŸš¨ ìµœì¢… ë¶„í¬: Fault={fault_count}, Good={good_count}")
    
    # ë¶„í¬ ë¹„ìœ¨ ê³„ì‚° ë° ê²½ê³ 
    if total_processed > 0:
        fault_ratio = fault_count / total_processed * 100
        good_ratio = good_count / total_processed * 100
        print(f"      ğŸ“ˆ ë¹„ìœ¨: Fault={fault_ratio:.1f}%, Good={good_ratio:.1f}%")
        
        # ë¶ˆê· í˜• ê²½ê³ 
        if fault_count == 0:
            print(f"      âš ï¸  ê²½ê³ : Fault ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤! AUROC ê³„ì‚°ì— ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        elif good_count == 0:
            print(f"      âš ï¸  ê²½ê³ : Good ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤! AUROC ê³„ì‚°ì— ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        elif abs(fault_count - good_count) > total_processed * 0.3:
            print(f"      âš ï¸  ê²½ê³ : ë¼ë²¨ ë¶„í¬ê°€ ë¶ˆê· í˜•í•©ë‹ˆë‹¤ (30% ì´ìƒ ì°¨ì´)")
        else:
            print(f"      âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¼ë²¨ ë¶„í¬ ì •ìƒ")
    
    return fault_count, good_count

def unified_model_evaluation(model, datamodule, experiment_dir, experiment_name, model_type, logger):
    """í†µí•©ëœ ëª¨ë¸ í‰ê°€ í•¨ìˆ˜
    
    Args:
        model: Lightning ëª¨ë¸
        datamodule: ë°ì´í„° ëª¨ë“ˆ
        experiment_dir: ì‹¤í—˜ ë””ë ‰í„°ë¦¬ ê²½ë¡œ
        experiment_name: ì‹¤í—˜ ì´ë¦„
        model_type: ëª¨ë¸ íƒ€ì… (ì†Œë¬¸ì)
        logger: ë¡œê±° ê°ì²´
                
    Returns:
        dict: AUROC, threshold, precision, recall, f1 score, confusion matrix ë“± í‰ê°€ ë©”íŠ¸ë¦­
    """
    import numpy as np
    from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix
    from PIL import Image
    from pathlib import Path
    
    print(f"   ğŸš€ í†µí•© ëª¨ë¸ í‰ê°€ ì‹œì‘...")
    
    # ëª¨ë¸ì„ evaluation ëª¨ë“œë¡œ ì„¤ì •
    model.eval()
    
    # PyTorch ëª¨ë¸ì— ì§ì ‘ ì ‘ê·¼
    torch_model = model.model
    torch_model.eval()
    
    # ëª¨ë¸ì˜ training flagë¥¼ ëª…ì‹œì ìœ¼ë¡œ Falseë¡œ ì„¤ì • (FastFlow ë“±ì—ì„œ ì¤‘ìš”)
    torch_model.training = False
    
    # ëª¨ë¸ì„ GPUë¡œ ì´ë™ (CUDA ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    torch_model = torch_model.to(device)
    print(f"   ğŸ–¥ï¸ ëª¨ë¸ì„ {device}ë¡œ ì´ë™ ì™„ë£Œ")
    
    # ì‹œê°í™” ë””ë ‰í„°ë¦¬ ìƒì„±
    visualization_dir = Path(experiment_dir) / "visualizations"
    visualization_dir.mkdir(exist_ok=True)
    print(f"   ğŸ–¼ï¸ ì‹œê°í™” ì €ì¥ ê²½ë¡œ: {visualization_dir}")
    
    # ë°ì´í„° ìˆ˜ì§‘ì„ ìœ„í•œ ë¦¬ìŠ¤íŠ¸ë“¤
    all_image_paths = []
    all_ground_truth = []
    all_scores = []
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œë” ìƒì„±
    test_dataloader = datamodule.test_dataloader()
    print(f"   âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œë” ìƒì„± ì™„ë£Œ")
    total_batches = len(test_dataloader)
    
    print(f"   ğŸ”„ {total_batches}ê°œ ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘...")
    
    # ë°°ì¹˜ë³„ë¡œ ì˜ˆì¸¡ ìˆ˜í–‰
    with torch.no_grad():
        for batch_idx, batch in enumerate(test_dataloader):
            print(f"   ğŸ“ ì²˜ë¦¬ ì¤‘: {batch_idx+1}/{total_batches} ë°°ì¹˜ (ì§„í–‰ë¥ : {100*(batch_idx+1)/total_batches:.1f}%)")
            
            # ì´ë¯¸ì§€ ê²½ë¡œ ì¶”ì¶œ (í•„ìˆ˜)
            image_paths = batch.image_path
            if not isinstance(image_paths, list):
                image_paths = [image_paths]
            
            # ì´ë¯¸ì§€ í…ì„œ ì¶”ì¶œ ë° ë””ë°”ì´ìŠ¤ ì´ë™ì„ í•œ ë²ˆì— ì²˜ë¦¬
            image_tensor = batch.image.to(device)
            print(f"      ğŸ–¼ï¸  ì´ë¯¸ì§€ í…ì„œ í¬ê¸°: {image_tensor.shape}, ê²½ë¡œ ìˆ˜: {len(image_paths)}, min: {image_tensor.min().item():.4f}, q1: {image_tensor.quantile(0.25).item():.4f}, q2: {image_tensor.quantile(0.5).item():.4f}, q3: {image_tensor.quantile(0.75).item():.4f}, max: {image_tensor.max().item():.4f}")
            
            # ëª¨ë¸ë¡œ ì§ì ‘ ì˜ˆì¸¡ ìˆ˜í–‰ (inference modeì—ì„œ ì‹¤í–‰)
            with torch.no_grad():
                # FastFlow ë“± ëª¨ë¸ì˜ ê²½ìš° ë°˜ë“œì‹œ eval modeì—ì„œ í˜¸ì¶œí•´ì•¼ í•¨
                model_output = torch_model(image_tensor)
            print(f"      âœ… ëª¨ë¸ ì¶œë ¥ ì™„ë£Œ: {type(model_output)}")
            
            # FastFlow ëª¨ë¸ì˜ ê²½ìš° training modeì¸ì§€ í™•ì¸
            if model_type.lower() == "fastflow":
                print(f"      ğŸ” FastFlow ëª¨ë¸ ìƒíƒœ: training={torch_model.training}")
                if hasattr(model_output, 'pred_score'):
                    print(f"      ğŸ“Š FastFlow pred_score shape: {model_output.pred_score.shape}")
                    print(f"      ğŸ“Š FastFlow pred_score ê°’: {model_output.pred_score.cpu().numpy()}")
                if hasattr(model_output, 'anomaly_map'):
                    print(f"      ğŸ“Š FastFlow anomaly_map shape: {model_output.anomaly_map.shape}")
                    amap_stats = model_output.anomaly_map.cpu().numpy()
                    print(f"      ğŸ“Š FastFlow anomaly_map í†µê³„: min={amap_stats.min():.4f}, max={amap_stats.max():.4f}, mean={amap_stats.mean():.4f}")
                        
            # ëª¨ë¸ë³„ ì¶œë ¥ì—ì„œ ì ìˆ˜ë“¤ ì¶”ì¶œ
            final_scores = extract_scores_from_model_output(
                model_output, image_tensor.shape[0], batch_idx, model_type
            )

            # ì‹œê°í™” ìƒì„± (ì „ì²´ ë°°ì¹˜)
            create_batch_visualizations(image_tensor, model_output, image_paths, visualization_dir, batch_idx, model=torch_model)

            # Ground truth ì¶”ì¶œ (ì´ë¯¸ì§€ ê²½ë¡œì—ì„œ)
            gt_labels = []
            for path in image_paths:
                if '/fault/' in path:
                    gt_labels.append(1)  # anomaly
                elif '/good/' in path:
                    gt_labels.append(0)  # normal
                else:
                    gt_labels.append(0)  # ê¸°ë³¸ê°’
            
            # ê²°ê³¼ ìˆ˜ì§‘
            all_image_paths.extend(image_paths)
            all_ground_truth.extend(gt_labels)
            all_scores.extend(final_scores.flatten() if hasattr(final_scores, 'flatten') else final_scores)
            
            print(f"      âœ… ë°°ì¹˜ {batch_idx+1} ì™„ë£Œ: {len(gt_labels)}ê°œ ìƒ˜í”Œ ì¶”ê°€")
    
    print(f"   âœ… ì´ {len(all_image_paths)}ê°œ ìƒ˜í”Œ ì²˜ë¦¬ ì™„ë£Œ")
    
    # ê¸¸ì´ ë§ì¶”ê¸°
    min_len = min(len(all_ground_truth), len(all_scores))
    all_ground_truth = all_ground_truth[:min_len]
    all_scores = all_scores[:min_len]
    
    print(f"   âœ… í†µí•© í‰ê°€: {len(all_ground_truth)}ê°œ ìƒ˜í”Œë¡œ ë©”íŠ¸ë¦­ ê³„ì‚°")
    
    # NaN ê°’ í™•ì¸ ë° í•„í„°ë§
    import numpy as np
    scores_array = np.array(all_scores)
    gt_array = np.array(all_ground_truth)
    
    nan_mask = np.isnan(scores_array)
    if nan_mask.any():
        nan_count = nan_mask.sum()
        print(f"   âš ï¸  NaN ì ìˆ˜ {nan_count}ê°œ ë°œê²¬, ì œê±° í›„ ê³„ì†")
        
        # NaNì´ ì•„ë‹Œ ê°’ë“¤ë§Œ í•„í„°ë§
        valid_mask = ~nan_mask
        scores_array = scores_array[valid_mask]
        gt_array = gt_array[valid_mask]
        all_scores = scores_array.tolist()
        all_ground_truth = gt_array.tolist()
        
        print(f"   âœ… ìœ íš¨í•œ ìƒ˜í”Œ: {len(all_scores)}ê°œ")
    
    # AUROC ê³„ì‚° ë° ROC curve ìƒì„±
    try:
        auroc = roc_auc_score(all_ground_truth, all_scores)
    except ValueError as e:
        print(f"   âŒ í‰ê°€ ì‹¤íŒ¨: {e}")
        return None
    
    # ì„ê³„ê°’ ê³„ì‚° (Youden's J statistic)
    fpr, tpr, thresholds = roc_curve(all_ground_truth, all_scores)
    optimal_idx = np.argmax(tpr - fpr)
    optimal_threshold = thresholds[optimal_idx]
    
    print(f"   ğŸ“ˆ AUROC: {auroc:.4f}, ìµœì  ì„ê³„ê°’: {optimal_threshold:.4f}")
    
    # ì˜ˆì¸¡ ë¼ë²¨ ìƒì„±
    predictions = (np.array(all_scores) > optimal_threshold).astype(int)
    
    # Confusion Matrix ê³„ì‚°
    cm = confusion_matrix(all_ground_truth, predictions)
    
    # ë©”íŠ¸ë¦­ ê³„ì‚°
    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, len(all_ground_truth), 0)
    
    accuracy = (tp + tn) / len(all_ground_truth) if len(all_ground_truth) > 0 else 0
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"   ğŸ§® í†µí•© Confusion Matrix:")
    print(f"       ì‹¤ì œ\\ì˜ˆì¸¡    Normal  Anomaly")
    print(f"       Normal     {cm[0,0]:6d}  {cm[0,1]:6d}")
    print(f"       Anomaly    {cm[1,0]:6d}  {cm[1,1]:6d}")
    
    print(f"   ğŸ“ˆ í†µí•© ë©”íŠ¸ë¦­:")
    print(f"      AUROC: {auroc:.4f}")
    print(f"      Accuracy: {accuracy:.4f}")
    print(f"      Precision: {precision:.4f}")
    print(f"      Recall: {recall:.4f}")
    print(f"      F1-Score: {f1:.4f}")
    print(f"      Threshold: {optimal_threshold:.4f}")
    
    # ê¸°ë³¸ ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬
    unified_metrics = {
        "auroc": float(auroc),
        "accuracy": float(accuracy), 
        "precision": float(precision),
        "recall": float(recall),
        "f1_score": float(f1),
        "confusion_matrix": cm.tolist(),
        "optimal_threshold": float(optimal_threshold),
        "total_samples": len(all_ground_truth),
        "positive_samples": int(np.sum(all_ground_truth)),
        "negative_samples": int(len(all_ground_truth) - np.sum(all_ground_truth))
    }
    
    # analysis í´ë” ìƒì„± ë° ê²°ê³¼ ì €ì¥
    analysis_dir = Path(experiment_dir) / "analysis"
    analysis_dir.mkdir(exist_ok=True)
    print(f"   ğŸ’¾ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì¤‘: {analysis_dir}")
    
    # ìƒì„¸ í…ŒìŠ¤íŠ¸ ê²°ê³¼ CSV ì €ì¥
    predictions_dict = {
        "pred_scores": all_scores,
    }
    ground_truth_dict = {
        "labels": all_ground_truth
    }
    save_detailed_test_results(
        predictions_dict, ground_truth_dict, all_image_paths, 
        analysis_dir, model_type
    )
    
    # ROC curve ìƒì„±
    plot_roc_curve(all_ground_truth, all_scores, analysis_dir, experiment_name)
    
    # ë©”íŠ¸ë¦­ ë³´ê³ ì„œ ì €ì¥
    save_metrics_report(all_ground_truth, predictions, all_scores, analysis_dir, auroc, optimal_threshold)
    
    # ì ìˆ˜ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨ ìƒì„±
    normal_scores = [score for gt, score in zip(all_ground_truth, all_scores) if gt == 0]
    anomaly_scores = [score for gt, score in zip(all_ground_truth, all_scores) if gt == 1]
    plot_score_distributions(normal_scores, anomaly_scores, analysis_dir, experiment_name)
        
    # ì‹¤í—˜ ìš”ì•½ ì €ì¥
    save_experiment_summary({}, {"auroc": auroc}, analysis_dir)
    
    logger.info(f"í†µí•© í‰ê°€ ì™„ë£Œ: AUROC={auroc:.4f}, F1={f1:.4f}, ìƒ˜í”Œìˆ˜={len(all_image_paths)}")
    
    return unified_metrics


def extract_scores_from_model_output(model_output, batch_size, batch_idx, model_type):
    """
    ëª¨ë¸ë³„ ì¶œë ¥ì—ì„œ ì ìˆ˜ë“¤ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    Args:
        model_output: ëª¨ë¸ ì¶œë ¥ ê°ì²´
        batch_size: ë°°ì¹˜ í¬ê¸°
        batch_idx: ë°°ì¹˜ ì¸ë±ìŠ¤
        model_type: ëª¨ë¸ íƒ€ì… (ì†Œë¬¸ì)
        
    Returns:
        tuple: (anomaly_scores,)
    """
    import numpy as np
    
    model_type = model_type.lower()
    
    if model_type == "draem":
        # DRAEM: pred_scoreë§Œ ìˆìŒ
        if hasattr(model_output, 'pred_score'):
            final_scores = model_output.pred_score.cpu().numpy()
            
            # NaN ê°’ í™•ì¸ ë° ì²˜ë¦¬
            if np.isnan(final_scores).any():
                print(f"      âš ï¸  DRAEM pred_scoreì— NaN ë°œê²¬, 0.0ìœ¼ë¡œ ëŒ€ì²´")
                final_scores = np.nan_to_num(final_scores, nan=0.0)
            
            # ì—†ëŠ” ê°’ì€ 0 ìœ¼ë¡œ ì²˜ë¦¬ (DRAEMì—ëŠ” mask_score, severity_score, raw_severity_score, normalized_severity_score ì—†ìŒ)
            print(f"      ğŸ“Š DRAEM ì ìˆ˜ ì¶”ì¶œ: pred_score={final_scores[0]:.4f}")
        elif hasattr(model_output, 'anomaly_map'):
            # anomaly_mapì—ì„œ ì ìˆ˜ ê³„ì‚°
            anomaly_map = model_output.anomaly_map.cpu().numpy()
            final_scores = [float(np.max(am)) if am.size > 0 else 0.0 for am in anomaly_map]
            print(f"      ğŸ“Š DRAEM ì ìˆ˜ ì¶”ì¶œ (anomaly_map): max={final_scores[0]:.4f}")
        else:
            raise AttributeError("DRAEM ì¶œë ¥ ì†ì„± ì—†ìŒ")

    elif model_type == "draem_cutpaste_clf":
        # draem_cutpaste_clfì˜ ê²½ìš° pred_scoreì™€ anomaly_mapì´ ëª¨ë‘ ì¡´ì¬
        try:
            final_scores = model_output.pred_score.cpu().numpy()

            # NaN ê°’ í™•ì¸ ë° ì²˜ë¦¬
            if np.isnan(final_scores).any():
                raise ValueError(f"      âŒ DRAEM CutPaste Clf pred_scoreì— NaNì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. (ë°°ì¹˜ {batch_idx})")

            print(f"      ğŸ“Š DRAEM CutPaste Clf ì ìˆ˜ ì¶”ì¶œ: first pred_score={final_scores[0]:.4f}")
        except AttributeError:
            raise AttributeError("DRAEM CutPaste Clf ì¶œë ¥ ì†ì„± ì—†ìŒ")

    elif model_type == "patchcore":
        # PatchCore: pred_scoreë§Œ ìˆìŒ
        if hasattr(model_output, 'pred_score'):
            final_scores = model_output.pred_score.cpu().numpy()
            print(f"      ğŸ“Š PatchCore ì ìˆ˜ ì¶”ì¶œ: pred_score={final_scores[0]:.4f}")
        elif hasattr(model_output, 'anomaly_map'):
            # anomaly_mapì—ì„œ ì ìˆ˜ ê³„ì‚°
            anomaly_map = model_output.anomaly_map.cpu().numpy()
            final_scores = [float(np.max(am)) if am.size > 0 else 0.0 for am in anomaly_map]
            print(f"      ğŸ“Š PatchCore ì ìˆ˜ ì¶”ì¶œ (anomaly_map): max={final_scores[0]:.4f}")
        else:
            raise AttributeError("PatchCore ì¶œë ¥ ì†ì„± ì—†ìŒ")
            
    elif model_type == "dinomaly":
        # Dinomaly: pred_score ë˜ëŠ” anomaly_map
        if hasattr(model_output, 'pred_score'):
            final_scores = model_output.pred_score.cpu().numpy()
            print(f"      ğŸ“Š Dinomaly ì ìˆ˜ ì¶”ì¶œ: pred_score={final_scores[0]:.4f}")
        elif hasattr(model_output, 'anomaly_map'):
            # anomaly_mapì—ì„œ ì ìˆ˜ ê³„ì‚°
            anomaly_map = model_output.anomaly_map.cpu().numpy()
            final_scores = [float(np.max(am)) if am.size > 0 else 0.0 for am in anomaly_map]
            print(f"      ğŸ“Š Dinomaly ì ìˆ˜ ì¶”ì¶œ (anomaly_map): max={final_scores[0]:.4f}")
        else:
            raise AttributeError("Dinomaly ì¶œë ¥ ì†ì„± ì—†ìŒ")
            
    elif model_type.lower() == "fastflow":
        # FastFlow ëª¨ë¸ ì²˜ë¦¬
        print(f"      ğŸŒŠ FastFlow ì ìˆ˜ ì¶”ì¶œ")
        
        # pred_scoreê°€ ìˆì§€ë§Œ ëª¨ë‘ 0ì¸ ê²½ìš° anomaly_mapì„ ì‚¬ìš©
        if hasattr(model_output, 'pred_score'):
            pred_scores = model_output.pred_score.cpu().numpy()
            print(f"      ğŸ“Š FastFlow pred_score: min={np.min(pred_scores):.4f}, max={np.max(pred_scores):.4f}")
            
            # pred_scoreê°€ ëª¨ë‘ 0ì´ë©´ anomaly_mapìœ¼ë¡œ ëŒ€ì²´
            if np.max(pred_scores) == 0.0 and hasattr(model_output, 'anomaly_map'):
                print(f"      âš ï¸ pred_scoreê°€ ëª¨ë‘ 0ì´ë¯€ë¡œ anomaly_map ì‚¬ìš©")
                anomaly_map = model_output.anomaly_map.cpu().numpy()
                final_scores = np.array([float(np.mean(am)) if am.size > 0 else 0.0 for am in anomaly_map])
                print(f"      ğŸ“Š FastFlow ì ìˆ˜ ì¶”ì¶œ (anomaly_map mean): min={np.min(final_scores):.4f}, max={np.max(final_scores):.4f}")
            else:
                final_scores = pred_scores
                
        elif hasattr(model_output, 'anomaly_map'):
            # anomaly_mapì—ì„œ ì ìˆ˜ ê³„ì‚° (fallback)
            anomaly_map = model_output.anomaly_map.cpu().numpy()
            final_scores = np.array([float(np.mean(am)) if am.size > 0 else 0.0 for am in anomaly_map])
            print(f"      ğŸ“Š FastFlow ì ìˆ˜ ì¶”ì¶œ (anomaly_map only): min={np.min(final_scores):.4f}, max={np.max(final_scores):.4f}")
        else:
            raise AttributeError("FastFlow ì¶œë ¥ ì†ì„± ì—†ìŒ")
            
    elif model_type == "draem_cutpaste":
        # DRAEM CutPaste: pred_score ì‚¬ìš©
        if hasattr(model_output, 'pred_score'):
            final_scores = model_output.pred_score.cpu().numpy()
            
            # NaN ê°’ í™•ì¸ ë° ì²˜ë¦¬
            if np.isnan(final_scores).any():
                print(f"      âš ï¸  DRAEM CutPaste pred_scoreì— NaN ë°œê²¬, 0.0ìœ¼ë¡œ ëŒ€ì²´")
                final_scores = np.nan_to_num(final_scores, nan=0.0)
            
            print(f"      ğŸ“Š DRAEM CutPaste ì ìˆ˜ ì¶”ì¶œ: pred_score={final_scores[0]:.4f}")
        elif hasattr(model_output, 'anomaly_map'):
            # anomaly_mapì—ì„œ ì ìˆ˜ ê³„ì‚°
            anomaly_map = model_output.anomaly_map.cpu().numpy()
            final_scores = [float(np.max(am)) if am.size > 0 else 0.0 for am in anomaly_map]
            print(f"      ğŸ“Š DRAEM CutPaste ì ìˆ˜ ì¶”ì¶œ (anomaly_map): max={final_scores[0]:.4f}")
        else:
            raise AttributeError("DRAEM CutPaste ì¶œë ¥ ì†ì„± ì—†ìŒ")
            
    else:
        # ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸ íƒ€ì…: ì¼ë°˜ì ì¸ ì†ì„±ìœ¼ë¡œ ì‹œë„
        print(f"   âš ï¸ Unknown model type: {model_type}, trying generic attributes")
        if hasattr(model_output, 'pred_score'):
            final_scores = model_output.pred_score.cpu().numpy()
        elif hasattr(model_output, 'final_score'):
            final_scores = model_output.final_score.cpu().numpy()
        elif hasattr(model_output, 'anomaly_map'):
            anomaly_map = model_output.anomaly_map.cpu().numpy()
            final_scores = np.array([float(np.max(am)) if am.size > 0 else 0.0 for am in anomaly_map])
        else:
            raise AttributeError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ëª¨ë¸ ì¶œë ¥ í˜•ì‹: {type(model_output)}")
            
        print(f"      ğŸ“Š ì¼ë°˜ ëª¨ë¸ ì ìˆ˜ ì¶”ì¶œ: anomaly_score={final_scores[0]:.4f}")
        
    return final_scores


def create_anomaly_heatmap_with_colorbar(
    anomaly_map_array,
    cmap='viridis',
    show_colorbar=False,
    fixed_range=True
):
    """Anomaly heatmap ìƒì„± (colorbar ì˜µì…˜)

    Args:
        anomaly_map_array: numpy array [H, W]
                          range: [0, 1] if fixed_range=True, ì•„ë‹ˆë©´ data-dependent
                          meaning: í”½ì…€ë³„ ì´ìƒ í™•ë¥  (0=ì •ìƒ, 1=ì´ìƒ)
        cmap: matplotlib colormap ì´ë¦„ (ê¸°ë³¸ê°’: 'viridis')
              - 'viridis': íŒŒë€ìƒ‰->ì´ˆë¡ìƒ‰->ë…¸ë€ìƒ‰ (ê¶Œì¥)
              - 'jet': íŒŒë€ìƒ‰->ì²­ë¡->ë…¸ë‘->ë¹¨ê°• (ê¸°ì¡´)
              - 'hot': ê²€ì •->ë¹¨ê°•->ë…¸ë‘->í°ìƒ‰
              - 'plasma': ë³´ë¼->ë¶„í™->ë…¸ë‘
              - 'inferno': ê²€ì •->ë³´ë¼->ë¹¨ê°•->ë…¸ë‘
              - 'turbo': íŒŒë€ìƒ‰->ì²­ë¡->ì´ˆë¡->ë…¸ë‘->ë¹¨ê°•
              - 'coolwarm': íŒŒë€ìƒ‰->í°ìƒ‰->ë¹¨ê°•
        show_colorbar: colorbar í‘œì‹œ ì—¬ë¶€ (ê¸°ë³¸ê°’: False)
        fixed_range: colorbar rangeë¥¼ 0~1ë¡œ ê³ ì •í• ì§€ ì—¬ë¶€ (ê¸°ë³¸ê°’: True)

    Returns:
        PIL.Image: heatmap ì´ë¯¸ì§€ (colorbar í¬í•¨/ì œì™¸)
    """
    import matplotlib.pyplot as plt
    import matplotlib.cm as cm
    from matplotlib.colors import Normalize
    import io
    from PIL import Image
    import numpy as np

    # ê°’ ë²”ìœ„ ì„¤ì •
    if fixed_range:
        vmin, vmax = 0.0, 1.0
    else:
        vmin = anomaly_map_array.min()
        vmax = anomaly_map_array.max()

    # Figure í¬ê¸° ë° ë ˆì´ì•„ì›ƒ ì„¤ì •
    if show_colorbar:
        # Colorbar í¬í•¨ ë ˆì´ì•„ì›ƒ
        fig_width = 6
        fig_height = 4
        fig, (ax_img, ax_cbar) = plt.subplots(1, 2, figsize=(fig_width, fig_height),
                                              gridspec_kw={'width_ratios': [4, 0.3]})
    else:
        # Colorbar ì—†ëŠ” ë ˆì´ì•„ì›ƒ
        fig_width = 4
        fig_height = 4
        fig, ax_img = plt.subplots(1, 1, figsize=(fig_width, fig_height))

    # Heatmap ìƒì„±
    norm = Normalize(vmin=vmin, vmax=vmax)
    im = ax_img.imshow(anomaly_map_array, cmap=cmap, norm=norm, aspect='auto')
    ax_img.axis('off')

    # Colorbar ìƒì„± (ì˜µì…˜ì— ë”°ë¼)
    if show_colorbar:
        cbar = plt.colorbar(im, cax=ax_cbar)
        cbar.set_label('Anomaly Score', rotation=270, labelpad=15, fontsize=9)
        cbar.ax.tick_params(labelsize=8)

    plt.tight_layout()

    # PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
    buf = io.BytesIO()
    plt.savefig(buf, format='png', dpi=100, bbox_inches='tight', pad_inches=0.05)
    buf.seek(0)
    heatmap_pil = Image.open(buf).convert('RGB')
    plt.close()

    return heatmap_pil


def create_batch_visualizations(image_tensor, model_output, image_paths, visualization_dir, batch_idx, model=None):
    """ë°°ì¹˜ì— ëŒ€í•œ ì‹œê°í™” ìƒì„±

    - DRAEM ê³„ì—´ (DRAEM, DRAEM CutPaste Clf): original, recon, disc_anomaly, residual, overlay_auto, overlay_fixed (6ê°œ)
    - ê¸°íƒ€ ëª¨ë¸: original, overlay_auto, overlay_fixed (3ê°œ)

    Args:
        image_tensor: ì…ë ¥ ì´ë¯¸ì§€ í…ì„œ [B, C, H, W] - range: [0, 1] normalized
        model_output: ëª¨ë¸ ì¶œë ¥ ê°ì²´ (InferenceBatch containing anomaly_map, pred_score, etc.)
        image_paths: ì´ë¯¸ì§€ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        visualization_dir: ì‹œê°í™” ì €ì¥ ë””ë ‰í„°ë¦¬
        batch_idx: ë°°ì¹˜ ì¸ë±ìŠ¤
        model: ëª¨ë¸ ê°ì²´ (DRAEM ê³„ì—´ ëª¨ë¸ì¸ ê²½ìš° reconstruction ê³„ì‚°ìš©)
    """
    import numpy as np
    from PIL import Image
    from pathlib import Path
    import torch

    # anomalib ì‹œê°í™” í•¨ìˆ˜ë“¤ import
    from anomalib.visualization.image.functional import (
        overlay_images,
        create_image_grid,
        add_text_to_image
    )

    # ë°°ì¹˜ì—ì„œ anomaly map ë° mask ì¶”ì¶œ
    anomaly_maps = None
    masks = None
    if hasattr(model_output, 'anomaly_map'):
        anomaly_maps = model_output.anomaly_map
    else:
        return

    # Mask ì¶”ì¶œ (ìˆëŠ” ê²½ìš°) - pred_mask ë˜ëŠ” gt_mask
    if hasattr(model_output, 'pred_mask') and model_output.pred_mask is not None:
        masks = model_output.pred_mask
    elif hasattr(model_output, 'gt_mask') and model_output.gt_mask is not None:
        masks = model_output.gt_mask

    # DRAEM ê³„ì—´ ëª¨ë¸ì¸ì§€ í™•ì¸ (DRAEM, DRAEM CutPaste Clf)
    is_draem = (model is not None and
                hasattr(model, 'reconstructive_subnetwork') and
                hasattr(model, 'discriminative_subnetwork'))

    # DRAEM ê³„ì—´ ëª¨ë¸ì¸ ê²½ìš° reconstructionê³¼ discriminative anomaly ê³„ì‚°
    recon_batch = None
    disc_anomaly_batch = None
    if is_draem:
        with torch.no_grad():
            # ëª¨ë¸ì˜ ì…ë ¥ ì±„ë„ ìˆ˜ í™•ì¸
            # DRAEM CutPaste Clf: 1ì±„ë„, ì›ë³¸ DRAEM: 3ì±„ë„
            # encoder.block1[0]ì´ ì²« ë²ˆì§¸ Conv2d ë ˆì´ì–´
            model_input_channels = model.reconstructive_subnetwork.encoder.block1[0].in_channels

            # ëª¨ë¸ íƒ€ì… í™•ì¸
            model_name = "DRAEM CutPaste Clf" if hasattr(model, 'severity_head') else "DRAEM"
            print(f"   ğŸ“Š Model Type: {model_name}")
            print(f"   ğŸ”§ Model Input Channels: {model_input_channels}")
            print(f"   ğŸ“· Image Tensor Shape: {image_tensor.shape}")

            # ëª¨ë¸ ì…ë ¥ ì±„ë„ ìˆ˜ì— ë§ê²Œ ì´ë¯¸ì§€ ì¤€ë¹„
            if model_input_channels == 1:
                # 1ì±„ë„ ëª¨ë¸: ì²« ë²ˆì§¸ ì±„ë„ë§Œ ì‚¬ìš©
                batch_input = image_tensor[:, :1, :, :]
                print(f"   âœ‚ï¸  Using 1-channel mode: {batch_input.shape}")
            else:
                # 3ì±„ë„ ëª¨ë¸: ì „ì²´ ì‚¬ìš©
                batch_input = image_tensor
                print(f"   ğŸ¨ Using 3-channel mode: {batch_input.shape}")

            # Reconstruction ê³„ì‚° (raw ê°’ ì§ì ‘ ì‚¬ìš©, sigmoid ì œê±°)
            recon_batch = model.reconstructive_subnetwork(batch_input)

            # ğŸ” DEBUG: reconstruction ê°’ ë²”ìœ„ ì¶œë ¥
            print(f"      ğŸ” Reconstruction stats:")
            print(f"         - min={recon_batch.min():.4f}, max={recon_batch.max():.4f}, mean={recon_batch.mean():.4f}, std={recon_batch.std():.4f}")
            print(f"      ğŸ” Input stats:")
            print(f"         - min={batch_input.min():.4f}, max={batch_input.max():.4f}, mean={batch_input.mean():.4f}, std={batch_input.std():.4f}")

            # Discriminative network ê³„ì‚° (anomaly channel ì¶”ì¶œ)
            # Follow DRAEM convention: [original, reconstruction]
            joined_input = torch.cat([batch_input, recon_batch], dim=1)
            print(f"   ğŸ”— Concat order: [original({batch_input.shape[1]}ch), recon({recon_batch.shape[1]}ch)] -> {joined_input.shape}")
            disc_output = model.discriminative_subnetwork(joined_input)

            # Softmax ì ìš©í•˜ì—¬ anomaly channel (channel 1) ì¶”ì¶œ
            # disc_output shape: [B, 2, H, W] -> [B, 1, H, W] (anomaly channelë§Œ)
            disc_anomaly_batch = torch.softmax(disc_output, dim=1)[:, 1:2, :, :]

            # ğŸ” DEBUG: discriminative anomaly ê°’ ë²”ìœ„ ì¶œë ¥
            print(f"      ğŸ” Discriminative Anomaly stats:")
            print(f"         - min={disc_anomaly_batch.min():.4f}, max={disc_anomaly_batch.max():.4f}, mean={disc_anomaly_batch.mean():.4f}, std={disc_anomaly_batch.std():.4f}")

    # ë°°ì¹˜ í¬ê¸°
    batch_size = image_tensor.shape[0]

    # ê° ì´ë¯¸ì§€ì— ëŒ€í•´ ì‹œê°í™” ìƒì„±
    for i in range(batch_size):  # ì „ì²´ ë°°ì¹˜ ì‹œê°í™”
        try:
            # ì›ë³¸ ì´ë¯¸ì§€ ì¶”ì¶œ ë° ë³€í™˜
            original_img_tensor = image_tensor[i]  # [C, H, W]

            # Min-max normalizationìœ¼ë¡œ [0, 1] ë²”ìœ„ë¡œ ë³€í™˜
            original_np = original_img_tensor.permute(1, 2, 0).cpu().numpy()  # [H, W, C]
            original_min = original_np.min()
            original_max = original_np.max()
            if original_max > original_min:
                original_normalized = (original_np - original_min) / (original_max - original_min)
            else:
                original_normalized = np.zeros_like(original_np)
            original_img_array = (original_normalized * 255).astype(np.uint8)

            # print(f"      ğŸ” Original normalization: [{original_min:.4f}, {original_max:.4f}] -> [0, 1]")

            # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ì¸ ê²½ìš° RGBë¡œ ë³€í™˜
            if original_img_array.shape[2] == 1:
                original_img_array = np.repeat(original_img_array, 3, axis=2)
            elif original_img_array.shape[2] > 3:
                original_img_array = original_img_array[:, :, :3]

            original_img_pil = Image.fromarray(original_img_array, mode='RGB')

            # Anomaly map ì¶”ì¶œ ë° ë³€í™˜
            anomaly_map_tensor = anomaly_maps[i]  # [H, W] ë˜ëŠ” [1, H, W]
            # range: [0, 1] (ì´ë¯¸ softmaxê°€ ì ìš©ëœ anomaly probability)
            # meaning: í”½ì…€ë³„ ì´ìƒ í™•ë¥  (0=ì •ìƒ, 1=ì´ìƒ)
            if len(anomaly_map_tensor.shape) == 3:
                anomaly_map_tensor = anomaly_map_tensor.squeeze(0)  # [H, W]

            # Mask ì¶”ì¶œ (boundary í‘œì‹œìš©)
            mask_for_boundary = None
            if masks is not None:
                mask_tensor = masks[i]  # [H, W] ë˜ëŠ” [1, H, W]
                if len(mask_tensor.shape) == 3:
                    mask_tensor = mask_tensor.squeeze(0)  # [H, W]
                mask_for_boundary = mask_tensor.cpu().numpy()

            # Auto-range anomaly map ì‹œê°í™” (colorbar=False)
            anomaly_map_vis_auto = create_anomaly_heatmap_with_colorbar(
                anomaly_map_tensor.cpu().numpy(),
                cmap='jet',
                show_colorbar=False,
                fixed_range=False  # Auto-range
            )

            # Fixed-range (0~1) anomaly map ì‹œê°í™” (colorbar=False)
            anomaly_map_vis_fixed = create_anomaly_heatmap_with_colorbar(
                anomaly_map_tensor.cpu().numpy(),
                cmap='jet',
                show_colorbar=False,
                fixed_range=True  # 0~1 ê³ ì •
            )

            # ì˜¤ë²„ë ˆì´ ìƒì„± (ì›ë³¸ + anomaly map, auto-range)
            overlay_img_auto = overlay_images(
                base=original_img_pil,
                overlays=anomaly_map_vis_auto,
                alpha=0.5
            )

            # ì˜¤ë²„ë ˆì´ ìƒì„± (ì›ë³¸ + anomaly map, fixed-range)
            overlay_img_fixed = overlay_images(
                base=original_img_pil,
                overlays=anomaly_map_vis_fixed,
                alpha=0.5
            )

            # DRAEM ê³„ì—´ ëª¨ë¸ì¸ ê²½ìš° 5ê°œ ì´ë¯¸ì§€ ì‹œê°í™”
            if is_draem and recon_batch is not None:
                # Reconstruction ì´ë¯¸ì§€ ìƒì„±
                recon_tensor = recon_batch[i]  # [C, H, W] - CëŠ” 1 ë˜ëŠ” 3
                # range: unbounded (raw network output)
                # meaning: ì¬êµ¬ì„±ëœ ì´ë¯¸ì§€ (í•™ìŠµ í›„ ~[0,1]ì— ìˆ˜ë ´)

                # Min-max normalizationìœ¼ë¡œ [0, 1] ë²”ìœ„ë¡œ ë³€í™˜
                recon_np = recon_tensor.permute(1, 2, 0).cpu().numpy()  # [H, W, C]
                recon_min = recon_np.min()
                recon_max = recon_np.max()
                if recon_max > recon_min:
                    recon_normalized = (recon_np - recon_min) / (recon_max - recon_min)
                else:
                    recon_normalized = np.zeros_like(recon_np)
                recon_array = (recon_normalized * 255).astype(np.uint8)

                print(f"      ğŸ” Recon normalization: [{recon_min:.4f}, {recon_max:.4f}] -> [0, 1]")

                # ì±„ë„ ìˆ˜ì— ë”°ë¼ ì²˜ë¦¬
                if recon_array.shape[2] == 1:
                    recon_array = np.repeat(recon_array, 3, axis=2)  # 1ì±„ë„ â†’ RGB
                elif recon_array.shape[2] == 3:
                    pass  # ì´ë¯¸ 3ì±„ë„, ê·¸ëŒ€ë¡œ ì‚¬ìš©
                else:
                    recon_array = recon_array[:, :, :3]  # 3ì±„ë„ ì´ˆê³¼ì‹œ ì• 3ê°œë§Œ
                recon_img_pil = Image.fromarray(recon_array, mode='RGB')

                # Discriminative Anomaly ì´ë¯¸ì§€ ìƒì„±
                disc_anomaly_tensor = disc_anomaly_batch[i]  # [1, H, W] - softmaxëœ anomaly channel
                # range: [0, 1] (ì´ë¯¸ softmaxê°€ ì ìš©ëœ anomaly probability)
                # meaning: discriminative networkì˜ í”½ì…€ë³„ ì´ìƒ í™•ë¥ 

                # Softmax ì¶œë ¥ì€ [0, 1] ë²”ìœ„
                disc_anomaly_array = (disc_anomaly_tensor.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)
                disc_anomaly_array = np.repeat(disc_anomaly_array, 3, axis=2)  # 1ì±„ë„ â†’ RGB
                disc_anomaly_img_pil = Image.fromarray(disc_anomaly_array, mode='RGB')

                # Residual ì´ë¯¸ì§€ ìƒì„± (ì›ë³¸ - ì¬êµ¬ì„±)
                # ì›ë³¸ê³¼ ì¬êµ¬ì„± ì´ë¯¸ì§€ë¥¼ ê°™ì€ ì •ê·œí™” ë²”ìœ„ë¡œ ë§ì¶¤
                original_for_residual = original_img_tensor.permute(1, 2, 0).cpu().numpy()  # [H, W, C]
                recon_for_residual = recon_tensor.permute(1, 2, 0).cpu().numpy()  # [H, W, C]
                
                # ì±„ë„ ìˆ˜ ë§ì¶¤ (ì›ë³¸ì´ 3ì±„ë„, ì¬êµ¬ì„±ì´ 1ì±„ë„ì¸ ê²½ìš°)
                if original_for_residual.shape[2] == 3 and recon_for_residual.shape[2] == 1:
                    # ì›ë³¸ì˜ ì²« ë²ˆì§¸ ì±„ë„ë§Œ ì‚¬ìš©
                    original_for_residual = original_for_residual[:, :, :1]
                elif original_for_residual.shape[2] == 1 and recon_for_residual.shape[2] == 3:
                    # ì¬êµ¬ì„±ì˜ ì²« ë²ˆì§¸ ì±„ë„ë§Œ ì‚¬ìš©
                    recon_for_residual = recon_for_residual[:, :, :1]
                
                # Residual ê³„ì‚° (ì°¨ì´ì˜ ì ˆëŒ“ê°’)
                residual_np = np.abs(original_for_residual - recon_for_residual)
                
                # Min-max normalization
                residual_min = residual_np.min()
                residual_max = residual_np.max()
                if residual_max > residual_min:
                    residual_normalized = (residual_np - residual_min) / (residual_max - residual_min)
                else:
                    residual_normalized = np.zeros_like(residual_np)
                
                residual_array = (residual_normalized * 255).astype(np.uint8)
                
                # 1ì±„ë„ì„ RGBë¡œ ë³€í™˜
                if residual_array.shape[2] == 1:
                    residual_array = np.repeat(residual_array, 3, axis=2)
                    
                residual_img_pil = Image.fromarray(residual_array, mode='RGB')
                
                print(f"      ğŸ” Residual stats: min={residual_min:.4f}, max={residual_max:.4f}")

                # DRAEMìš© ì˜¤ë²„ë ˆì´ ìƒì„±
                # 5ë²ˆì§¸: Auto-range, colorbar ì—†ìŒ
                anomaly_map_vis_draem_auto = create_anomaly_heatmap_with_colorbar(
                    anomaly_map_tensor.cpu().numpy(),
                    cmap='jet',
                    show_colorbar=False,
                    fixed_range=False  # Auto-range
                )
                overlay_img_draem_auto = overlay_images(
                    base=original_img_pil,
                    overlays=anomaly_map_vis_draem_auto,
                    alpha=0.5
                )

                # 6ë²ˆì§¸: Fixed 0-1, colorbar ìˆìŒ
                anomaly_map_vis_draem_fixed = create_anomaly_heatmap_with_colorbar(
                    anomaly_map_tensor.cpu().numpy(),
                    cmap='jet',
                    show_colorbar=True,
                    fixed_range=True  # Fixed 0-1
                )
                overlay_img_draem_fixed = overlay_images(
                    base=original_img_pil,
                    overlays=anomaly_map_vis_draem_fixed,
                    alpha=0.5
                )

                # í…ìŠ¤íŠ¸ ì¶”ê°€
                original_with_text = add_text_to_image(
                    original_img_pil.copy(),
                    "Original",
                    font=None, size=10, color="white", background=(0, 0, 0, 128)
                )

                recon_with_text = add_text_to_image(
                    recon_img_pil.copy(),
                    "Reconstruction",
                    font=None, size=10, color="white", background=(0, 0, 0, 128)
                )

                disc_anomaly_with_text = add_text_to_image(
                    disc_anomaly_img_pil.copy(),
                    "Disc Anomaly",
                    font=None, size=10, color="white", background=(0, 0, 0, 128)
                )

                residual_with_text = add_text_to_image(
                    residual_img_pil.copy(),
                    "Residual (|Orig-Recon|)",
                    font=None, size=10, color="white", background=(0, 0, 0, 128)
                )

                overlay_auto_with_text = add_text_to_image(
                    overlay_img_draem_auto.copy(),
                    "Original + Anomaly (Auto)",
                    font=None, size=10, color="white", background=(0, 0, 0, 128)
                )

                overlay_fixed_with_text = add_text_to_image(
                    overlay_img_draem_fixed.copy(),
                    "Original + Anomaly (Fixed 0-1, colorbar)",
                    font=None, size=10, color="white", background=(0, 0, 0, 128)
                )

                # 6ê°œ ì´ë¯¸ì§€ë¥¼ 3ì—´ ê·¸ë¦¬ë“œë¡œ ë°°ì¹˜ (2í–‰: 3ê°œ + 3ê°œ)
                visualization_grid = create_image_grid(
                    [original_with_text, recon_with_text, disc_anomaly_with_text,
                     residual_with_text, overlay_auto_with_text, overlay_fixed_with_text],
                    nrow=3
                )
            else:
                # ê¸°íƒ€ ëª¨ë¸: 3ê°œ ì´ë¯¸ì§€ ì‹œê°í™”
                original_with_text = add_text_to_image(
                    original_img_pil.copy(),
                    "Original Image",
                    font=None, size=10, color="white", background=(0, 0, 0, 128)
                )

                overlay_auto_with_text = add_text_to_image(
                    overlay_img_auto.copy(),
                    "Original + Anomaly Map (Auto Range)",
                    font=None, size=10, color="white", background=(0, 0, 0, 128)
                )

                overlay_fixed_with_text = add_text_to_image(
                    overlay_img_fixed.copy(),
                    "Original + Anomaly Map (Fixed 0-1)",
                    font=None, size=10, color="white", background=(0, 0, 0, 128)
                )

                # 3ê°œ ì´ë¯¸ì§€ë¥¼ ê°€ë¡œë¡œ ë°°ì¹˜
                visualization_grid = create_image_grid(
                    [original_with_text, overlay_auto_with_text, overlay_fixed_with_text],
                    nrow=3
                )

            # íŒŒì¼ëª… ë° ë””ë ‰í„°ë¦¬ ìƒì„± (ë ˆì´ë¸”ë³„ë¡œ ë¶„ë¥˜)
            if i < len(image_paths):
                image_path = Path(image_paths[i])
                image_filename = image_path.stem

                # ê²½ë¡œì—ì„œ ë ˆì´ë¸” ì¶”ì¶œ (fault ë˜ëŠ” good)
                label = None
                if '/fault/' in str(image_path):
                    label = 'fault'
                elif '/good/' in str(image_path):
                    label = 'good'
                else:
                    label = 'unknown'

                # ë ˆì´ë¸”ë³„ ë””ë ‰í„°ë¦¬ ìƒì„±
                label_dir = visualization_dir / label
                label_dir.mkdir(exist_ok=True)

                # íŒŒì¼ëª…ì€ ì›ë³¸ ì´ë¯¸ì§€ ì´ë¦„ ì‚¬ìš©
                save_filename = f"{image_filename}.png"
                save_path = label_dir / save_filename
            else:
                # ì´ë¯¸ì§€ ê²½ë¡œê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ í˜•ì‹ ì‚¬ìš©
                save_filename = f"batch_{batch_idx:03d}_sample_{i:02d}.png"
                save_path = visualization_dir / save_filename

            # ì´ë¯¸ì§€ ì €ì¥
            visualization_grid.save(save_path)

        except Exception as e:
            print(f"âŒ ìƒ˜í”Œ {i} ì‹œê°í™” ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()


# ===========================
# Multi-Domain Specific Functions
# ===========================

def create_multi_domain_datamodule(
    source_domain: str,
    target_domains: Union[List[str], str],
    dataset_root: str,
    batch_size: int = 32,
    image_size: Tuple[int, int] = (256, 256),
    resize_method: str = "resize",
    num_workers: int = 8,
    seed: int = 42,
    verbose: bool = True
):
    """Multi-Domainìš© MultiDomainHDMAPDataModule ìƒì„± ë° ì„¤ì •.

    Args:
        source_domain: ì†ŒìŠ¤ ë„ë©”ì¸ ì´ë¦„ (ì˜ˆ: "domain_A")
        target_domains: íƒ€ê²Ÿ ë„ë©”ì¸ ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” "auto" (autoë©´ source ì œì™¸ ëª¨ë“  ë„ë©”ì¸)
        dataset_root: ë°ì´í„°ì…‹ ë£¨íŠ¸ ê²½ë¡œ (í•„ìˆ˜)
        batch_size: ë°°ì¹˜ í¬ê¸°
        image_size: ì´ë¯¸ì§€ í¬ê¸° (height, width)
        num_workers: ì›Œì»¤ ìˆ˜
        seed: ëœë¤ ì‹œë“œ
        verbose: ìƒì„¸ ë¡œê·¸ ì¶œë ¥ ì—¬ë¶€

    Returns:
        ì„¤ì •ëœ MultiDomainHDMAPDataModule

    Examples:
        # ìˆ˜ë™ìœ¼ë¡œ íƒ€ê²Ÿ ë„ë©”ì¸ ì§€ì •
        datamodule = create_multi_domain_datamodule(
            source_domain="domain_A",
            target_domains=["domain_B", "domain_C"],
            dataset_root="/path/to/dataset",
            batch_size=32
        )

        # ìë™ìœ¼ë¡œ íƒ€ê²Ÿ ë„ë©”ì¸ ì„¤ì • (source ì œì™¸í•œ ëª¨ë“  ë„ë©”ì¸)
        datamodule = create_multi_domain_datamodule(
            source_domain="domain_A",
            target_domains="auto",
            dataset_root="/path/to/dataset"
        )
    """
    from anomalib.data.datamodules.image.multi_domain_hdmap import MultiDomainHDMAPDataModule
    from pathlib import Path

    # Path ê°ì²´ë¡œ ë³€í™˜ ë° ê²€ì¦
    dataset_root = Path(dataset_root).resolve()

    if not dataset_root.exists():
        raise FileNotFoundError(f"ë°ì´í„°ì…‹ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {dataset_root}")

    # target_domains ì²˜ë¦¬
    if target_domains == "auto":
        all_domains = ["domain_A", "domain_B", "domain_C", "domain_D"]
        target_domains_list = [d for d in all_domains if d != source_domain]
    else:
        target_domains_list = target_domains

    if verbose:
        print(f"\nğŸ“¦ MultiDomainHDMAPDataModule ìƒì„± ì¤‘...")
        print(f"   ğŸŒ ì†ŒìŠ¤ ë„ë©”ì¸: {source_domain}")
        print(f"   ğŸ¯ íƒ€ê²Ÿ ë„ë©”ì¸: {target_domains_list}")
        print(f"   ğŸ“ ë°ì´í„°ì…‹ ê²½ë¡œ: {dataset_root}")
        print(f"   ğŸ“ ì´ë¯¸ì§€ í¬ê¸°: {image_size[0]}x{image_size[1]}")
        print(f"   ğŸ“Š ë°°ì¹˜ í¬ê¸°: {batch_size}")
        print(f"   ğŸ‘· ì›Œì»¤ ìˆ˜: {num_workers}")
        print(f"   ğŸ² ì‹œë“œ: {seed}")

    # MultiDomainHDMAPDataModule ìƒì„±
    datamodule = MultiDomainHDMAPDataModule(
        root=str(dataset_root),
        source_domain=source_domain,
        target_domains=target_domains_list,
        validation_strategy="source_test",  # ê³ ì •ê°’: source testë¥¼ validationìœ¼ë¡œ ì‚¬ìš©
        train_batch_size=batch_size,
        eval_batch_size=batch_size,
        num_workers=num_workers,
        target_size=image_size,
        resize_method=resize_method,
        seed=seed
    )

    # Setup í˜¸ì¶œ
    datamodule.setup()

    if verbose:
        # ë°ì´í„°ì…‹ ì •ë³´ ì¶œë ¥
        print(f"\nâœ… MultiDomainHDMAPDataModule ì„¤ì • ì™„ë£Œ:")
        print(f"   - Train ìƒ˜í”Œ ìˆ˜: {len(datamodule.train_data)}")
        print(f"   - Validation ìƒ˜í”Œ ìˆ˜ (source test): {len(datamodule.val_data)}")
        print(f"   - Test ë„ë©”ì¸ ìˆ˜: {len(datamodule.test_data)}")
        for i, target_domain in enumerate(target_domains_list):
            print(f"     â€¢ {target_domain}: {len(datamodule.test_data[i])} ìƒ˜í”Œ")

    return datamodule


def evaluate_source_domain(
    model,
    datamodule,
    visualization_dir: Optional[Path] = None,
    model_type: str = "unknown",
    max_visualization_batches: int = 5,
    verbose: bool = True,
    analysis_dir: Optional[Path] = None
):
    """ì†ŒìŠ¤ ë„ë©”ì¸ì—ì„œ ëª¨ë¸ í‰ê°€ (validation ì—­í• ).

    Source domainì˜ test ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ validation ìˆ˜í–‰.
    ì„ íƒì ìœ¼ë¡œ ì‹œê°í™”ë„ ìƒì„±.

    Args:
        model: í•™ìŠµëœ ëª¨ë¸
        datamodule: MultiDomainHDMAPDataModule
        visualization_dir: ì‹œê°í™” ì €ì¥ ë””ë ‰í„°ë¦¬ (Noneì´ë©´ ì‹œê°í™” ì•ˆí•¨)
        model_type: ëª¨ë¸ íƒ€ì… (ì ìˆ˜ ì¶”ì¶œì— ì‚¬ìš©)
        max_visualization_batches: ì‹œê°í™”í•  ìµœëŒ€ ë°°ì¹˜ ìˆ˜ (-1ì´ë©´ ì „ì²´ ì‹œê°í™”)
        verbose: ìƒì„¸ ì¶œë ¥ ì—¬ë¶€
        analysis_dir: ë¶„ì„ ê²°ê³¼ ì €ì¥ ë””ë ‰í„°ë¦¬ (Noneì´ë©´ ë¶„ì„ ì•ˆí•¨)

    Returns:
        dict: í‰ê°€ ê²°ê³¼
            - domain: ì†ŒìŠ¤ ë„ë©”ì¸ ì´ë¦„
            - auroc: AUROC ì ìˆ˜
            - metrics: ê¸°íƒ€ ë©”íŠ¸ë¦­ (accuracy, precision, recall, f1)
            - num_samples: í‰ê°€ ìƒ˜í”Œ ìˆ˜
            - visualization_dir: ì‹œê°í™” ë””ë ‰í„°ë¦¬ ê²½ë¡œ

    Example:
        >>> source_results = evaluate_source_domain(
        ...     model=trained_model,
        ...     datamodule=datamodule,
        ...     visualization_dir=Path("./results/viz/source"),
        ...     model_type="draem"
        ... )
    """
    import torch
    import numpy as np
    from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score
    from pathlib import Path

    if verbose:
        print(f"\nğŸ“Š ì†ŒìŠ¤ ë„ë©”ì¸ ({datamodule.source_domain}) í‰ê°€ ì‹œì‘...")

    # ì‹œê°í™” ë””ë ‰í„°ë¦¬ ìƒì„±
    if visualization_dir:
        visualization_dir = Path(visualization_dir)
        visualization_dir.mkdir(parents=True, exist_ok=True)
        if verbose:
            print(f"   ğŸ“ ì‹œê°í™” ì €ì¥ ê²½ë¡œ: {visualization_dir}")

    # Validation dataloader (source domain test)
    val_loader = datamodule.val_dataloader()

    # í‰ê°€ ëª¨ë“œë¡œ ì „í™˜í•˜ê³  GPUë¡œ ì´ë™
    model.eval()
    if torch.cuda.is_available():
        model = model.cuda()

    all_scores = []
    all_labels = []
    all_image_paths = []

    with torch.no_grad():
        for batch_idx, batch in enumerate(val_loader):
            # GPUë¡œ ì´ë™ (ê°€ëŠ¥í•œ ê²½ìš°)
            if torch.cuda.is_available():
                image_tensor = batch.image.cuda()
                if hasattr(batch, 'mask'):
                    mask_tensor = batch.mask.cuda()
            else:
                image_tensor = batch.image

            # ëª¨ë¸ ì˜ˆì¸¡
            outputs = model(image_tensor)

            # ì ìˆ˜ ì¶”ì¶œ (ëª¨ë¸ë³„ë¡œ ë‹¤ë¦„)
            scores = extract_scores_from_model_output(
                outputs,
                len(image_tensor),
                batch_idx,
                model_type
            )
            all_scores.extend(scores)

            # ì´ë¯¸ì§€ ê²½ë¡œ ì¶”ì¶œ
            if hasattr(batch, 'image_path'):
                image_paths = batch.image_path
                if not isinstance(image_paths, list):
                    image_paths = [image_paths]
                all_image_paths.extend(image_paths)

                # Ground truth ì¶”ì¶œ (ì´ë¯¸ì§€ ê²½ë¡œì—ì„œ)
                gt_labels = []
                for path in image_paths:
                    if '/fault/' in path:
                        gt_labels.append(1)  # anomaly
                    elif '/good/' in path:
                        gt_labels.append(0)  # normal
                    else:
                        gt_labels.append(0)  # ê¸°ë³¸ê°’
                all_labels.extend(gt_labels)

            # ì‹œê°í™” ìƒì„± (max_visualization_batches=-1ì´ë©´ ì „ì²´, ì•„ë‹ˆë©´ ì§€ì •ëœ ë°°ì¹˜ ìˆ˜ë§Œ)
            should_visualize = (max_visualization_batches == -1 or batch_idx < max_visualization_batches)
            if visualization_dir and should_visualize:
                # create_batch_visualizationsê°€ ë‚´ë¶€ì ìœ¼ë¡œ fault/good í´ë”ë¥¼ ìƒì„±í•¨
                create_batch_visualizations(
                    image_tensor,
                    outputs,
                    image_paths,
                    visualization_dir,  # batch í´ë” ì—†ì´ ì§ì ‘ ì „ë‹¬
                    batch_idx,
                    model=model.model if hasattr(model, 'model') else None
                )

            if verbose and (batch_idx + 1) % 10 == 0:
                print(f"   ì²˜ë¦¬ ì¤‘: {batch_idx + 1}/{len(val_loader)} ë°°ì¹˜")

    # ë©”íŠ¸ë¦­ ê³„ì‚°
    all_scores = np.array(all_scores)
    all_labels = np.array(all_labels)

    # NaN ì²˜ë¦¬
    nan_mask = np.isnan(all_scores)
    if nan_mask.any():
        if verbose:
            print(f"   âš ï¸ NaN ì ìˆ˜ {nan_mask.sum()}ê°œ ë°œê²¬, ì œê±° í›„ ê³„ì†")
        valid_mask = ~nan_mask
        all_scores = all_scores[valid_mask]
        all_labels = all_labels[valid_mask]

    # AUROC ê³„ì‚°
    try:
        auroc = roc_auc_score(all_labels, all_scores)
    except Exception as e:
        if verbose:
            print(f"   âš ï¸ AUROC ê³„ì‚° ì‹¤íŒ¨: {e}")
        auroc = 0.0

    # ì´ì§„ ë¶„ë¥˜ ë©”íŠ¸ë¦­ ê³„ì‚° (threshold = 0.5)
    predictions = (all_scores > 0.5).astype(int)
    accuracy = accuracy_score(all_labels, predictions)
    precision = precision_score(all_labels, predictions, zero_division=0)
    recall = recall_score(all_labels, predictions, zero_division=0)
    f1 = f1_score(all_labels, predictions, zero_division=0)

    if verbose:
        print(f"\nâœ… ì†ŒìŠ¤ ë„ë©”ì¸ í‰ê°€ ì™„ë£Œ:")
        print(f"   - Domain: {datamodule.source_domain}")
        print(f"   - AUROC: {auroc:.4f}")
        print(f"   - Accuracy: {accuracy:.4f}")
        print(f"   - Precision: {precision:.4f}")
        print(f"   - Recall: {recall:.4f}")
        print(f"   - F1-Score: {f1:.4f}")
        print(f"   - ìƒ˜í”Œ ìˆ˜: {len(all_scores)}")

    # Analysis ê¸°ëŠ¥ ì¶”ê°€
    if analysis_dir:
        analysis_dir = Path(analysis_dir)
        source_analysis_dir = analysis_dir / f"source_{datamodule.source_domain}"
        source_analysis_dir.mkdir(parents=True, exist_ok=True)
        
        if verbose:
            print(f"   ğŸ’¾ ì†ŒìŠ¤ ë„ë©”ì¸ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì¤‘: {source_analysis_dir}")
        
        # ìƒì„¸ í…ŒìŠ¤íŠ¸ ê²°ê³¼ CSV ì €ì¥
        predictions_dict = {"pred_scores": all_scores}
        ground_truth_dict = {"labels": all_labels}
        
        # Debug: ê¸¸ì´ í™•ì¸
        if verbose:
            print(f"   ğŸ” Debug - scores: {len(all_scores)}, labels: {len(all_labels)}, paths: {len(all_image_paths)}")
        
        # ê¸¸ì´ê°€ ë§ì§€ ì•ŠëŠ” ê²½ìš° ì²˜ë¦¬
        min_length = min(len(all_scores), len(all_labels), len(all_image_paths))
        if len(all_scores) != len(all_labels) or len(all_scores) != len(all_image_paths):
            if verbose:
                print(f"   âš ï¸ ê¸¸ì´ ë¶ˆì¼ì¹˜ ë°œê²¬, ìµœì†Œ ê¸¸ì´ {min_length}ë¡œ ì¡°ì •")
            all_scores = all_scores[:min_length]
            all_labels = all_labels[:min_length]
            all_image_paths = all_image_paths[:min_length]
            predictions_dict = {"pred_scores": all_scores}
            ground_truth_dict = {"labels": all_labels}
        
        save_detailed_test_results(
            predictions_dict, ground_truth_dict, all_image_paths, 
            source_analysis_dir, f"{model_type}_source_{datamodule.source_domain}"
        )
        
        # ROC curve ìƒì„±
        plot_roc_curve(all_labels, all_scores, source_analysis_dir, 
                      f"{model_type.upper()} Source {datamodule.source_domain}")
        
        # Optimal threshold ê³„ì‚° ë° ì €ì¥
        optimal_threshold = find_optimal_threshold(all_labels, all_scores)
        optimal_threshold_data = {
            "optimal_threshold": optimal_threshold,
            "method": "youden_j_statistic",
            "source_domain": datamodule.source_domain,
            "model_type": model_type
        }
        
        # Source thresholdë¡œ source domain ì„±ëŠ¥ í‰ê°€ (ì°¸ê³ ìš©)
        source_metrics_with_optimal = evaluate_with_fixed_threshold(all_scores, all_labels, optimal_threshold)
        optimal_threshold_data["source_performance_with_optimal_threshold"] = source_metrics_with_optimal
        
        # Optimal threshold ì €ì¥
        threshold_path = source_analysis_dir / "optimal_threshold.json"
        with open(threshold_path, 'w', encoding='utf-8') as f:
            import json
            json.dump(optimal_threshold_data, f, indent=2, ensure_ascii=False)
        
        if verbose:
            print(f"   ğŸ¯ Optimal threshold: {optimal_threshold:.4f}")
            print(f"   ğŸ’¾ Optimal threshold ì €ì¥: {threshold_path}")
        
        # ë©”íŠ¸ë¦­ ë³´ê³ ì„œ ì €ì¥ (ê¸°ì¡´ 0.5 threshold ê¸°ì¤€)
        save_metrics_report(all_labels, predictions, all_scores, source_analysis_dir, auroc, 0.5)
        
        # ì ìˆ˜ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨ ìƒì„±
        normal_scores = [score for gt, score in zip(all_labels, all_scores) if gt == 0]
        anomaly_scores = [score for gt, score in zip(all_labels, all_scores) if gt == 1]
        plot_score_distributions(normal_scores, anomaly_scores, source_analysis_dir, 
                               f"{model_type.upper()} Source {datamodule.source_domain}")
        
        # ì‹¤í—˜ ìš”ì•½ ì €ì¥
        source_config = {"domain": datamodule.source_domain, "model_type": model_type}
        source_results = {"auroc": auroc, "accuracy": accuracy, "precision": precision, 
                         "recall": recall, "f1_score": f1}
        save_experiment_summary(source_config, source_results, source_analysis_dir)

    result = {
        'domain': datamodule.source_domain,
        'auroc': float(auroc),
        'metrics': {
            'accuracy': float(accuracy),
            'precision': float(precision),
            'recall': float(recall),
            'f1_score': float(f1)
        },
        'num_samples': len(all_scores),
        'visualization_dir': str(visualization_dir) if visualization_dir else None,
        'analysis_dir': str(source_analysis_dir) if analysis_dir else None
    }
    
    # Analysisê°€ í™œì„±í™”ëœ ê²½ìš° optimal threshold ì¶”ê°€
    if analysis_dir:
        result['optimal_threshold'] = float(optimal_threshold)
        result['optimal_threshold_metrics'] = source_metrics_with_optimal
    
    return result


def evaluate_target_domains(
    model,
    datamodule,
    visualization_base_dir: Optional[Path] = None,
    model_type: str = "unknown",
    max_visualization_batches: int = 5,
    verbose: bool = True,
    analysis_base_dir: Optional[Path] = None,
    source_optimal_threshold: Optional[float] = None
):
    """íƒ€ê²Ÿ ë„ë©”ì¸ë“¤ì—ì„œ ëª¨ë¸ í‰ê°€ ë° ì‹œê°í™”.

    ê° íƒ€ê²Ÿ ë„ë©”ì¸ì— ëŒ€í•´ ê°œë³„ì ìœ¼ë¡œ í‰ê°€í•˜ê³  ê²°ê³¼ë¥¼ ìˆ˜ì§‘.
    ì„ íƒì ìœ¼ë¡œ ê° ë„ë©”ì¸ë³„ ì‹œê°í™” ìƒì„±.

    Args:
        model: í•™ìŠµëœ ëª¨ë¸
        datamodule: MultiDomainHDMAPDataModule
        visualization_base_dir: ì‹œê°í™” ì €ì¥ ê¸°ë³¸ ë””ë ‰í„°ë¦¬ (Noneì´ë©´ ì‹œê°í™” ì•ˆí•¨)
        model_type: ëª¨ë¸ íƒ€ì… (ì ìˆ˜ ì¶”ì¶œì— ì‚¬ìš©)
        max_visualization_batches: ë„ë©”ì¸ë³„ ì‹œê°í™”í•  ìµœëŒ€ ë°°ì¹˜ ìˆ˜ (-1ì´ë©´ ì „ì²´ ì‹œê°í™”)
        verbose: ìƒì„¸ ì¶œë ¥ ì—¬ë¶€
        analysis_base_dir: ë¶„ì„ ê²°ê³¼ ì €ì¥ ê¸°ë³¸ ë””ë ‰í„°ë¦¬ (Noneì´ë©´ ë¶„ì„ ì•ˆí•¨)

    Returns:
        dict: íƒ€ê²Ÿ ë„ë©”ì¸ë³„ í‰ê°€ ê²°ê³¼
            ê° ë„ë©”ì¸ í‚¤ì— ëŒ€í•´:
            - domain: ë„ë©”ì¸ ì´ë¦„
            - auroc: AUROC ì ìˆ˜
            - metrics: ê¸°íƒ€ ë©”íŠ¸ë¦­ (accuracy, precision, recall, f1)
            - num_samples: í‰ê°€ ìƒ˜í”Œ ìˆ˜
            - visualization_dir: ì‹œê°í™” ë””ë ‰í„°ë¦¬ ê²½ë¡œ

    Example:
        >>> target_results = evaluate_target_domains(
        ...     model=trained_model,
        ...     datamodule=datamodule,
        ...     visualization_base_dir=Path("./results/viz/targets"),
        ...     model_type="draem"
        ... )
        >>> for domain, result in target_results.items():
        ...     print(f"{domain}: AUROC={result['auroc']:.4f}")
    """
    import torch
    import numpy as np
    from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score
    from pathlib import Path

    if verbose:
        print(f"\nğŸ¯ íƒ€ê²Ÿ ë„ë©”ì¸ í‰ê°€ ì‹œì‘...")
        print(f"   íƒ€ê²Ÿ ë„ë©”ì¸ ëª©ë¡: {datamodule.target_domains}")

    # ì‹œê°í™” ê¸°ë³¸ ë””ë ‰í„°ë¦¬ ìƒì„±
    if visualization_base_dir:
        visualization_base_dir = Path(visualization_base_dir)
        visualization_base_dir.mkdir(parents=True, exist_ok=True)

    # Test dataloaders ê°€ì ¸ì˜¤ê¸°
    test_dataloaders = datamodule.test_dataloader()
    if not isinstance(test_dataloaders, list):
        test_dataloaders = [test_dataloaders]

    # í‰ê°€ ëª¨ë“œë¡œ ì „í™˜í•˜ê³  GPUë¡œ ì´ë™
    model.eval()
    if torch.cuda.is_available():
        model = model.cuda()

    target_results = {}

    # ê° íƒ€ê²Ÿ ë„ë©”ì¸ë³„ë¡œ í‰ê°€
    for domain_idx, target_domain in enumerate(datamodule.target_domains):
        if verbose:
            print(f"\n   ğŸ“Š {target_domain} í‰ê°€ ì¤‘...")

        # í•´ë‹¹ ë„ë©”ì¸ì˜ dataloader
        test_loader = test_dataloaders[domain_idx]

        # ë„ë©”ì¸ë³„ ì‹œê°í™” ë””ë ‰í„°ë¦¬
        if visualization_base_dir:
            domain_viz_dir = visualization_base_dir / target_domain
            domain_viz_dir.mkdir(exist_ok=True)
            if verbose:
                print(f"      ğŸ“ ì‹œê°í™” ì €ì¥: {domain_viz_dir}")
        else:
            domain_viz_dir = None

        # í‰ê°€ ë°ì´í„° ìˆ˜ì§‘
        all_scores = []
        all_labels = []
        all_image_paths = []

        with torch.no_grad():
            for batch_idx, batch in enumerate(test_loader):
                # GPUë¡œ ì´ë™ (ê°€ëŠ¥í•œ ê²½ìš°)
                if torch.cuda.is_available():
                    image_tensor = batch.image.cuda()
                    if hasattr(batch, 'mask'):
                        mask_tensor = batch.mask.cuda()
                else:
                    image_tensor = batch.image

                # ëª¨ë¸ ì˜ˆì¸¡
                outputs = model(image_tensor)

                # ì ìˆ˜ ì¶”ì¶œ
                scores = extract_scores_from_model_output(
                    outputs,
                    len(image_tensor),
                    batch_idx,
                    model_type
                )
                all_scores.extend(scores)

                # ì´ë¯¸ì§€ ê²½ë¡œ ì¶”ì¶œ
                if hasattr(batch, 'image_path'):
                    image_paths = batch.image_path
                    if not isinstance(image_paths, list):
                        image_paths = [image_paths]
                    all_image_paths.extend(image_paths)

                    # Ground truth ì¶”ì¶œ (ì´ë¯¸ì§€ ê²½ë¡œì—ì„œ)
                    gt_labels = []
                    for path in image_paths:
                        if '/fault/' in path:
                            gt_labels.append(1)  # anomaly
                        elif '/good/' in path:
                            gt_labels.append(0)  # normal
                        else:
                            gt_labels.append(0)  # ê¸°ë³¸ê°’
                    all_labels.extend(gt_labels)

                # ì‹œê°í™” ìƒì„± (max_visualization_batches=-1ì´ë©´ ì „ì²´, ì•„ë‹ˆë©´ ì§€ì •ëœ ë°°ì¹˜ ìˆ˜ë§Œ)
                should_visualize = (max_visualization_batches == -1 or batch_idx < max_visualization_batches)
                if domain_viz_dir and should_visualize:
                    # create_batch_visualizationsê°€ ë‚´ë¶€ì ìœ¼ë¡œ fault/good í´ë”ë¥¼ ìƒì„±í•¨
                    create_batch_visualizations(
                        image_tensor,
                        outputs,
                        image_paths,
                        domain_viz_dir,  # batch í´ë” ì—†ì´ ì§ì ‘ ì „ë‹¬
                        batch_idx,
                        model=model.model if hasattr(model, 'model') else None
                    )

                if verbose and (batch_idx + 1) % 10 == 0:
                    print(f"      ì²˜ë¦¬ ì¤‘: {batch_idx + 1}/{len(test_loader)} ë°°ì¹˜")

        # ë©”íŠ¸ë¦­ ê³„ì‚°
        all_scores = np.array(all_scores)
        all_labels = np.array(all_labels)

        # NaN ì²˜ë¦¬
        nan_mask = np.isnan(all_scores)
        if nan_mask.any():
            if verbose:
                print(f"      âš ï¸ NaN ì ìˆ˜ {nan_mask.sum()}ê°œ ë°œê²¬, ì œê±° í›„ ê³„ì†")
            valid_mask = ~nan_mask
            all_scores = all_scores[valid_mask]
            all_labels = all_labels[valid_mask]

        # AUROC ê³„ì‚°
        try:
            auroc = roc_auc_score(all_labels, all_scores)
        except Exception as e:
            if verbose:
                print(f"      âš ï¸ AUROC ê³„ì‚° ì‹¤íŒ¨: {e}")
            auroc = 0.0

        # ì´ì§„ ë¶„ë¥˜ ë©”íŠ¸ë¦­ ê³„ì‚° (threshold = 0.5)
        predictions = (all_scores > 0.5).astype(int)
        accuracy = accuracy_score(all_labels, predictions)
        precision = precision_score(all_labels, predictions, zero_division=0)
        recall = recall_score(all_labels, predictions, zero_division=0)
        f1 = f1_score(all_labels, predictions, zero_division=0)

        if verbose:
            print(f"      âœ… {target_domain} í‰ê°€ ì™„ë£Œ:")
            print(f"         - AUROC: {auroc:.4f}")
            print(f"         - Accuracy: {accuracy:.4f}")
            print(f"         - F1-Score: {f1:.4f}")
            print(f"         - ìƒ˜í”Œ ìˆ˜: {len(all_scores)}")

        # ê²°ê³¼ ì €ì¥
        # Analysis ê¸°ëŠ¥ ì¶”ê°€ (ë„ë©”ì¸ë³„)
        domain_analysis_dir = None
        if analysis_base_dir:
            analysis_base_dir = Path(analysis_base_dir)
            domain_analysis_dir = analysis_base_dir / f"target_{target_domain}"
            domain_analysis_dir.mkdir(parents=True, exist_ok=True)
            
            if verbose:
                print(f"   ğŸ’¾ íƒ€ê²Ÿ ë„ë©”ì¸ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì¤‘: {domain_analysis_dir}")
            
            # ìƒì„¸ í…ŒìŠ¤íŠ¸ ê²°ê³¼ CSV ì €ì¥
            predictions_dict = {"pred_scores": all_scores}
            ground_truth_dict = {"labels": all_labels}
            
            # Debug: ê¸¸ì´ í™•ì¸
            if verbose:
                print(f"   ğŸ” Debug [{target_domain}] - scores: {len(all_scores)}, labels: {len(all_labels)}, paths: {len(all_image_paths)}")
            
            # ê¸¸ì´ê°€ ë§ì§€ ì•ŠëŠ” ê²½ìš° ì²˜ë¦¬
            min_length = min(len(all_scores), len(all_labels), len(all_image_paths))
            if len(all_scores) != len(all_labels) or len(all_scores) != len(all_image_paths):
                if verbose:
                    print(f"   âš ï¸ [{target_domain}] ê¸¸ì´ ë¶ˆì¼ì¹˜ ë°œê²¬, ìµœì†Œ ê¸¸ì´ {min_length}ë¡œ ì¡°ì •")
                all_scores = all_scores[:min_length]
                all_labels = all_labels[:min_length]
                all_image_paths = all_image_paths[:min_length]
                predictions_dict = {"pred_scores": all_scores}
                ground_truth_dict = {"labels": all_labels}
            
            save_detailed_test_results(
                predictions_dict, ground_truth_dict, all_image_paths, 
                domain_analysis_dir, f"{model_type}_target_{target_domain}"
            )
            
            # ROC curve ìƒì„±
            plot_roc_curve(all_labels, all_scores, domain_analysis_dir, 
                          f"{model_type.upper()} Target {target_domain}")
            
            # ë©”íŠ¸ë¦­ ë³´ê³ ì„œ ì €ì¥ (ê¸°ì¡´ 0.5 threshold ê¸°ì¤€)
            save_metrics_report(all_labels, predictions, all_scores, domain_analysis_dir, auroc, 0.5)
            
            # Source optimal threshold ê¸°ë°˜ í‰ê°€ (ìˆëŠ” ê²½ìš°)
            if source_optimal_threshold is not None:
                target_metrics_with_source_threshold = evaluate_with_fixed_threshold(
                    all_scores, all_labels, source_optimal_threshold
                )
                
                # Source threshold ê¸°ë°˜ ë©”íŠ¸ë¦­ ì €ì¥
                source_threshold_data = {
                    "source_optimal_threshold": source_optimal_threshold,
                    "target_domain": target_domain,
                    "metrics_with_source_threshold": target_metrics_with_source_threshold,
                    "comparison": {
                        "default_threshold_0.5": {
                            "accuracy": float(accuracy),
                            "precision": float(precision),
                            "recall": float(recall),
                            "f1_score": float(f1)
                        },
                        "source_optimal_threshold": target_metrics_with_source_threshold
                    }
                }
                
                source_threshold_path = domain_analysis_dir / "metrics_with_source_threshold.json"
                with open(source_threshold_path, 'w', encoding='utf-8') as f:
                    import json
                    json.dump(source_threshold_data, f, indent=2, ensure_ascii=False)
                
                if verbose:
                    print(f"   ğŸ¯ Source threshold ({source_optimal_threshold:.4f}) ì ìš© ê²°ê³¼:")
                    print(f"      - Accuracy: {target_metrics_with_source_threshold['accuracy']:.4f}")
                    print(f"      - F1-Score: {target_metrics_with_source_threshold['f1_score']:.4f}")
                    print(f"   ğŸ’¾ Source threshold ë¶„ì„ ì €ì¥: {source_threshold_path}")
            
            # ì ìˆ˜ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨ ìƒì„±
            normal_scores = [score for gt, score in zip(all_labels, all_scores) if gt == 0]
            anomaly_scores = [score for gt, score in zip(all_labels, all_scores) if gt == 1]
            plot_score_distributions(normal_scores, anomaly_scores, domain_analysis_dir, 
                                   f"{model_type.upper()} Target {target_domain}")
            
            # ì‹¤í—˜ ìš”ì•½ ì €ì¥
            target_config = {"domain": target_domain, "model_type": model_type}
            target_results_dict = {"auroc": auroc, "accuracy": accuracy, "precision": precision, 
                                 "recall": recall, "f1_score": f1}
            save_experiment_summary(target_config, target_results_dict, domain_analysis_dir)

        target_results[target_domain] = {
            'domain': target_domain,
            'auroc': float(auroc),
            'metrics': {
                'accuracy': float(accuracy),
                'precision': float(precision),
                'recall': float(recall),
                'f1_score': float(f1)
            },
            'num_samples': len(all_scores),
            'visualization_dir': str(domain_viz_dir) if domain_viz_dir else None,
            'analysis_dir': str(domain_analysis_dir) if domain_analysis_dir else None
        }

    # í‰ê·  ì„±ëŠ¥ ê³„ì‚°
    if verbose:
        auroc_values = [r['auroc'] for r in target_results.values()]
        avg_auroc = np.mean(auroc_values) if auroc_values else 0.0

        print(f"\nğŸ“ˆ íƒ€ê²Ÿ ë„ë©”ì¸ í‰ê·  ì„±ëŠ¥:")
        print(f"   - í‰ê·  AUROC: {avg_auroc:.4f}")
        print(f"   - ë„ë©”ì¸ë³„ AUROC:")
        for domain, result in target_results.items():
            print(f"     â€¢ {domain}: {result['auroc']:.4f}")

    return target_results


def analyze_experiment_results(
    source_results: Dict,
    target_results: Dict,
    training_info: Optional[Dict] = None,
    experiment_config: Optional[Dict] = None,
    save_path: Optional[Union[str, Path]] = None,
    verbose: bool = True
) -> Dict:
    """Multi-domain ì‹¤í—˜ ê²°ê³¼ ì¢…í•© ë¶„ì„.

    ì†ŒìŠ¤ ë° íƒ€ê²Ÿ ë„ë©”ì¸ ê²°ê³¼ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•˜ê³  ë³´ê³ ì„œ ìƒì„±.

    Args:
        source_results: ì†ŒìŠ¤ ë„ë©”ì¸ í‰ê°€ ê²°ê³¼
        target_results: íƒ€ê²Ÿ ë„ë©”ì¸ë³„ í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        training_info: í›ˆë ¨ ì •ë³´ (optional)
        experiment_config: ì‹¤í—˜ ì„¤ì • (optional)
        save_path: ë¶„ì„ ê²°ê³¼ ì €ì¥ ê²½ë¡œ (optional)
        verbose: ìƒì„¸ ì¶œë ¥ ì—¬ë¶€

    Returns:
        dict: ì¢…í•© ë¶„ì„ ê²°ê³¼
    """
    import numpy as np
    from pathlib import Path
    import json
    from datetime import datetime

    if verbose:
        print("\n" + "="*80)
        print("ğŸ“Š Multi-Domain ì‹¤í—˜ ê²°ê³¼ ì¢…í•© ë¶„ì„")
        print("="*80)

    # ì†ŒìŠ¤ ë„ë©”ì¸ ì„±ëŠ¥
    source_performance = {
        'domain': source_results['domain'],
        'auroc': source_results['auroc'],
        'metrics': source_results.get('metrics', {})
    }

    # íƒ€ê²Ÿ ë„ë©”ì¸ë³„ ì„±ëŠ¥
    target_performance = {}
    for domain, result in target_results.items():
        target_performance[domain] = {
            'auroc': result['auroc'],
            'metrics': result.get('metrics', {}),
            'num_samples': result.get('num_samples', 0)
        }

    # í‰ê·  ë©”íŠ¸ë¦­ ê³„ì‚°
    target_aurocs = [r['auroc'] for r in target_results.values()]
    target_accuracies = [r['metrics']['accuracy'] for r in target_results.values() if 'metrics' in r]
    target_f1_scores = [r['metrics']['f1_score'] for r in target_results.values() if 'metrics' in r]

    average_metrics = {
        'source_auroc': source_performance['auroc'],
        'target_avg_auroc': np.mean(target_aurocs) if target_aurocs else 0.0,
        'target_std_auroc': np.std(target_aurocs) if target_aurocs else 0.0,
        'target_avg_accuracy': np.mean(target_accuracies) if target_accuracies else 0.0,
        'target_avg_f1': np.mean(target_f1_scores) if target_f1_scores else 0.0
    }

    # ë„ë©”ì¸ ì „ì´ ì„±ëŠ¥ ì°¨ì´ (Source - Target Average)
    domain_transfer_gap = source_performance['auroc'] - average_metrics['target_avg_auroc']

    # ìµœê³ /ìµœì € ì„±ëŠ¥ íƒ€ê²Ÿ ë„ë©”ì¸
    if target_aurocs:
        best_idx = np.argmax(target_aurocs)
        worst_idx = np.argmin(target_aurocs)
        target_domains_list = list(target_results.keys())

        best_target_domain = {
            'domain': target_domains_list[best_idx],
            'auroc': target_aurocs[best_idx]
        }

        worst_target_domain = {
            'domain': target_domains_list[worst_idx],
            'auroc': target_aurocs[worst_idx]
        }
    else:
        best_target_domain = worst_target_domain = None

    # ë¶„ì„ ê²°ê³¼ êµ¬ì„±
    analysis = {
        'timestamp': datetime.now().isoformat(),
        'source_performance': source_performance,
        'target_performance': target_performance,
        'average_metrics': average_metrics,
        'domain_transfer_gap': float(domain_transfer_gap),
        'best_target_domain': best_target_domain,
        'worst_target_domain': worst_target_domain,
        'training_info': training_info or {},
        'experiment_config': experiment_config or {}
    }

    # ê²°ê³¼ ì¶œë ¥
    if verbose:
        print(f"\nğŸ“Œ ì†ŒìŠ¤ ë„ë©”ì¸ ({source_performance['domain']}):")
        print(f"   - AUROC: {source_performance['auroc']:.4f}")
        if source_performance['metrics']:
            print(f"   - Accuracy: {source_performance['metrics'].get('accuracy', 0):.4f}")
            print(f"   - F1-Score: {source_performance['metrics'].get('f1_score', 0):.4f}")

        print(f"\nğŸ“Œ íƒ€ê²Ÿ ë„ë©”ì¸ í‰ê·  ì„±ëŠ¥:")
        print(f"   - í‰ê·  AUROC: {average_metrics['target_avg_auroc']:.4f} (Â±{average_metrics['target_std_auroc']:.4f})")
        print(f"   - í‰ê·  Accuracy: {average_metrics['target_avg_accuracy']:.4f}")
        print(f"   - í‰ê·  F1-Score: {average_metrics['target_avg_f1']:.4f}")

        print(f"\nğŸ“Œ ë„ë©”ì¸ë³„ AUROC:")
        for domain, perf in target_performance.items():
            print(f"   - {domain}: {perf['auroc']:.4f}")

        if best_target_domain:
            print(f"\nğŸ“Œ ìµœê³  ì„±ëŠ¥ íƒ€ê²Ÿ: {best_target_domain['domain']} (AUROC: {best_target_domain['auroc']:.4f})")
            print(f"ğŸ“Œ ìµœì € ì„±ëŠ¥ íƒ€ê²Ÿ: {worst_target_domain['domain']} (AUROC: {worst_target_domain['auroc']:.4f})")

        print(f"\nğŸ“Œ ë„ë©”ì¸ ì „ì´ ì„±ëŠ¥ ì°¨ì´: {domain_transfer_gap:+.4f}")
        print(f"   {'âœ… ê¸ì •ì ' if domain_transfer_gap < 0.1 else 'âš ï¸ ì£¼ì˜ í•„ìš”'}: Source-Target Gap")

        print("="*80)

    # ë¶„ì„ ê²°ê³¼ ì €ì¥
    if save_path:
        save_path = Path(save_path)
        save_path.parent.mkdir(parents=True, exist_ok=True)

        with open(save_path, 'w') as f:
            json.dump(analysis, f, indent=2)

        if verbose:
            print(f"\nğŸ’¾ ë¶„ì„ ê²°ê³¼ ì €ì¥: {save_path}")

    return analysis
