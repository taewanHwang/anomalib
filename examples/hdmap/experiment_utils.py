#!/usr/bin/env python3
"""Anomaly Detection ì‹¤í—˜ì„ ìœ„í•œ ê³µí†µ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤.

ì´ ëª¨ë“ˆì€ DRAEM, PaDiM ë“± ë‹¤ì–‘í•œ Anomaly Detection ëª¨ë¸ ì‹¤í—˜ì—ì„œ 
ì¬ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ê³µí†µ í•¨ìˆ˜ë“¤ì„ ì œê³µí•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- GPU ë©”ëª¨ë¦¬ ê´€ë¦¬
- ì‹¤í—˜ ë¡œê¹… ì„¤ì •
- ê²°ê³¼ ì´ë¯¸ì§€ ì •ë¦¬ ë° ì‹œê°í™”
- Target domain í‰ê°€
- ì‹¤í—˜ ê²°ê³¼ ë¶„ì„ ë° ì €ì¥
"""

import os
import gc
import json
import shutil
import warnings
import logging
import torch
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List, Optional, Union, Tuple
from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint

# Anomalib imports
from anomalib.engine import Engine
from anomalib.data.datamodules.image.multi_domain_hdmap import MultiDomainHDMAPDataModule
from anomalib.data.datamodules.image.all_domains_hdmap import AllDomainsHDMAPDataModule


def load_experiment_conditions(json_filename: str) -> List[Dict[str, Any]]:
    """
    JSON íŒŒì¼ì—ì„œ ì‹¤í—˜ ì¡°ê±´ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    
    Args:
        json_filename: ë¡œë“œí•  JSON íŒŒì¼ëª… (í™•ì¥ì í¬í•¨)
        
    Returns:
        ì‹¤í—˜ ì¡°ê±´ ë¦¬ìŠ¤íŠ¸
        
    Raises:
        FileNotFoundError: JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš°
        json.JSONDecodeError: JSON íŒŒì‹± ì˜¤ë¥˜ê°€ ë°œìƒí•œ ê²½ìš°
    """
    # caller ìŠ¤í¬ë¦½íŠ¸ê°€ ìˆëŠ” ë””ë ‰í† ë¦¬ ê¸°ì¤€ìœ¼ë¡œ JSON íŒŒì¼ ê²½ë¡œ ìƒì„±
    import inspect
    caller_frame = inspect.stack()[1]
    caller_dir = os.path.dirname(os.path.abspath(caller_frame.filename))
    json_path = os.path.join(caller_dir, json_filename)
    
    if not os.path.exists(json_path):
        raise FileNotFoundError(f"ì‹¤í—˜ ì¡°ê±´ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {json_path}")
    
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except json.JSONDecodeError as e:
        raise json.JSONDecodeError(f"JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
    
    # JSONì—ì„œ ë¡œë“œí•œ ë°ì´í„°ì˜ ìœ íš¨ì„± ê²€ì‚¬
    if 'experiment_conditions' not in data:
        raise ValueError("JSON íŒŒì¼ì— 'experiment_conditions' í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    experiment_conditions = data['experiment_conditions']
    
    # JSONì—ì„œëŠ” tupleì´ listë¡œ ì €ì¥ë˜ë¯€ë¡œ, í•„ìš”í•œ í•„ë“œë“¤ì„ ë‹¤ì‹œ tupleë¡œ ë³€í™˜
    for condition in experiment_conditions:
        if 'config' not in condition:
            continue
            
        config = condition['config']
        
        # range íƒ€ì…ì˜ í•„ë“œë“¤ì„ tupleë¡œ ë³€í™˜
        range_fields = ['patch_width_range', 'patch_ratio_range']
        for field in range_fields:
            if field in config and isinstance(config[field], list):
                config[field] = tuple(config[field])
    
    return experiment_conditions
    
    return experiment_conditions


def load_all_domains_experiment_conditions(json_filename: str) -> List[Dict[str, Any]]:
    """
    AllDomains ì‹¤í—˜ì„ ìœ„í•œ JSON íŒŒì¼ì—ì„œ ì‹¤í—˜ ì¡°ê±´ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    
    Args:
        json_filename: ë¡œë“œí•  JSON íŒŒì¼ëª… (í™•ì¥ì í¬í•¨)
        
    Returns:
        ì‹¤í—˜ ì¡°ê±´ ë¦¬ìŠ¤íŠ¸
        
    Raises:
        FileNotFoundError: JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš°
        json.JSONDecodeError: JSON íŒŒì‹± ì˜¤ë¥˜ê°€ ë°œìƒí•œ ê²½ìš°
    """
    # caller ìŠ¤í¬ë¦½íŠ¸ê°€ ìˆëŠ” ë””ë ‰í† ë¦¬ ê¸°ì¤€ìœ¼ë¡œ JSON íŒŒì¼ ê²½ë¡œ ìƒì„±
    import inspect
    caller_frame = inspect.stack()[1]
    caller_dir = os.path.dirname(os.path.abspath(caller_frame.filename))
    json_path = os.path.join(caller_dir, json_filename)
    
    if not os.path.exists(json_path):
        raise FileNotFoundError(f"ì‹¤í—˜ ì¡°ê±´ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {json_path}")
    
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except json.JSONDecodeError as e:
        raise json.JSONDecodeError(f"JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
    
    # AllDomains JSONì€ ì§ì ‘ ë¦¬ìŠ¤íŠ¸ í˜•íƒœ
    if isinstance(data, list):
        return data
    else:
        raise ValueError("AllDomains JSON íŒŒì¼ì€ ë¦¬ìŠ¤íŠ¸ í˜•íƒœì—¬ì•¼ í•©ë‹ˆë‹¤.")


def get_experiment_by_name(experiment_conditions: List[Dict[str, Any]], 
                          experiment_name: str) -> Dict[str, Any]:
    """
    ì‹¤í—˜ ì´ë¦„ìœ¼ë¡œ íŠ¹ì • ì‹¤í—˜ ì¡°ê±´ì„ ì°¾ìŠµë‹ˆë‹¤.
    
    Args:
        experiment_conditions: ì „ì²´ ì‹¤í—˜ ì¡°ê±´ ë¦¬ìŠ¤íŠ¸
        experiment_name: ì°¾ì„ ì‹¤í—˜ ì´ë¦„
        
    Returns:
        í•´ë‹¹ ì‹¤í—˜ ì¡°ê±´ ë”•ì…”ë„ˆë¦¬
        
    Raises:
        ValueError: í•´ë‹¹ ì´ë¦„ì˜ ì‹¤í—˜ì„ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš°
    """
    for condition in experiment_conditions:
        if condition.get('name') == experiment_name:
            return condition
    
    available_names = [c.get('name', 'Unknown') for c in experiment_conditions]
    raise ValueError(f"ì‹¤í—˜ '{experiment_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. "
                    f"ì‚¬ìš© ê°€ëŠ¥í•œ ì‹¤í—˜: {available_names}")


def validate_experiment_config(config: Dict[str, Any]) -> bool:
    """
    ì‹¤í—˜ ì„¤ì •ì˜ ìœ íš¨ì„±ì„ ê²€ì‚¬í•©ë‹ˆë‹¤.
    
    Args:
        config: ê²€ì‚¬í•  ì‹¤í—˜ ì„¤ì • ë”•ì…”ë„ˆë¦¬
        
    Returns:
        ì„¤ì •ì´ ìœ íš¨í•˜ë©´ True, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ False
    """
    required_fields = [
        'max_epochs', 'learning_rate', 'batch_size', 'image_size',
        'source_domain', 'target_domains'
    ]
    
    for field in required_fields:
        if field not in config:
            print(f"í•„ìˆ˜ ì„¤ì • '{field}'ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return False
    
    # ê°’ì˜ ìœ íš¨ì„± ê²€ì‚¬
    if config['max_epochs'] <= 0:
        print("max_epochsëŠ” 0ë³´ë‹¤ ì»¤ì•¼ í•©ë‹ˆë‹¤.")
        return False
        
    if config['learning_rate'] <= 0:
        print("learning_rateëŠ” 0ë³´ë‹¤ ì»¤ì•¼ í•©ë‹ˆë‹¤.")
        return False
        
    if config['batch_size'] <= 0:
        print("batch_sizeëŠ” 0ë³´ë‹¤ ì»¤ì•¼ í•©ë‹ˆë‹¤.")
        return False
    
    return True


def print_experiment_summary(experiment_conditions: List[Dict[str, Any]]) -> None:
    """
    ì‹¤í—˜ ì¡°ê±´ë“¤ì˜ ìš”ì•½ ì •ë³´ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
    
    Args:
        experiment_conditions: ì‹¤í—˜ ì¡°ê±´ ë¦¬ìŠ¤íŠ¸
    """
    print(f"\n=== ì‹¤í—˜ ì¡°ê±´ ìš”ì•½ (ì´ {len(experiment_conditions)}ê°œ) ===")
    
    for i, condition in enumerate(experiment_conditions, 1):
        name = condition.get('name', 'Unknown')
        description = condition.get('description', 'No description')
        config = condition.get('config', {})
        
        epochs = config.get('max_epochs', 'Unknown')
        lr = config.get('learning_rate', 'Unknown')
        
        print(f"{i:2d}. {name}")
        print(f"    ì„¤ëª…: {description}")
        print(f"    ì—í¬í¬: {epochs}, í•™ìŠµë¥ : {lr}")
        
        if 'patch_width_range' in config and 'patch_ratio_range' in config:
            width_range = config['patch_width_range']
            ratio_range = config['patch_ratio_range']
            print(f"    íŒ¨ì¹˜ í¬ê¸°: {width_range}, ë¹„ìœ¨: {ratio_range}")
        
        print()


def extract_target_domains_from_config(config: Dict[str, Any]) -> List[str]:
    """
    ì‹¤í—˜ ì„¤ì •ì—ì„œ target domainsë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    Args:
        config: ì‹¤í—˜ ì„¤ì • ë”•ì…”ë„ˆë¦¬
        
    Returns:
        List[str]: target domain ë¦¬ìŠ¤íŠ¸
    """
    target_domains = config['target_domains']
    
    if target_domains == 'auto':
        # ê¸°ë³¸ HDMAP ë„ë©”ì¸ (source_domain ì œì™¸)
        source_domain = config['source_domain']
        all_domains = ['domain_A', 'domain_B', 'domain_C', 'domain_D']
        target_domains = [d for d in all_domains if d != source_domain]
    elif isinstance(target_domains, str):
        target_domains = [target_domains]
    elif not isinstance(target_domains, list):
        target_domains = ['domain_B', 'domain_C', 'domain_D']
    
    return target_domains


def analyze_experiment_results(
    source_results: Dict[str, Any],
    target_results: Dict[str, Dict[str, Any]],
    training_info: Dict[str, Any],
    condition: Dict[str, Any],
    model_type: str = "Model"
) -> Dict[str, Any]:
    """
    ì‹¤í—˜ ê²°ê³¼ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤ (ëª¨ë“  ëª¨ë¸ì—ì„œ ê³µí†µ ì‚¬ìš© ê°€ëŠ¥).
    
    Args:
        source_results: ì†ŒìŠ¤ ë„ë©”ì¸ í‰ê°€ ê²°ê³¼
        target_results: íƒ€ê²Ÿ ë„ë©”ì¸ í‰ê°€ ê²°ê³¼
        training_info: í›ˆë ¨ ì •ë³´
        condition: ì‹¤í—˜ ì¡°ê±´
        model_type: ëª¨ë¸ íƒ€ì… (ì¶œë ¥ìš©)
        
    Returns:
        Dict[str, Any]: ë¶„ì„ëœ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    print(f"\nğŸ“Š {model_type} ì‹¤í—˜ ê²°ê³¼ ë¶„ì„")
    
    # íƒ€ê²Ÿ ë„ë©”ì¸ í‰ê·  AUROC ê³„ì‚°
    target_aurocs = []
    for domain, result in target_results.items():
        if isinstance(result.get('image_AUROC'), (int, float)):
            target_aurocs.append(result['image_AUROC'])
    
    avg_target_auroc = sum(target_aurocs) / len(target_aurocs) if target_aurocs else 0.0
    
    # ì†ŒìŠ¤ ë„ë©”ì¸ AUROC
    source_auroc = source_results.get('image_AUROC', 0.0) if source_results else 0.0
    
    # ë„ë©”ì¸ ì „ì´ íš¨ê³¼ ê³„ì‚°
    transfer_ratio = avg_target_auroc / source_auroc if source_auroc > 0 else 0.0
    
    # ì„±ëŠ¥ í‰ê°€
    if transfer_ratio > 0.9:
        transfer_grade = "ìš°ìˆ˜"
    elif transfer_ratio > 0.8:
        transfer_grade = "ì–‘í˜¸"
    elif transfer_ratio > 0.7:
        transfer_grade = "ë³´í†µ"
    else:
        transfer_grade = "ê°œì„ í•„ìš”"
    
    # ê²°ê³¼ ìš”ì•½
    analysis = {
        "experiment_name": condition["name"],
        "source_auroc": source_auroc,
        "avg_target_auroc": avg_target_auroc,
        "transfer_ratio": transfer_ratio,
        "transfer_grade": transfer_grade,
        "target_domain_count": len(target_results),
        "training_epochs": training_info.get("last_trained_epoch", 0),
        "early_stopped": training_info.get("early_stopped", False),
        "best_val_auroc": training_info.get("best_val_auroc", 0.0)
    }
    
    # ë„ë©”ì¸ë³„ ìƒì„¸ ì„±ëŠ¥
    domain_performances = {}
    for domain, result in target_results.items():
        domain_performances[domain] = {
            "auroc": result.get('image_AUROC', 0.0),
            "f1_score": result.get('image_F1Score', 0.0)
        }
    
    analysis["domain_performances"] = domain_performances
    
    # ë¡œê¹…
    print(f"   ğŸ“ˆ Source AUROC: {source_auroc:.4f}")
    print(f"   ğŸ¯ Target í‰ê·  AUROC: {avg_target_auroc:.4f}")
    print(f"   ğŸ”„ ì „ì´ ë¹„ìœ¨: {transfer_ratio:.3f} ({transfer_grade})")
    print(f"   ğŸ“š í›ˆë ¨ ì—í¬í¬: {analysis['training_epochs']}")
    
    for domain, perf in domain_performances.items():
        print(f"   â””â”€ {domain}: AUROC={perf['auroc']:.4f}")
    
    return analysis


def create_common_experiment_result(
    condition: Dict[str, Any],
    status: str = "success",
    experiment_path: str = None,
    source_results: Dict[str, Any] = None,
    target_results: Dict[str, Dict[str, Any]] = None,
    training_info: Dict[str, Any] = None,
    best_checkpoint: str = None,
    error: str = None
) -> Dict[str, Any]:
    """
    ê³µí†µ ì‹¤í—˜ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        condition: ì‹¤í—˜ ì¡°ê±´
        status: ì‹¤í—˜ ìƒíƒœ ("success" ë˜ëŠ” "failed")
        experiment_path: ì‹¤í—˜ ê²½ë¡œ
        source_results: ì†ŒìŠ¤ ë„ë©”ì¸ ê²°ê³¼
        target_results: íƒ€ê²Ÿ ë„ë©”ì¸ ê²°ê³¼ë“¤
        training_info: í›ˆë ¨ ì •ë³´
        best_checkpoint: ìµœê³  ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
        error: ì—ëŸ¬ ë©”ì‹œì§€ (ì‹¤íŒ¨ ì‹œ)
        
    Returns:
        Dict[str, Any]: ì‹¤í—˜ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    result = {
        "condition": condition,
        "status": status,
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "experiment_path": experiment_path,
        "source_results": source_results or {},
        "target_results": target_results or {},
        "training_info": training_info or {},
        "best_checkpoint": best_checkpoint,
    }
    
    if status == "failed":
        result["error"] = error
    else:
        # Target domain í‰ê·  AUROC ê³„ì‚°
        if target_results:
            target_aurocs = []
            for domain, domain_result in target_results.items():
                auroc = domain_result.get('image_AUROC')
                if isinstance(auroc, (int, float)):
                    target_aurocs.append(auroc)
            
            if target_aurocs:
                result["avg_target_auroc"] = sum(target_aurocs) / len(target_aurocs)
            else:
                result["avg_target_auroc"] = 0.0
        else:
            result["avg_target_auroc"] = 0.0
    
    return result


def create_experiment_visualization(
    experiment_name: str,
    model_type: str,
    results_base_dir: str,
    source_domain: str = None,
    target_domains: list = None,
    source_results: Dict[str, Any] = None,
    target_results: Dict[str, Dict[str, Any]] = None,
    single_domain: bool = False
) -> str:
    """ì‹¤í—˜ ê²°ê³¼ ì‹œê°í™” í´ë” êµ¬ì¡° ìƒì„± ë° ì‹¤í—˜ ì •ë³´ ì €ì¥.
    
    Args:
        experiment_name: ì‹¤í—˜ ì´ë¦„
        model_type: ëª¨ë¸ íƒ€ì… (ì˜ˆ: "DRAEM-SevNet", "PaDiM")
        results_base_dir: ê¸°ë³¸ ê²°ê³¼ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        source_domain: ì†ŒìŠ¤ ë„ë©”ì¸ ì´ë¦„ (single_domain=Trueì¼ ë•ŒëŠ” None ê°€ëŠ¥)
        target_domains: íƒ€ê²Ÿ ë„ë©”ì¸ ë¦¬ìŠ¤íŠ¸
        source_results: ì†ŒìŠ¤ ë„ë©”ì¸ í‰ê°€ ê²°ê³¼
        target_results: íƒ€ê²Ÿ ë„ë©”ì¸ë“¤ í‰ê°€ ê²°ê³¼
        single_domain: ë‹¨ì¼ ë„ë©”ì¸ ì‹¤í—˜ ì—¬ë¶€
        
    Returns:
        str: ìƒì„±ëœ visualize ë””ë ‰í† ë¦¬ ê²½ë¡œ
    """
    print(f"\nğŸ¨ {model_type} Visualization ìƒì„±")
    
    # ê¸°ë³¸ ê²½ë¡œë¥¼ ì‚¬ìš© (ì´ë¯¸ ì‹¤í—˜ë³„ ê³ ìœ  ê²½ë¡œì„)
    base_path = Path(results_base_dir)
    
    # ë¨¼ì € v* íŒ¨í„´ì˜ ë²„ì „ í´ë”ê°€ ìˆëŠ”ì§€ í™•ì¸
    version_dirs = [d for d in base_path.glob("v*") if d.is_dir() and d.name.startswith('v') and d.name[1:].isdigit()]
    if version_dirs:
        # v0, v1 ë“±ì˜ íŒ¨í„´ì´ ìˆìœ¼ë©´ ìµœì‹  ë²„ì „ ì‚¬ìš©
        latest_version_path = max(version_dirs, key=lambda x: int(x.name[1:]))
    else:
        # ë²„ì „ í´ë”ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©
        latest_version_path = base_path
    
    # visualize í´ë” ìƒì„±
    viz_path = latest_version_path / "visualize"
    viz_path.mkdir(exist_ok=True)
    
    # í´ë” êµ¬ì¡° ìƒì„± (single_domain ì‹¤í—˜ì—ì„œëŠ” target_domains í´ë” ìƒì„±í•˜ì§€ ì•ŠìŒ)
    if single_domain:
        folders_to_create = ["results"]
    else:
        folders_to_create = [
            "source_domain",
            "target_domains"
        ]
    
    for folder in folders_to_create:
        (viz_path / folder).mkdir(exist_ok=True)
    
    # íƒ€ê²Ÿ ë„ë©”ì¸ë³„ í•˜ìœ„ í´ë” ìƒì„± (multi-domain ì‹¤í—˜ì—ë§Œ ì ìš©)
    if not single_domain and target_domains:
        for domain in target_domains:
            (viz_path / "target_domains" / domain).mkdir(exist_ok=True)
    
    # ì‹¤í—˜ ì •ë³´ë¥¼ JSONìœ¼ë¡œ ì €ì¥
    experiment_info = {
        "experiment_name": experiment_name,
        "model_type": model_type,
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "results_path": str(latest_version_path),
        "source_domain": source_domain,
        "target_domains": target_domains or [],
        "results_summary": {
            "source_results": source_results or {},
            "target_results": target_results or {}
        }
    }
    
    # JSON íŒŒì¼ë¡œ ì €ì¥
    info_file = viz_path / "experiment_info.json"
    with open(info_file, 'w', encoding='utf-8') as f:
        json.dump(experiment_info, f, indent=2, ensure_ascii=False)
    
    # ì‹¤ì œ ìƒì„±ëœ ì´ë¯¸ì§€ íŒŒì¼ë“¤ì„ visualize/resultsë¡œ ë³µì‚¬ (single domainì˜ ê²½ìš°)
    if single_domain:
        try:
            # anomalib ëª¨ë¸ ê²°ê³¼ ì´ë¯¸ì§€ê°€ ì €ì¥ë˜ëŠ” íŒ¨í„´ íƒìƒ‰
            image_patterns = [
                latest_version_path / "*" / "*" / source_domain / "latest" / "images",  # ì¼ë°˜ì ì¸ íŒ¨í„´
                latest_version_path / "*" / source_domain / "latest" / "images",        # ì¶•ì•½ëœ íŒ¨í„´
                latest_version_path / "images",                                          # ì§ì ‘ images í´ë”
            ]
            
            images_found = False
            for pattern_path in image_patterns:
                # glob íŒ¨í„´ìœ¼ë¡œ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ ì°¾ê¸°
                for images_dir in Path(str(pattern_path).replace('*', '')).parent.glob('**/images'):
                    if images_dir.exists() and any(images_dir.iterdir()):
                        print(f"ğŸ“ ì´ë¯¸ì§€ ë°œê²¬: {images_dir}")
                        
                        # ì´ë¯¸ì§€ íŒŒì¼ë“¤ì„ visualize/resultsë¡œ ë³µì‚¬
                        results_dir = viz_path / "results"
                        
                        # ì„œë¸Œ ë””ë ‰í† ë¦¬ë³„ë¡œ ë³µì‚¬ (good, fault ë“±)
                        for subdir in images_dir.iterdir():
                            if subdir.is_dir():
                                target_subdir = results_dir / subdir.name
                                target_subdir.mkdir(exist_ok=True)
                                
                                # ëª¨ë“  ì´ë¯¸ì§€ íŒŒì¼ ë³µì‚¬
                                image_files = list(subdir.glob('*.png'))
                                for img_file in image_files:
                                    shutil.copy2(img_file, target_subdir / img_file.name)
                                
                                print(f"   ğŸ“¸ {len(image_files)}ê°œ ì´ë¯¸ì§€ë¥¼ {target_subdir}ì— ë³µì‚¬")
                        
                        images_found = True
                        break
                
                if images_found:
                    break
            
            if not images_found:
                print(f"âš ï¸ ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {latest_version_path}")
        
        except Exception as copy_error:
            print(f"âš ï¸ ì´ë¯¸ì§€ ë³µì‚¬ ì¤‘ ì˜¤ë¥˜: {copy_error}")
    
    print(f"âœ… {model_type} í´ë” êµ¬ì¡° ìƒì„± ì™„ë£Œ: {viz_path}")
    
    return str(viz_path)


def create_multi_domain_datamodule(
    datamodule_class,
    source_domain: str = "domain_A",
    target_domains: Union[str, List[str]] = "auto",
    batch_size: int = 16,
    image_size: str = "224x224",
    dataset_root: str = None,
    validation_strategy: str = "source_test",
    num_workers: int = 16
):
    """ì¼ë°˜í™”ëœ MultiDomain DataModule ìƒì„±.
    
    Args:
        datamodule_class: DataModule í´ë˜ìŠ¤ (ì˜ˆ: MultiDomainHDMAPDataModule)
        source_domain: ì†ŒìŠ¤ ë„ë©”ì¸ ì´ë¦„
        target_domains: íƒ€ê²Ÿ ë„ë©”ì¸ ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” "auto"
        batch_size: ë°°ì¹˜ í¬ê¸°
        image_size: ì´ë¯¸ì§€ í¬ê¸°
        dataset_root: ë°ì´í„°ì…‹ ë£¨íŠ¸ ê²½ë¡œ
        validation_strategy: ê²€ì¦ ì „ëµ
        num_workers: ì›Œì»¤ ìˆ˜
        
    Returns:
        ìƒì„±ëœ datamodule ì¸ìŠ¤í„´ìŠ¤
    """
    print(f"\nğŸ“¦ {datamodule_class.__name__} ìƒì„± ì¤‘...")
    print(f"   Source Domain: {source_domain}")
    print(f"   Target Domains: {target_domains}")
    
    # ê¸°ë³¸ dataset_root ì„¤ì •
    if dataset_root is None:
        dataset_root = f"./datasets/HDMAP/1000_8bit_resize_{image_size}"
    
    datamodule = datamodule_class(
        root=dataset_root,
        source_domain=source_domain,
        target_domains=target_domains,
        validation_strategy=validation_strategy,
        train_batch_size=batch_size,
        eval_batch_size=batch_size,
        num_workers=num_workers,
    )
    
    # ë°ì´í„° ì¤€ë¹„ ë° ì„¤ì •
    datamodule.prepare_data()
    datamodule.setup()
    
    print(f"âœ… {datamodule_class.__name__} ì„¤ì • ì™„ë£Œ")
    print(f"   ì‹¤ì œ Target Domains: {datamodule.target_domains}")
    print(f"   í›ˆë ¨ ë°ì´í„°: {len(datamodule.train_data)} ìƒ˜í”Œ (source: {datamodule.source_domain})")
    print(f"   ê²€ì¦ ë°ì´í„°: {len(datamodule.val_data)} ìƒ˜í”Œ (source test)")
    
    total_test_samples = sum(len(test_data) for test_data in datamodule.test_data)
    print(f"   í…ŒìŠ¤íŠ¸ ë°ì´í„°: {total_test_samples} ìƒ˜í”Œ (targets)")
    
    for i, target_domain in enumerate(datamodule.target_domains):
        print(f"     â””â”€ {target_domain}: {len(datamodule.test_data[i])} ìƒ˜í”Œ")
    
    return datamodule


def create_all_domains_datamodule(
    datamodule_class,
    batch_size: int,
    image_size: str,
    domains: list[str] = None,
    val_split_ratio: float = 0.2,
    dataset_root: str = None,
    num_workers: int = 8
) -> AllDomainsHDMAPDataModule:
    """AllDomainsHDMAPDataModule ìƒì„± ë° ì„¤ì •.
    
    Args:
        datamodule_class: AllDomainsHDMAPDataModule í´ë˜ìŠ¤ (ì¼ê´€ì„±ì„ ìœ„í•´ ì¶”ê°€, ì‹¤ì œë¡œëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
        batch_size: ë°°ì¹˜ í¬ê¸°
        image_size: ì´ë¯¸ì§€ í¬ê¸° (ì˜ˆ: "392x392")
        domains: ì‚¬ìš©í•  ë„ë©”ì¸ ë¦¬ìŠ¤íŠ¸. Noneì´ë©´ ëª¨ë“  ë„ë©”ì¸ ì‚¬ìš©
        val_split_ratio: ê²€ì¦ ë°ì´í„° ë¶„í•  ë¹„ìœ¨
        dataset_root: ë°ì´í„°ì…‹ ë£¨íŠ¸ ê²½ë¡œ (Noneì´ë©´ ìë™ ìƒì„±)
        num_workers: ì›Œì»¤ ìˆ˜
        
    Returns:
        ì„¤ì •ëœ AllDomainsHDMAPDataModule
    """
    print(f"\nğŸ“¦ AllDomainsHDMAPDataModule ìƒì„± ì¤‘...")
    
    # ì´ë¯¸ì§€ í¬ê¸° íŒŒì‹±
    try:
        width, height = map(int, image_size.split('x'))
        image_size_tuple = (width, height)
    except ValueError:
        # ê¸°ë³¸ê°’ ì‚¬ìš©
        image_size_tuple = (392, 392)
        print(f"   âš ï¸ ì´ë¯¸ì§€ í¬ê¸° íŒŒì‹± ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©: {image_size_tuple}")
    
    # ë„ë©”ì¸ ì •ë³´ ì¶œë ¥
    domains_info = f"ì „ì²´ ë„ë©”ì¸ (A~D)" if not domains else f"{domains}"
    print(f"   ğŸŒ ë„ë©”ì¸: {domains_info}")
    print(f"   ğŸ“ ì´ë¯¸ì§€ í¬ê¸°: {image_size_tuple}")
    print(f"   ğŸ“Š ë°°ì¹˜ í¬ê¸°: {batch_size}")
    print(f"   ğŸ”„ Val ë¶„í•  ë¹„ìœ¨: {val_split_ratio}")
    
    # ì´ë¯¸ì§€ í¬ê¸°ì— ë”°ë¥¸ ë°ì´í„°ì…‹ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
    if dataset_root is None:
        # í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì ˆëŒ€ ê²½ë¡œ ìƒì„±
        import os
        current_dir = os.getcwd()
        dataset_root = os.path.join(current_dir, "datasets", "HDMAP", f"1000_8bit_resize_{image_size}")
    
    # AllDomainsHDMAPDataModule ìƒì„±
    datamodule = AllDomainsHDMAPDataModule(
        root=dataset_root,
        domains=domains,  # Noneì´ë©´ ëª¨ë“  ë„ë©”ì¸ ì‚¬ìš©
        train_batch_size=batch_size,
        eval_batch_size=batch_size,
        num_workers=num_workers,
        val_split_ratio=val_split_ratio,  # trainì—ì„œ validation ë¶„í• 
        seed=42
    )
    
    # ë°ì´í„° ì„¤ì •
    print(f"   âš™ï¸  DataModule ì„¤ì • ì¤‘...")
    datamodule.setup()
    
    # ë°ì´í„° í†µê³„ ì¶œë ¥
    print(f"   âœ… DataModule ì„¤ì • ì™„ë£Œ!")
    print(f"      â€¢ Train ìƒ˜í”Œ: {len(datamodule.train_data):,}ê°œ (ëª¨ë“  ë„ë©”ì¸ ì •ìƒ ë°ì´í„°)")
    print(f"      â€¢ Val ìƒ˜í”Œ: {len(datamodule.val_data):,}ê°œ (trainì—ì„œ ë¶„í• )")
    print(f"      â€¢ Test ìƒ˜í”Œ: {len(datamodule.test_data):,}ê°œ (ëª¨ë“  ë„ë©”ì¸ ì •ìƒ+ê²°í•¨)")
    
    return datamodule


def evaluate_source_domain(
    model: Any, 
    engine: Any, 
    datamodule: Any,
    checkpoint_path: str = None
) -> Dict[str, Any]:
    """ì¼ë°˜í™”ëœ Source Domain ì„±ëŠ¥ í‰ê°€.
    
    Args:
        model: í‰ê°€í•  ëª¨ë¸
        engine: Lightning Engine
        datamodule: ë°ì´í„° ëª¨ë“ˆ
        checkpoint_path: ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ (ì„ íƒì‚¬í•­)
        
    Returns:
        Dict[str, Any]: í‰ê°€ ê²°ê³¼
    """
    print(f"\nğŸ“Š Source Domain ì„±ëŠ¥ í‰ê°€ - {datamodule.source_domain}")
    print("   ğŸ’¡ í‰ê°€ ë°ì´í„°: Source domain test (validationìœ¼ë¡œ ì‚¬ìš©ëœ ë°ì´í„°)")
    print("   ğŸ¯ ì¬í˜„ì„±ì„ ìœ„í•´ í›ˆë ¨ì— ì‚¬ìš©ëœ ë™ì¼í•œ DataModuleì˜ val_dataloader ì‚¬ìš©")
    print(f"   ğŸ“‹ ê²€ì¦ ë°ì´í„°ì…‹ í¬ê¸°: {len(datamodule.val_data)} ìƒ˜í”Œ")
    
    # í›ˆë ¨ì— ì‚¬ìš©ëœ ë™ì¼í•œ DataModuleì˜ validation DataLoader ì‚¬ìš©
    # ì´ë ‡ê²Œ í•˜ë©´ ì™„ì „íˆ ë™ì¼í•œ ë°ì´í„°ì…‹ ì¸ìŠ¤í„´ìŠ¤ì™€ ìˆœì„œë¥¼ ë³´ì¥
    val_dataloader = datamodule.val_dataloader()
    
    # Engineì˜ ê²½ë¡œ ì„¤ì • í™•ì¸ (fit() í›„ì—ë§Œ ì ‘ê·¼ ê°€ëŠ¥)
    try:
        if hasattr(engine, 'trainer') and engine.trainer is not None and hasattr(engine.trainer, 'default_root_dir'):
            print(f"   ğŸ”§ Source domain í‰ê°€ ì‹œ Engine default_root_dir: {engine.trainer.default_root_dir}")
    except Exception as e:
        print(f"   âš ï¸ Warning: Engine ê²½ë¡œ í™•ì¸ ì‹¤íŒ¨: {e}")
    
    if checkpoint_path:
        results = engine.test(
            model=model,
            dataloaders=val_dataloader,
            ckpt_path=checkpoint_path
        )
    else:
        results = engine.test(
            model=model,
            dataloaders=val_dataloader
        )
    
    if results and len(results) > 0:
        source_metrics = results[0]
        
        # test_image_AUROC -> image_AUROC í‚¤ ë³€í™˜ (í‘œì¤€í™”)
        if 'test_image_AUROC' in source_metrics:
            source_metrics['image_AUROC'] = source_metrics['test_image_AUROC']
        if 'test_image_F1Score' in source_metrics:
            source_metrics['image_F1Score'] = source_metrics['test_image_F1Score']
        
        print(f"   âœ… Source Domain í‰ê°€ ì™„ë£Œ:")
        print(f"   ğŸ“ ì£¼ìš” ë©”íŠ¸ë¦­ (Validationê³¼ ë™ì¼í•´ì•¼ í•¨):")
        
        # ì£¼ìš” ë©”íŠ¸ë¦­ ì¶œë ¥
        for key, value in source_metrics.items():
            if isinstance(value, (int, float)):
                if 'AUROC' in key or 'F1Score' in key:
                    print(f"     â””â”€ {key}: {value:.4f}")
                else:
                    print(f"     â””â”€ {key}: {value}")
        
        return source_metrics
    else:
        print(f"   âŒ Source Domain í‰ê°€ ì‹¤íŒ¨")
        return {}


def cleanup_gpu_memory():
    """GPU ë©”ëª¨ë¦¬ ì •ë¦¬ (ëª¨ë“  ëª¨ë¸ì—ì„œ ê³µí†µ ì‚¬ìš© ê°€ëŠ¥)."""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        gc.collect()


def setup_warnings_filter():
    """ì‹¤í—˜ ì‹œ ë¶ˆí•„ìš”í•œ ê²½ê³  ë©”ì‹œì§€ í•„í„°ë§ (ëª¨ë“  ëª¨ë¸ì—ì„œ ê³µí†µ ì‚¬ìš© ê°€ëŠ¥)."""
    warnings.filterwarnings("ignore", category=FutureWarning)
    warnings.filterwarnings("ignore", category=DeprecationWarning)
    warnings.filterwarnings("ignore", category=UserWarning)
    
    # ì‹œê°í™” ê´€ë ¨ íŠ¹ì • ê²½ê³  í•„í„°ë§ (ë” í¬ê´„ì )
    warnings.filterwarnings("ignore", message=".*Field.*gt_mask.*is None.*")
    warnings.filterwarnings("ignore", message=".*Skipping visualization.*")
    warnings.filterwarnings("ignore", message=".*gt_mask.*None.*")


def setup_experiment_logging(log_file_path: str, experiment_name: str) -> logging.Logger:
    """ì‹¤í—˜ ë¡œê¹… ì„¤ì • (ëª¨ë“  ëª¨ë¸ì—ì„œ ê³µí†µ ì‚¬ìš© ê°€ëŠ¥).
    
    Args:
        log_file_path: ë¡œê·¸ íŒŒì¼ ê²½ë¡œ
        experiment_name: ì‹¤í—˜ ì´ë¦„
        
    Returns:
        logging.Logger: ì„¤ì •ëœ ë¡œê±°
    """
    # ë¡œê±° ì„¤ì •
    logger = logging.getLogger(experiment_name)
    logger.setLevel(logging.INFO)
    
    # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±°
    for handler in logger.handlers[:]:
        logger.removeHandler(handler)
    
    # íŒŒì¼ í•¸ë“¤ëŸ¬ ì¶”ê°€
    file_handler = logging.FileHandler(log_file_path, encoding='utf-8')
    file_handler.setLevel(logging.INFO)
    
    # í¬ë§·í„° ì„¤ì •
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    file_handler.setFormatter(formatter)
    
    logger.addHandler(file_handler)
    
    return logger


def extract_training_info(engine: Engine) -> Dict[str, Any]:
    """PyTorch Lightning Engineì—ì„œ í•™ìŠµ ì •ë³´ ì¶”ì¶œ (ëª¨ë“  ëª¨ë¸ì—ì„œ ê³µí†µ ì‚¬ìš© ê°€ëŠ¥).
    
    Args:
        engine: Anomalib Engine ê°ì²´
        
    Returns:
        Dict[str, Any]: í•™ìŠµ ì •ë³´ ë”•ì…”ë„ˆë¦¬
    """
    import torch
    
    trainer = engine.trainer
    
    # Early Stopping ì½œë°± ì°¾ê¸°
    early_stopping_callback = None
    for callback in trainer.callbacks:
        if isinstance(callback, EarlyStopping):
            early_stopping_callback = callback
            break
    
    # ì‹¤ì œ í•™ìŠµ ì™„ë£Œ ì—í­ ê³„ì‚°
    if early_stopping_callback and hasattr(early_stopping_callback, 'stopped_epoch') and early_stopping_callback.stopped_epoch > 0:
        # Early stoppingì´ ë°œìƒí•œ ê²½ìš°: stopped_epochì´ ë§ˆì§€ë§‰ í•™ìŠµ ì—í­
        last_trained_epoch = early_stopping_callback.stopped_epoch + 1  # 0-based â†’ 1-based
        early_stopped = True
    else:
        # ì •ìƒ ì™„ë£Œ ë˜ëŠ” early stopping ì—†ìŒ
        last_trained_epoch = trainer.current_epoch + 1  # 0-based â†’ 1-based
        early_stopped = False
    
    # ê¸°ë³¸ ì •ë³´
    training_info = {
        "max_epochs_configured": trainer.max_epochs,
        "last_trained_epoch": last_trained_epoch,  # ì‹¤ì œ ë§ˆì§€ë§‰ìœ¼ë¡œ í•™ìŠµí•œ ì—í­ (1-based)
        "total_steps": trainer.global_step,
        "early_stopped": early_stopped,
        "early_stop_reason": None,
        "best_val_auroc": None,  # ìµœê³  validation AUROC
        "f1_threshold_issue": "F1ScoreëŠ” ê¸°ë³¸ thresholdë¡œ ê³„ì‚°ë¨. AUROC ëŒ€ë¹„ ë‚®ì„ ìˆ˜ ìˆìŒ."
    }
    
    # Early stopping ì„¸ë¶€ ì •ë³´ ì¶”ê°€
    if early_stopping_callback:
        if training_info["early_stopped"]:
            training_info["early_stop_reason"] = f"No improvement for {early_stopping_callback.patience} epochs"
        
        # ìµœê³  ì„±ëŠ¥ ê¸°ë¡
        if hasattr(early_stopping_callback, 'best_score') and early_stopping_callback.best_score is not None:
            best_score = early_stopping_callback.best_score
            if hasattr(best_score, 'cpu'):
                training_info["best_val_auroc"] = float(best_score.cpu())
            else:
                training_info["best_val_auroc"] = float(best_score)
    
    # ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ì •ë³´ì—ì„œë„ ìµœê³  ì„±ëŠ¥ ì¶”ì¶œ
    checkpoint_callback = None
    for callback in trainer.callbacks:
        if hasattr(callback, 'best_model_score'):
            checkpoint_callback = callback
            break
    
    if checkpoint_callback and hasattr(checkpoint_callback, 'best_model_score') and checkpoint_callback.best_model_score is not None:
        best_score = checkpoint_callback.best_model_score
        if hasattr(best_score, 'cpu'):
            training_info["best_val_auroc"] = float(best_score.cpu())
        else:
            training_info["best_val_auroc"] = float(best_score)
    
    # í•™ìŠµ ì™„ë£Œ ë°©ì‹ ê²°ì •
    if training_info["early_stopped"]:
        completion_type = "early_stopping"
        completion_description = f"ì—í­ {training_info['last_trained_epoch']}ì—ì„œ early stoppingìœ¼ë¡œ ì¤‘ë‹¨"
    elif training_info["last_trained_epoch"] >= training_info["max_epochs_configured"]:
        completion_type = "max_epochs_reached"
        completion_description = f"ìµœëŒ€ ì—í­ {training_info['max_epochs_configured']} ì™„ë£Œ"
    else:
        completion_type = "interrupted"
        completion_description = f"ì—í­ {training_info['last_trained_epoch']}ì—ì„œ ì¤‘ë‹¨ë¨"
    
    training_info["completion_type"] = completion_type
    training_info["completion_description"] = completion_description
    
    return training_info


def find_anomalib_image_paths(base_search_path: Path) -> Optional[Path]:
    """Anomalibì´ ìƒì„±í•œ ì´ë¯¸ì§€ ê²½ë¡œë¥¼ ìë™ìœ¼ë¡œ íƒìƒ‰ (ëª¨ë“  ëª¨ë¸ì—ì„œ ê³µí†µ ì‚¬ìš© ê°€ëŠ¥).
    
    Args:
        base_search_path: íƒìƒ‰í•  ê¸°ë³¸ ê²½ë¡œ
        
    Returns:
        Optional[Path]: ë°œê²¬ëœ ì´ë¯¸ì§€ ê²½ë¡œì˜ ë¶€ëª¨ ë””ë ‰í† ë¦¬ (images í´ë”ì˜ ë¶€ëª¨)
    """
    # ë‹¤ì–‘í•œ ëª¨ë¸ êµ¬ì¡°ì— ëŒ€ì‘í•˜ëŠ” íŒ¨í„´ë“¤
    search_patterns = [
        "**/DraemSevNet/MultiDomainHDMAPDataModule/**/images",  # DRAEM ê³„ì—´
        "**/Padim/MultiDomainHDMAPDataModule/**/images",        # PaDiM ê³„ì—´
        "**/*/MultiDomainHDMAPDataModule/**/images",            # ì¼ë°˜ì ì¸ íŒ¨í„´
        "**/images"                                              # ê°€ì¥ ì¼ë°˜ì ì¸ íŒ¨í„´
    ]
    
    anomalib_image_paths = []
    
    for pattern in search_patterns:
        found_paths = list(base_search_path.glob(pattern))
        anomalib_image_paths.extend(found_paths)
    
    if anomalib_image_paths:
        # ê²½ë¡œ ìƒì„± ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ìµœì‹  ì„ íƒ
        latest_image_path = max(anomalib_image_paths, key=lambda p: p.stat().st_mtime if p.exists() else 0)
        anomalib_results_path = latest_image_path.parent  # images í´ë”ì˜ ë¶€ëª¨
        print(f"   âœ… ì‹¤ì œ Anomalib ê²°ê³¼ ê²½ë¡œ: {anomalib_results_path}")
        return anomalib_results_path
    
    return None


def organize_source_domain_results(
    sevnet_viz_path: str,
    results_base_dir: str,
    source_domain: str,
    specific_version_path: str = None
) -> bool:
    """Source Domain í‰ê°€ ê²°ê³¼ ì´ë¯¸ì§€ë¥¼ ì •ë¦¬ëœ í´ë”ë¡œ ë³µì‚¬ (ëª¨ë“  ëª¨ë¸ì—ì„œ ê³µí†µ ì‚¬ìš© ê°€ëŠ¥).
    
    ëª©ì : engine.test()ë¡œ ìƒì„±ëœ Source Domain ì‹œê°í™” ê²°ê³¼ë¥¼ source_domain/ í´ë”ë¡œ ì¬ë°°ì¹˜í•˜ì—¬
          ë‚˜ì¤‘ì— ë¶„ì„í•  ë•Œ ìš©ì´í•˜ê²Œ ì ‘ê·¼í•  ìˆ˜ ìˆë„ë¡ í•¨
    
    Source Domainì€ ë³´í†µ fault/, good/ í´ë” êµ¬ì¡°ë¡œ êµ¬ì„±ë¨:
    - fault/: ì‹¤ì œ anomalyê°€ ìˆëŠ” ì´ë¯¸ì§€ë“¤ì˜ ì‹œê°í™” ê²°ê³¼
    - good/: ì •ìƒ ì´ë¯¸ì§€ë“¤ì˜ ì‹œê°í™” ê²°ê³¼
    
    ê° ì´ë¯¸ì§€ëŠ” ë‹¤ìŒ ì •ë³´ë¥¼ í¬í•¨:
    - Original Image: ì›ë³¸ ì´ë¯¸ì§€
    - Reconstructed: ì¬êµ¬ì„±ëœ ì´ë¯¸ì§€ (reconstruction quality í™•ì¸)
    - Anomaly Map: Heat map í˜•íƒœì˜ anomaly ì ìˆ˜ ë¶„í¬
    - Image + Pred Mask: Threshold ê¸°ë°˜ binary mask (ë¹¨ê°„ìƒ‰ ì˜ì—­ë§Œ í‘œì‹œ)
    - Severity Score: SeverityHeadì˜ ì‹¬ê°ë„ ì˜ˆì¸¡ê°’ (0.0~1.0)
      * DRAEM-SevNetì€ mask + severity ê²°í•©ìœ¼ë¡œ ë” ì •êµí•œ anomaly detection ì œê³µ
    
    Args:
        sevnet_viz_path: visualize í´ë” ê²½ë¡œ
        results_base_dir: ê¸°ë³¸ ê²°ê³¼ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        source_domain: ì†ŒìŠ¤ ë„ë©”ì¸ ì´ë¦„
        specific_version_path: íŠ¹ì • ë²„ì „ ê²½ë¡œ (ì„ íƒì )
        
    Returns:
        bool: ì„±ê³µ ì—¬ë¶€
    """
    try:
        # Source í´ë”ì—ì„œ ì´ë¯¸ì§€ íŒŒì¼ ì°¾ê¸°
        if specific_version_path:
            source_path = Path(specific_version_path)
        else:
            source_path = Path(results_base_dir)
        
        # images í´ë” ì°¾ê¸°
        images_folder = None
        for images_path in source_path.rglob("images"):
            if images_path.is_dir():
                images_folder = images_path
                break
        
        if not images_folder or not images_folder.exists():
            print(f"   âš ï¸ Warning: {source_domain} images í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {source_path}")
            return False
        
        # íƒ€ê²Ÿ ê²½ë¡œ (visualize/source_domain/)
        sevnet_viz_path_obj = Path(sevnet_viz_path)
        target_source_path = sevnet_viz_path_obj / "source_domain"
        target_source_path.mkdir(parents=True, exist_ok=True)
        
        print(f"   ğŸ“‚ Source Domain ì´ë¯¸ì§€ ë³µì‚¬: {images_folder} â†’ {target_source_path}")
        
        # fault/, good/ í´ë” ë³µì‚¬
        copied_folders = []
        for subfolder in ["fault", "good"]:
            source_subfolder = images_folder / subfolder
            target_subfolder = target_source_path / subfolder
            
            if source_subfolder.exists():
                if target_subfolder.exists():
                    shutil.rmtree(target_subfolder)
                shutil.copytree(source_subfolder, target_subfolder)
                
                image_count = len(list(target_subfolder.glob("*.png")))
                copied_folders.append(f"{subfolder}({image_count})")
                print(f"     âœ… {subfolder}: {image_count} ì´ë¯¸ì§€ ë³µì‚¬ ì™„ë£Œ")
        
        if copied_folders:
            print(f"   âœ… Source Domain ë³µì‚¬ ì™„ë£Œ: {', '.join(copied_folders)}")
            return True
        else:
            print(f"   âš ï¸ Warning: ë³µì‚¬í•  ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤")
            return False
            
    except Exception as e:
        print(f"   âŒ Error: Source Domain ì´ë¯¸ì§€ ë³µì‚¬ ì‹¤íŒ¨: {e}")
        return False


def copy_target_domain_results(
    domain: str,
    results_base_dir: str = None,
    specific_version_path: str = None,
    visualization_base_path: str = None
) -> bool:
    """Target Domain í‰ê°€ ê²°ê³¼ ì „ì²´ ë³µì‚¬ ë° ë³´ì¡´ (ëª¨ë“  ëª¨ë¸ì—ì„œ ê³µí†µ ì‚¬ìš© ê°€ëŠ¥).
    
    ê° Target Domain í‰ê°€ê°€ ì™„ë£Œë˜ë©´ images/ í´ë”ì˜ ëª¨ë“  ê²°ê³¼ë¥¼ 
    visualize/target_domains/{domain}/ í´ë”ë¡œ ì™„ì „íˆ ë³µì‚¬í•˜ì—¬ ë³´ì¡´í•©ë‹ˆë‹¤.
    
    ëª©ì : engine.test()ë¡œ ìƒì„±ëœ ì‹œê°í™” ê²°ê³¼ë¥¼ ë„ë©”ì¸ë³„ë¡œ ì¬ë°°ì¹˜í•˜ì—¬ 
          ë‚˜ì¤‘ì— ë¶„ì„í•  ë•Œ ìš©ì´í•˜ê²Œ ì ‘ê·¼í•  ìˆ˜ ìˆë„ë¡ í•¨
    
    Args:
        domain: íƒ€ê²Ÿ ë„ë©”ì¸ ì´ë¦„
        results_base_dir: ê¸°ë³¸ ê²°ê³¼ ë””ë ‰í† ë¦¬ ê²½ë¡œ (ì„ íƒì )
        specific_version_path: íŠ¹ì • ë²„ì „ ê²½ë¡œ (ì„ íƒì )
        visualization_base_path: ì‹œê°í™” ì €ì¥ ê¸°ë³¸ ê²½ë¡œ (ì„ íƒì )
        
    Returns:
        bool: ì„±ê³µ ì—¬ë¶€
    """
    try:
        # ê²½ë¡œ ê²°ì • (specific_version_path ìš°ì„ , ê·¸ ë‹¤ìŒ results_base_dir)
        if specific_version_path:
            base_path = Path(specific_version_path)
        elif results_base_dir:
            base_path = Path(results_base_dir)
        else:
            print(f"         âŒ Error: ê²½ë¡œê°€ ì§€ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return False
        
        # ì‹œê°í™” ê²½ë¡œ ê²°ì •
        if visualization_base_path:
            viz_base_path = Path(visualization_base_path)
        else:
            viz_base_path = base_path / "visualize"
        
        # íƒ€ê²Ÿ ê²½ë¡œ (visualize/target_domains/{domain}/)
        target_domain_path = viz_base_path / "target_domains" / domain
        target_domain_path.mkdir(parents=True, exist_ok=True)
        
        # Sourceì—ì„œ images í´ë” ì°¾ê¸°
        all_images_paths = list(base_path.rglob("images"))
        
        # ë§Œì•½ ì°¾ì§€ ëª»í–ˆë‹¤ë©´, ë¶€ëª¨ ê²½ë¡œì—ì„œë„ íƒìƒ‰ (ì‹¤ì œ Anomalib ê²°ê³¼ ê²½ë¡œ í¬í•¨)
        if not all_images_paths and base_path.name == "tensorboard_logs":
            parent_path = base_path.parent
            all_images_paths = list(parent_path.rglob("images"))
        
        images_folder = None
        for images_path in all_images_paths:
            if images_path.is_dir():
                images_folder = images_path
                break
        
        if not images_folder or not images_folder.exists():
            print(f"         âš ï¸ Warning: {domain} images í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return False
        
        # images í´ë” ì „ì²´ ë³µì‚¬
        copied_count = 0
        for item in images_folder.iterdir():
            if item.is_file() and item.suffix.lower() in ['.png', '.jpg', '.jpeg']:
                target_file = target_domain_path / item.name
                shutil.copy2(item, target_file)
                copied_count += 1
            elif item.is_dir():
                target_subfolder = target_domain_path / item.name
                if target_subfolder.exists():
                    shutil.rmtree(target_subfolder)
                shutil.copytree(item, target_subfolder)
                subfolder_count = len(list(target_subfolder.rglob("*.png")))
                copied_count += subfolder_count
        
        if copied_count > 0:
            print(f"         âœ… {domain}: {copied_count} ì´ë¯¸ì§€ ë³µì‚¬ ì™„ë£Œ")
            return True
        else:
            print(f"         âš ï¸ Warning: {domain}ì— ë³µì‚¬í•  ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤")
            return False
            
    except Exception as e:
        print(f"         âŒ Error: {domain} ì´ë¯¸ì§€ ë³µì‚¬ ì‹¤íŒ¨: {e}")
        return False


def evaluate_target_domains(
    model: Any,
    engine: Engine,
    datamodule: Any,
    checkpoint_path: str,
    results_base_dir: str,
    target_domains: List[str] = None,
    datamodule_class = None,
    save_samples: bool = True,
    current_version_path: str = None
) -> Dict[str, Dict[str, Any]]:
    """Target domainsì— ëŒ€í•œ ì„±ëŠ¥ í‰ê°€ ìˆ˜í–‰ (ëª¨ë“  ëª¨ë¸ì—ì„œ ê³µí†µ ì‚¬ìš© ê°€ëŠ¥).
    
    Args:
        model: í›ˆë ¨ëœ ëª¨ë¸
        engine: Anomalib Engine
        datamodule: Multi-domain ë°ì´í„°ëª¨ë“ˆ (sourceìš©)
        checkpoint_path: ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
        results_base_dir: ê²°ê³¼ ì €ì¥ ê¸°ë³¸ ê²½ë¡œ
        target_domains: í‰ê°€í•  target domain ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ datamoduleì—ì„œ ì¶”ì¶œ)
        datamodule_class: DataModule í´ë˜ìŠ¤ (Noneì´ë©´ ìë™ ê°ì§€)
        save_samples: ìƒ˜í”Œ ì´ë¯¸ì§€ ì €ì¥ ì—¬ë¶€
        current_version_path: í˜„ì¬ ë²„ì „ ê²½ë¡œ (ì‹œê°í™” ì €ì¥ìš©)
        
    Returns:
        Dict[str, Dict[str, Any]]: ê° target domainë³„ í‰ê°€ ê²°ê³¼
    """
    # DataModule í´ë˜ìŠ¤ ìë™ ê°ì§€
    if datamodule_class is None:
        datamodule_class = type(datamodule)
    
    # Target domains ìë™ ì¶”ì¶œ
    if target_domains is None:
        if hasattr(datamodule, 'target_domains'):
            target_domains = datamodule.target_domains
        else:
            # ê¸°ë³¸ê°’ìœ¼ë¡œ HDMAP ë„ë©”ì¸ ì‚¬ìš©
            target_domains = ["domain_B", "domain_C", "domain_D"]
            print(f"   âš ï¸ Warning: target_domainsë¥¼ ìë™ ê°ì§€í•  ìˆ˜ ì—†ì–´ ê¸°ë³¸ê°’ ì‚¬ìš©: {target_domains}")
    
    print(f"   ğŸ¯ í‰ê°€í•  Target Domains: {target_domains}")
    
    target_results = {}
    
    for domain in target_domains:
        print(f"      ğŸ¯ Target Domain í‰ê°€: {domain}")
        
        try:
            # ê°œë³„ Target Domainìš© DataModule ìƒì„± (ë™ì  í´ë˜ìŠ¤ ì‚¬ìš©)
            target_datamodule = datamodule_class(
                root=datamodule.root,
                source_domain=getattr(datamodule, 'source_domain', "domain_A"),  # ì›ë˜ source domain ìœ ì§€
                target_domains=[domain],   # í‰ê°€í•  domainì„ targetìœ¼ë¡œ ì„¤ì •
                validation_strategy=getattr(datamodule, 'validation_strategy', "source_test"),
                train_batch_size=getattr(datamodule, 'train_batch_size', 16),
                eval_batch_size=getattr(datamodule, 'eval_batch_size', 16),
                num_workers=getattr(datamodule, 'num_workers', 16)
            )
            
            # Test ë‹¨ê³„ ì„¤ì •
            target_datamodule.setup(stage="test")
            
            # ëª¨ë¸ í‰ê°€
            print(f"         ğŸ“Š {domain} DataModule ì„¤ì • ì™„ë£Œ, test ì‹œì‘...")
            
            # Note: Engineì˜ default_root_dirì€ í›ˆë ¨ í›„ ë³€ê²½ ë¶ˆê°€ëŠ¥í•˜ë¯€ë¡œ ì¬ì„¤ì •í•˜ì§€ ì•ŠìŒ
            # ê²°ê³¼ëŠ” ê° ì‹¤í—˜ì˜ tensorboard_logs í´ë”ì— ì •ìƒì ìœ¼ë¡œ ì €ì¥ë¨
            
            result = engine.test(
                model=model, 
                datamodule=target_datamodule,
                ckpt_path=checkpoint_path
            )
            
            print(f"         ğŸ” {domain} í‰ê°€ ê²°ê³¼ íƒ€ì…: {type(result)}")
            print(f"         ğŸ” {domain} í‰ê°€ ê²°ê³¼ ë‚´ìš©: {result}")
            
            # ê²°ê³¼ ì €ì¥
            if result:
                domain_result = result[0] if isinstance(result, list) else result
                
                # test_image_AUROC -> image_AUROC í‚¤ ë³€í™˜ (í‘œì¤€í™”)
                if 'test_image_AUROC' in domain_result:
                    domain_result['image_AUROC'] = domain_result['test_image_AUROC']
                if 'test_image_F1Score' in domain_result:
                    domain_result['image_F1Score'] = domain_result['test_image_F1Score']
                
                target_results[domain] = domain_result
                print(f"         âœ… {domain} í‰ê°€ ì™„ë£Œ - AUROC: {target_results[domain].get('image_AUROC', 'N/A')}")
                if isinstance(target_results[domain].get('image_AUROC'), (int, float)):
                    print(f"         ğŸ“Š {domain} ìƒì„¸ ì„±ëŠ¥: AUROC={target_results[domain].get('image_AUROC'):.4f}, F1={target_results[domain].get('image_F1Score', 'N/A')}")
            else:
                print(f"         âš ï¸ Warning: {domain} í‰ê°€ ê²°ê³¼ê°€ Noneì…ë‹ˆë‹¤")
                target_results[domain] = {"image_AUROC": 0.0, "image_F1Score": 0.0}
            
            # ì´ë¯¸ì§€ ë³µì‚¬ (ì„ íƒì )
            if save_samples and current_version_path:
                copy_success = copy_target_domain_results(
                    domain=domain,
                    results_base_dir=results_base_dir,
                    specific_version_path=current_version_path
                )
                if not copy_success:
                    print(f"         âš ï¸ Warning: {domain} ì´ë¯¸ì§€ ë³µì‚¬ ì‹¤íŒ¨")
                
        except Exception as e:
            print(f"         âŒ Error: {domain} í‰ê°€ ì‹¤íŒ¨: {e}")
            target_results[domain] = {"image_AUROC": 0.0, "image_F1Score": 0.0, "error": str(e)}
    
    return target_results


def save_experiment_results(
    result: Dict[str, Any], 
    result_filename: str, 
    log_dir: Path, 
    logger: logging.Logger,
    model_type: str = "Model"
) -> Path:
    """ì‹¤í—˜ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥ (ëª¨ë“  ëª¨ë¸ì—ì„œ ê³µí†µ ì‚¬ìš© ê°€ëŠ¥).
    
    Args:
        result: ì‹¤í—˜ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        result_filename: ì €ì¥í•  íŒŒì¼ëª…
        log_dir: ë¡œê·¸ ë””ë ‰í† ë¦¬ (ì‹¤íŒ¨ ì‹œ ì‚¬ìš©)
        logger: ë¡œê±° ê°ì²´
        model_type: ëª¨ë¸ íƒ€ì… (ë¡œê¹…ìš©)
        
    Returns:
        Path: ì‹¤ì œ ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
    """
    # ì‹¤í—˜ë³„ ê²½ë¡œì— ì €ì¥í•˜ê±°ë‚˜, ì‹¤íŒ¨ ì‹œ log_dirì— ì €ì¥
    if result.get("experiment_path") and Path(result["experiment_path"]).exists():
        result_path = Path(result["experiment_path"]) / result_filename
        print(f"   ğŸ“ ê²°ê³¼ íŒŒì¼ì„ ì‹¤í—˜ í´ë”ì— ì €ì¥: {result_path}")
    else:
        result_path = log_dir / result_filename
        print(f"   ğŸ“ ê²°ê³¼ íŒŒì¼ì„ ë¡œê·¸ í´ë”ì— ì €ì¥: {result_path}")
    
    # ê²°ê³¼ JSON ì§ë ¬í™”ë¥¼ ìœ„í•œ ì²˜ë¦¬
    serializable_result = json.loads(json.dumps(result, default=str))
    
    with open(result_path, 'w', encoding='utf-8') as f:
        json.dump(serializable_result, f, indent=2, ensure_ascii=False)
    
    # ê²°ê³¼ ìš”ì•½ ë¡œê¹…
    if result["status"] == "success":
        logger.info("âœ… ì‹¤í—˜ ì„±ê³µ!")
        
        # AUROC ì •ë³´ ë¡œê¹… (multi-domain ë˜ëŠ” all-domainsì— ë”°ë¼ ë‹¤ë¥´ê²Œ ì²˜ë¦¬)
        if 'source_results' in result:
            # Multi-domain ì‹¤í—˜ì˜ ê²½ìš°
            source_auroc = result['source_results'].get('image_AUROC', None)
            if isinstance(source_auroc, (int, float)):
                logger.info(f"   Source Domain AUROC: {source_auroc:.4f}")
            else:
                logger.info(f"   Source Domain AUROC: {source_auroc or 'N/A'}")
            
            # Target Domains Avg AUROC ì•ˆì „í•œ í¬ë§·íŒ…
            avg_target_auroc = result.get('avg_target_auroc', None)
            if isinstance(avg_target_auroc, (int, float)):
                logger.info(f"   Target Domains Avg AUROC: {avg_target_auroc:.4f}")
            else:
                logger.info(f"   Target Domains Avg AUROC: {avg_target_auroc or 'N/A'}")
                
        elif 'all_domains_results' in result:
            # All-domains ì‹¤í—˜ì˜ ê²½ìš°
            all_domains_auroc = result['all_domains_results'].get('test_image_AUROC', None)
            if isinstance(all_domains_auroc, (int, float)):
                logger.info(f"   All Domains AUROC: {all_domains_auroc:.4f}")
            else:
                logger.info(f"   All Domains AUROC: {all_domains_auroc or 'N/A'}")
        else:
            logger.info("   AUROC ì •ë³´ ì—†ìŒ")
        
        logger.info(f"   ì²´í¬í¬ì¸íŠ¸: {result.get('best_checkpoint', 'N/A')}")
        
        # í•™ìŠµ ê³¼ì • ì •ë³´ ë¡œê¹…
        training_info = result.get('training_info', {})
        if training_info:
            logger.info("ğŸ“Š í•™ìŠµ ê³¼ì • ì •ë³´:")
            logger.info(f"   ì„¤ì •ëœ ìµœëŒ€ ì—í¬í¬: {training_info.get('max_epochs_configured', 'N/A')}")
            logger.info(f"   ì‹¤ì œ í•™ìŠµ ì—í¬í¬: {training_info.get('last_trained_epoch', 'N/A')}")
            logger.info(f"   ì´ í•™ìŠµ ìŠ¤í…: {training_info.get('total_steps', 'N/A')}")
            logger.info(f"   Early Stopping ì ìš©: {training_info.get('early_stopped', 'N/A')}")
            if training_info.get('early_stopped'):
                logger.info(f"   Early Stopping ì‚¬ìœ : {training_info.get('early_stop_reason', 'N/A')}")
            # ìµœê³  Validation AUROC ì•ˆì „í•œ í¬ë§·íŒ…
            best_val_auroc = training_info.get('best_val_auroc', None)
            if isinstance(best_val_auroc, (int, float)):
                logger.info(f"   ìµœê³  Validation AUROC: {best_val_auroc:.4f}")
            else:
                logger.info(f"   ìµœê³  Validation AUROC: {best_val_auroc or 'N/A'}")
            logger.info(f"   í•™ìŠµ ì™„ë£Œ ë°©ì‹: {training_info.get('completion_description', 'N/A')}")
        
        # Target Domainë³„ ìƒì„¸ ì„±ëŠ¥ ë¡œê¹…
        target_results = result.get('target_results', {})
        if target_results:
            logger.info("ğŸ¯ Target Domainë³„ ì„±ëŠ¥:")
            for domain, domain_result in target_results.items():
                domain_auroc = domain_result.get('image_AUROC', 'N/A')
                if isinstance(domain_auroc, (int, float)):
                    logger.info(f"   {domain}: {domain_auroc:.4f}")
                else:
                    logger.info(f"   {domain}: {domain_auroc}")
    else:
        logger.info("âŒ ì‹¤í—˜ ì‹¤íŒ¨!")
        logger.info(f"   ì˜¤ë¥˜: {result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
    
    logger.info(f"ğŸ“ ê²°ê³¼ íŒŒì¼: {result_path}")
    
    # ì‹¤í—˜ë³„ ê²½ë¡œì— ì €ì¥ëœ ê²½ìš° ì¶”ê°€ ì •ë³´ ë¡œê¹…
    if result.get("experiment_path"):
        logger.info(f"ğŸ“‚ ì‹¤í—˜ í´ë”: {result['experiment_path']}")
    
    return result_path


def analyze_multi_experiment_results(all_results: list, source_domain: str):
    """ë‹¤ì¤‘ ì‹¤í—˜ ê²°ê³¼ ë¶„ì„ ë° ë¹„êµ (ëª¨ë“  ëª¨ë¸ì—ì„œ ê³µí†µ ì‚¬ìš© ê°€ëŠ¥).
    
    Args:
        all_results: ëª¨ë“  ì‹¤í—˜ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        source_domain: ì†ŒìŠ¤ ë„ë©”ì¸ ì´ë¦„
    """
    print(f"\n{'='*80}")
    print(f"ğŸ“ˆ ë‹¤ì¤‘ ì‹¤í—˜ ê²°ê³¼ ë¶„ì„ ë° ë¹„êµ")
    print(f"Source Domain: {source_domain}")
    print(f"{'='*80}")
    
    successful_results = [r for r in all_results if r["status"] == "success"]
    failed_results = [r for r in all_results if r["status"] == "failed"]
    
    print(f"\nğŸ“Š ì‹¤í—˜ ìš”ì•½:")
    print(f"   ì„±ê³µ: {len(successful_results)}/{len(all_results)} ê°œ")
    print(f"   ì‹¤íŒ¨: {len(failed_results)}/{len(all_results)} ê°œ")
    
    if failed_results:
        print(f"\nâŒ ì‹¤íŒ¨í•œ ì‹¤í—˜ë“¤:")
        for result in failed_results:
            print(f"   - {result['condition']['name']}: {result['error']}")
    
    if successful_results:
        print(f"\nğŸ† ì‹¤í—˜ ê²°ê³¼ ìˆœìœ„ (Target Domain í‰ê·  AUROC ê¸°ì¤€):")
        # Target Domain í‰ê·  AUROC ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        sorted_results = sorted(successful_results, 
                              key=lambda x: x.get("avg_target_auroc", 0), 
                              reverse=True)
        
        print(f"{'ìˆœìœ„':<4} {'ì‹¤í—˜ ì¡°ê±´':<30} {'Source AUROC':<12} {'Target Avg':<12} {'ë„ë©”ì¸ ì „ì´':<10}")
        print("-" * 80)
        
        for idx, result in enumerate(sorted_results, 1):
            condition_name = result["condition"]["name"]
            source_auroc = result["source_results"].get("image_AUROC", 0)
            avg_target_auroc = result.get("avg_target_auroc", 0)
            
            # ë„ë©”ì¸ ì „ì´ íš¨ê³¼ ê³„ì‚°
            if source_auroc > 0:
                transfer_effect = avg_target_auroc / source_auroc
                transfer_desc = "ìš°ìˆ˜" if transfer_effect > 0.9 else "ì–‘í˜¸" if transfer_effect > 0.8 else "ê°œì„ í•„ìš”"
            else:
                transfer_desc = "N/A"
            
            print(f"{idx:<4} {condition_name:<30} {source_auroc:<12.3f} {avg_target_auroc:<12.3f} {transfer_desc:<10}")
        
        # ìµœê³  ì„±ëŠ¥ ì‹¤í—˜ ì„¸ë¶€ ë¶„ì„
        best_result = sorted_results[0]
        print(f"\nğŸ¥‡ ìµœê³  ì„±ëŠ¥ ì‹¤í—˜: {best_result['condition']['name']}")
        print(f"   ğŸ“Š Target Domainë³„ ì„¸ë¶€ ì„±ëŠ¥:")
        
        target_performances = []
        for domain, result in best_result["target_results"].items():
            domain_auroc = result.get("image_AUROC", 0)
            if isinstance(domain_auroc, (int, float)):
                print(f"   {domain:<12} {domain_auroc:<12.3f}")
                target_performances.append((domain, domain_auroc))
        
        # ë„ë©”ì¸ë³„ ì„±ëŠ¥ ë¶„ì„
        if target_performances:
            best_domain = max(target_performances, key=lambda x: x[1])
            worst_domain = min(target_performances, key=lambda x: x[1])
            print(f"\n   ğŸ¯ ìµœê³  ì„±ëŠ¥ ë„ë©”ì¸: {best_domain[0]} (AUROC: {best_domain[1]:.3f})")
            print(f"   âš ï¸  ìµœì € ì„±ëŠ¥ ë„ë©”ì¸: {worst_domain[0]} (AUROC: {worst_domain[1]:.3f})")


def create_single_domain_datamodule(
    domain: str,
    dataset_root: str,
    batch_size: int = 16,
    target_size: tuple[int, int] | None = None,
    resize_method: str = "resize",
    val_split_ratio: float = 0.2,
    val_split_mode: str = "FROM_TEST",
    num_workers: int = 4,
    seed: int = 42,
    verbose: bool = True,
):
    """Single Domainìš© HDMAPDataModule ìƒì„± ë° ì„¤ì •.
    
    Args:
        domain: ë‹¨ì¼ ë„ë©”ì¸ ì´ë¦„ (ì˜ˆ: "domain_A")
        dataset_root: ë°ì´í„°ì…‹ ë£¨íŠ¸ ê²½ë¡œ (í•„ìˆ˜)
        batch_size: ë°°ì¹˜ í¬ê¸°
        target_size: íƒ€ê²Ÿ ì´ë¯¸ì§€ í¬ê¸° (height, width). Noneì´ë©´ ë¦¬ì‚¬ì´ì¦ˆ ì•ˆ í•¨
        resize_method: ë¦¬ì‚¬ì´ì¦ˆ ë°©ë²• ("resize", "black_padding", "noise_padding")
        val_split_ratio: validation ë¶„í•  ë¹„ìœ¨
        val_split_mode: validation ë¶„í•  ëª¨ë“œ ("FROM_TEST", "NONE", "FROM_TRAIN")
        num_workers: ì›Œì»¤ ìˆ˜
        seed: ëœë¤ ì‹œë“œ
        verbose: ìƒì„¸ ë¡œê·¸ ì¶œë ¥ ì—¬ë¶€
        
    Returns:
        ì„¤ì •ëœ HDMAPDataModule
        
    Examples:
        # 256x256 ë¦¬ì‚¬ì´ì¦ˆ
        datamodule = create_single_domain_datamodule(
            domain="domain_A",
            dataset_root="/path/to/dataset",
            target_size=(256, 256),
            batch_size=8
        )
        
        # 224x224 ë¸”ë™ íŒ¨ë”©
        datamodule = create_single_domain_datamodule(
            domain="domain_B",
            dataset_root="/path/to/dataset",
            target_size=(224, 224),
            resize_method="black_padding"
        )
        
        # ì›ë³¸ í¬ê¸° ìœ ì§€
        datamodule = create_single_domain_datamodule(
            domain="domain_C",
            dataset_root="/path/to/dataset",
            target_size=None
        )
    """
    from anomalib.data.datamodules.image.hdmap import HDMAPDataModule
    from anomalib.data.utils import ValSplitMode
    from pathlib import Path
    
    # ValSplitMode ë¬¸ìì—´ì„ enumìœ¼ë¡œ ë³€í™˜
    split_mode_map = {
        "FROM_TEST": ValSplitMode.FROM_TEST,
        "NONE": ValSplitMode.NONE,
        "FROM_TRAIN": ValSplitMode.FROM_TRAIN
    }
    val_split_mode_enum = split_mode_map.get(val_split_mode, ValSplitMode.FROM_TEST)
    
    if verbose:
        print(f"\nğŸ“¦ HDMAPDataModule ìƒì„± ì¤‘...")
        print(f"   ğŸ¯ ë„ë©”ì¸: {domain}")
        if target_size:
            print(f"   ğŸ“ íƒ€ê²Ÿ í¬ê¸°: {target_size[0]}x{target_size[1]}")
            print(f"   ğŸ”§ ë¦¬ì‚¬ì´ì¦ˆ ë°©ë²•: {resize_method}")
        else:
            print(f"   ğŸ“ íƒ€ê²Ÿ í¬ê¸°: ì›ë³¸ í¬ê¸° ìœ ì§€")
        print(f"   ğŸ“Š ë°°ì¹˜ í¬ê¸°: {batch_size}")
        print(f"   ğŸ”„ Val ë¶„í• : {val_split_mode} (ë¹„ìœ¨: {val_split_ratio})")
    
    # Path ê°ì²´ë¡œ ë³€í™˜ ë° ê²€ì¦
    dataset_root = Path(dataset_root).resolve()
    
    if not dataset_root.exists():
        raise FileNotFoundError(f"ë°ì´í„°ì…‹ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {dataset_root}")
    
    if verbose:
        print(f"   ğŸ“ ë°ì´í„°ì…‹ ê²½ë¡œ: {dataset_root}")
        print(f"   ğŸ“ ë„ë©”ì¸ ê²½ë¡œ: {dataset_root / domain}")
    
    # HDMAPDataModule ìƒì„±
    datamodule = HDMAPDataModule(
        root=str(dataset_root),
        domain=domain,
        train_batch_size=batch_size,
        eval_batch_size=batch_size,
        num_workers=num_workers,
        val_split_mode=val_split_mode_enum,
        val_split_ratio=val_split_ratio,
        seed=seed,
        target_size=target_size,
        resize_method=resize_method
    )
    
    # ë°ì´í„° ì¤€ë¹„ ë° ì„¤ì •
    if verbose:
        print(f"   âš™ï¸  DataModule ì„¤ì • ì¤‘...")
    
    try:
        datamodule.prepare_data()
        datamodule.setup()
    except Exception as e:
        print(f"âŒ DataModule ì„¤ì • ì‹¤íŒ¨: {e}")
        raise
    
    # ë°ì´í„° í†µê³„ ì¶œë ¥
    if verbose:
        print(f"âœ… {domain} ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
        print(f"   í›ˆë ¨ ìƒ˜í”Œ: {len(datamodule.train_data):,}ê°œ")
        
        val_count = len(datamodule.val_data) if hasattr(datamodule, 'val_data') and datamodule.val_data else 0
        print(f"   ê²€ì¦ ìƒ˜í”Œ: {val_count:,}ê°œ")
        print(f"   í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ: {len(datamodule.test_data):,}ê°œ")
        
        # ì²« ë²ˆì§¸ ë°°ì¹˜ë¡œ ë°ì´í„° í˜•íƒœ í™•ì¸
        try:
            train_loader = datamodule.train_dataloader()
            sample_batch = next(iter(train_loader))
            print(f"   ğŸ“Š ì´ë¯¸ì§€ í˜•íƒœ: {sample_batch.image.shape}")
            print(f"   ğŸ“Š ë ˆì´ë¸” í˜•íƒœ: {sample_batch.gt_label.shape}")
            print(f"   ğŸ“Š ë°ì´í„° ë²”ìœ„: [{sample_batch.image.min():.3f}, {sample_batch.image.max():.3f}]")
        except Exception as e:
            print(f"   âš ï¸  ë°°ì¹˜ ì •ë³´ í™•ì¸ ì‹¤íŒ¨: {e}")
    
    return datamodule



# =============================================================================
# ìƒì„¸ ë¶„ì„ í•¨ìˆ˜ë“¤
# =============================================================================

def save_detailed_test_results(
    predictions: Dict[str, Any],
    ground_truth: Dict[str, Any], 
    image_paths: List[str],
    result_dir: Path,
    model_type: str = "unknown"
) -> None:
    """
    í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ ì´ë¯¸ì§€ë³„ë¡œ ìƒì„¸í•˜ê²Œ CSV íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        predictions: ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        ground_truth: ì‹¤ì œ ì •ë‹µ ë”•ì…”ë„ˆë¦¬
        image_paths: ì´ë¯¸ì§€ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        result_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        model_type: ëª¨ë¸ íƒ€ì… (draem_sevnet, patchcore ë“±)
    """
    import pandas as pd
    
    # result_dirì„ analysis_dirë¡œ ì§ì ‘ ì‚¬ìš© (ì¤‘ë³µ í´ë” ìƒì„± ë°©ì§€)
    analysis_dir = Path(result_dir)
    
    # ê²°ê³¼ ë°ì´í„° ìˆ˜ì§‘
    results_data = []
    
    for i, img_path in enumerate(image_paths):
        row = {
            "image_path": img_path,
            "ground_truth": ground_truth.get("labels", [0] * len(image_paths))[i] if isinstance(ground_truth.get("labels"), list) else ground_truth.get("label", [0])[i],
            "anomaly_score": predictions.get("pred_scores", [0] * len(image_paths))[i] if isinstance(predictions.get("pred_scores"), list) else 0,
        }
        
        # ëª¨ë¸ë³„ ì¶”ê°€ ì ìˆ˜ë“¤ (ìˆëŠ” ê²½ìš°ì—ë§Œ)
        if predictions.get("mask_scores") and isinstance(predictions.get("mask_scores"), list) and len(predictions.get("mask_scores")) > i:
            row["mask_score"] = predictions["mask_scores"][i]
        else:
            row["mask_score"] = 0.0
            
        # DRAEM-SevNet ëª¨ë¸ì˜ ê²½ìš° raw_severity_scoreì™€ normalized_severity_score êµ¬ë¶„
        if predictions.get("raw_severity_scores") and isinstance(predictions.get("raw_severity_scores"), list) and len(predictions.get("raw_severity_scores")) > i:
            row["raw_severity_score"] = predictions["raw_severity_scores"][i]
        else:
            row["raw_severity_score"] = 0.0
            
        if predictions.get("normalized_severity_scores") and isinstance(predictions.get("normalized_severity_scores"), list) and len(predictions.get("normalized_severity_scores")) > i:
            row["normalized_severity_score"] = predictions["normalized_severity_scores"][i]
        else:
            row["normalized_severity_score"] = 0.0
            
        # ê¸°ì¡´ severity_scoreëŠ” backward compatibilityë¥¼ ìœ„í•´ ìœ ì§€ (normalized_severity_scoreì™€ ë™ì¼)
        if predictions.get("severity_scores") and isinstance(predictions.get("severity_scores"), list) and len(predictions.get("severity_scores")) > i:
            row["severity_score"] = predictions["severity_scores"][i]
        else:
            # normalized_severity_scoreì™€ ë™ì¼í•œ ê°’ ì‚¬ìš©
            row["severity_score"] = row["normalized_severity_score"]
        
        # ì˜ˆì¸¡ ë ˆì´ë¸” ê³„ì‚° (ê¸°ë³¸ threshold 0.5 ì‚¬ìš©)
        row["predicted_label"] = 1 if row["anomaly_score"] > 0.5 else 0
        
        results_data.append(row)
    
    # DataFrame ìƒì„± ë° ì €ì¥
    df = pd.DataFrame(results_data)
    csv_path = analysis_dir / "test_results.csv"
    df.to_csv(csv_path, index=False)
    
    print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ CSV ì €ì¥: {csv_path}")


def plot_roc_curve(
    ground_truth: List[int],
    scores: List[float], 
    result_dir: Path,
    experiment_name: str = "Experiment"
) -> float:
    """
    ROC curveë¥¼ ê·¸ë¦¬ê³  AUROC ê°’ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Args:
        ground_truth: ì‹¤ì œ ì •ë‹µ ë¦¬ìŠ¤íŠ¸ (0 ë˜ëŠ” 1)
        scores: ì˜ˆì¸¡ ì ìˆ˜ ë¦¬ìŠ¤íŠ¸
        result_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        experiment_name: ì‹¤í—˜ ì´ë¦„
        
    Returns:
        AUROC ê°’
    """
    import matplotlib.pyplot as plt
    from sklearn.metrics import roc_curve, auc
    import numpy as np
    
    # result_dirì„ analysis_dirë¡œ ì§ì ‘ ì‚¬ìš© (ì¤‘ë³µ í´ë” ìƒì„± ë°©ì§€)
    analysis_dir = Path(result_dir)
    
    # ROC curve ê³„ì‚°
    fpr, tpr, thresholds = roc_curve(ground_truth, scores)
    auroc = auc(fpr, tpr)
    
    # ìµœì  threshold ê³„ì‚° (Youden's index)
    optimal_idx = np.argmax(tpr - fpr)
    optimal_threshold = thresholds[optimal_idx]
    
    # í”Œë¡¯ ìƒì„±
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUROC = {auroc:.3f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', alpha=0.6)
    plt.scatter(fpr[optimal_idx], tpr[optimal_idx], color='red', s=100, 
                label=f'Optimal threshold = {optimal_threshold:.3f}')
    
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'ROC Curve - {experiment_name}')
    plt.legend(loc="lower right")
    plt.grid(True, alpha=0.3)
    
    # ì €ì¥
    roc_path = analysis_dir / "roc_curve.png"
    plt.savefig(roc_path, dpi=150, bbox_inches='tight')
    plt.close()
    
    print(f"ğŸ“ˆ ROC Curve ì €ì¥: {roc_path}")
    return auroc


def save_metrics_report(
    ground_truth: List[int],
    predictions: List[int],
    scores: List[float],
    result_dir: Path,
    auroc: float,
    optimal_threshold: float = 0.5
) -> None:
    """
    ì„±ëŠ¥ ë©”íŠ¸ë¦­ì„ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        ground_truth: ì‹¤ì œ ì •ë‹µ ë¦¬ìŠ¤íŠ¸
        predictions: ì˜ˆì¸¡ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸  
        scores: ì˜ˆì¸¡ ì ìˆ˜ ë¦¬ìŠ¤íŠ¸
        result_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        auroc: AUROC ê°’
        optimal_threshold: ìµœì  threshold
    """
    from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix
    import json
    
    # result_dirì„ analysis_dirë¡œ ì§ì ‘ ì‚¬ìš© (ì¤‘ë³µ í´ë” ìƒì„± ë°©ì§€)
    analysis_dir = Path(result_dir)
    
    # ë©”íŠ¸ë¦­ ê³„ì‚°
    precision = precision_score(ground_truth, predictions, zero_division=0)
    recall = recall_score(ground_truth, predictions, zero_division=0)
    f1 = f1_score(ground_truth, predictions, zero_division=0)
    cm = confusion_matrix(ground_truth, predictions).tolist()
    
    # ë©”íŠ¸ë¦­ ë³´ê³ ì„œ ìƒì„±
    metrics_report = {
        "auroc": float(auroc),
        "optimal_threshold": float(optimal_threshold),
        "precision": float(precision),
        "recall": float(recall),
        "f1_score": float(f1),
        "confusion_matrix": cm,
        "total_samples": len(ground_truth),
        "positive_samples": sum(ground_truth),
        "negative_samples": len(ground_truth) - sum(ground_truth)
    }
    
    # JSON íŒŒì¼ë¡œ ì €ì¥
    metrics_path = analysis_dir / "metrics_report.json"
    with open(metrics_path, 'w', encoding='utf-8') as f:
        json.dump(metrics_report, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ“‹ ë©”íŠ¸ë¦­ ë³´ê³ ì„œ ì €ì¥: {metrics_path}")


def plot_score_distributions(
    normal_scores: List[float],
    anomaly_scores: List[float], 
    result_dir: Path,
    experiment_name: str = "Experiment"
) -> None:
    """
    ì •ìƒ/ì´ìƒ ìƒ˜í”Œì˜ ì ìˆ˜ ë¶„í¬ë¥¼ íˆìŠ¤í† ê·¸ë¨ìœ¼ë¡œ ì‹œê°í™”í•©ë‹ˆë‹¤.
    
    Args:
        normal_scores: ì •ìƒ ìƒ˜í”Œ ì ìˆ˜ ë¦¬ìŠ¤íŠ¸
        anomaly_scores: ì´ìƒ ìƒ˜í”Œ ì ìˆ˜ ë¦¬ìŠ¤íŠ¸
        result_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        experiment_name: ì‹¤í—˜ ì´ë¦„
    """
    import matplotlib.pyplot as plt
    import numpy as np
    
    # result_dirì„ analysis_dirë¡œ ì§ì ‘ ì‚¬ìš© (ì¤‘ë³µ í´ë” ìƒì„± ë°©ì§€)
    analysis_dir = Path(result_dir)
    
    # íˆìŠ¤í† ê·¸ë¨ ìƒì„±
    plt.figure(figsize=(10, 6))
    
    # ì •ìƒ ìƒ˜í”Œ ë¶„í¬
    plt.hist(normal_scores, bins=50, alpha=0.6, label=f'Normal (n={len(normal_scores)})', 
             color='blue', density=True)
    
    # ì´ìƒ ìƒ˜í”Œ ë¶„í¬  
    plt.hist(anomaly_scores, bins=50, alpha=0.6, label=f'Anomaly (n={len(anomaly_scores)})', 
             color='red', density=True)
    
    plt.xlabel('Anomaly Score')
    plt.ylabel('Density')
    plt.title(f'Score Distributions - {experiment_name}')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # í†µê³„ ì •ë³´ í…ìŠ¤íŠ¸ ì¶”ê°€
    normal_mean, normal_std = np.mean(normal_scores), np.std(normal_scores)
    anomaly_mean, anomaly_std = np.mean(anomaly_scores), np.std(anomaly_scores)
    
    stats_text = f'Normal: Î¼={normal_mean:.3f}, Ïƒ={normal_std:.3f}\\nAnomaly: Î¼={anomaly_mean:.3f}, Ïƒ={anomaly_std:.3f}'
    plt.text(0.02, 0.98, stats_text, transform=plt.gca().transAxes, 
             verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
    
    # ì €ì¥
    dist_path = analysis_dir / "score_distributions.png"
    plt.savefig(dist_path, dpi=150, bbox_inches='tight')
    plt.close()
    
    print(f"ğŸ“Š ì ìˆ˜ ë¶„í¬ ì €ì¥: {dist_path}")


def save_extreme_samples(
    image_paths: List[str],
    ground_truth: List[int],
    scores: List[float],
    predictions: List[int],
    result_dir: Path,
    n_samples: int = 10
) -> None:
    """
    ê·¹ê°’ ìƒ˜í”Œë“¤(ê³ ì‹ ë¢°ë„ ë§ì¶¤/í‹€ë¦¼, ì €ì‹ ë¢°ë„)ì˜ ê²½ë¡œë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        image_paths: ì´ë¯¸ì§€ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        ground_truth: ì‹¤ì œ ì •ë‹µ ë¦¬ìŠ¤íŠ¸
        scores: ì˜ˆì¸¡ ì ìˆ˜ ë¦¬ìŠ¤íŠ¸
        predictions: ì˜ˆì¸¡ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        result_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬  
        n_samples: ê° ì¹´í…Œê³ ë¦¬ë³„ ì €ì¥í•  ìƒ˜í”Œ ìˆ˜
    """
    import numpy as np
    import pandas as pd
    
    # result_dirì„ analysis_dirë¡œ ì§ì ‘ ì‚¬ìš© (ì¤‘ë³µ í´ë” ìƒì„± ë°©ì§€)  
    analysis_dir = Path(result_dir)
    extreme_dir = analysis_dir / "extreme_samples"
    extreme_dir.mkdir(parents=True, exist_ok=True)
    
    # ë°ì´í„° ì •ë¦¬
    data = pd.DataFrame({
        'image_path': image_paths,
        'ground_truth': ground_truth,
        'score': scores,
        'prediction': predictions
    })
    
    # ì •í™•ë„ ê³„ì‚°
    data['correct'] = (data['ground_truth'] == data['prediction'])
    data['confidence'] = np.abs(data['score'] - 0.5)  # 0.5ì—ì„œ ì–¼ë§ˆë‚˜ ë¨¼ì§€ë¡œ ì‹ ë¢°ë„ ì¸¡ì •
    
    # ì¹´í…Œê³ ë¦¬ë³„ ìƒ˜í”Œ ì¶”ì¶œ
    categories = {
        'high_confidence_correct': data[(data['correct'] == True)].nlargest(n_samples, 'confidence'),
        'high_confidence_wrong': data[(data['correct'] == False)].nlargest(n_samples, 'confidence'), 
        'low_confidence_samples': data.nsmallest(n_samples, 'confidence')
    }
    
    # ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ CSV ì €ì¥
    for category, samples in categories.items():
        if len(samples) > 0:
            csv_path = extreme_dir / f"{category}.csv"
            samples.to_csv(csv_path, index=False)
            print(f"ğŸ“¸ {category} ìƒ˜í”Œ ì €ì¥: {csv_path}")


def save_experiment_summary(
    experiment_config: Dict[str, Any],
    results: Dict[str, float],
    result_dir: Path,
    training_time: Optional[str] = None
) -> None:
    """
    ì‹¤í—˜ ì„¤ì •ê³¼ ê²°ê³¼ë¥¼ YAML íŒŒì¼ë¡œ ìš”ì•½ ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        experiment_config: ì‹¤í—˜ ì„¤ì • ë”•ì…”ë„ˆë¦¬
        results: ì‹¤í—˜ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        result_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        training_time: í•™ìŠµ ì‹œê°„ (ì„ íƒì )
    """
    import yaml
    from datetime import datetime
    
    # result_dirì„ analysis_dirë¡œ ì§ì ‘ ì‚¬ìš© (ì¤‘ë³µ í´ë” ìƒì„± ë°©ì§€)
    analysis_dir = Path(result_dir)
    
    # ìš”ì•½ ì •ë³´ ìƒì„±
    summary = {
        'experiment_info': {
            'name': experiment_config.get('name', 'unknown'),
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'training_time': training_time or 'N/A'
        },
        'hyperparameters': experiment_config,
        'results': results,
        'analysis_files': [
            'test_results.csv',
            'roc_curve.png', 
            'metrics_report.json',
            'score_distributions.png',
            'extreme_samples/'
        ]
    }
    
    # YAML íŒŒì¼ë¡œ ì €ì¥
    summary_path = analysis_dir / "experiment_summary.yaml"
    with open(summary_path, 'w', encoding='utf-8') as f:
        yaml.dump(summary, f, default_flow_style=False, allow_unicode=True, indent=2)
    
    print(f"ğŸ“„ ì‹¤í—˜ ìš”ì•½ ì €ì¥: {summary_path}")


def analyze_test_data_distribution(datamodule, test_size: int) -> Tuple[int, int]:
    """í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ë¼ë²¨ ë¶„í¬ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
    
    Args:
        datamodule: í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì œê³µí•˜ëŠ” ë°ì´í„°ëª¨ë“ˆ
        test_size: ì „ì²´ í…ŒìŠ¤íŠ¸ ë°ì´í„° í¬ê¸°
        
    Returns:
        Tuple[int, int]: (fault_count, good_count) 
    """
    import torch
    import numpy as np
    
    print(f"   ğŸ” í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¼ë²¨ ë¶„í¬ ì „ì²´ í™•ì¸ ì¤‘ (ì´ {test_size}ê°œ)...")
    
    test_loader = datamodule.test_dataloader()
    fault_count = 0
    good_count = 0
    total_processed = 0
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(test_loader):
            if hasattr(batch, 'gt_label'):
                labels = batch.gt_label.numpy()
                batch_fault_count = (labels == 1).sum()
                batch_good_count = (labels == 0).sum()
                
                fault_count += batch_fault_count
                good_count += batch_good_count
                total_processed += len(labels)
                
                # ì§„í–‰ë¥  í‘œì‹œ (100 ë°°ì¹˜ë§ˆë‹¤)
                if batch_idx % 100 == 0 and batch_idx > 0:
                    print(f"      ğŸ“Š ì§„í–‰ë¥ : {batch_idx+1} ë°°ì¹˜, {total_processed}ê°œ ì²˜ë¦¬ë¨")
    
    print(f"   âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„í¬ ë¶„ì„ ì™„ë£Œ - ì´ {total_processed}ê°œ ìƒ˜í”Œ")
    print(f"      ğŸš¨ ìµœì¢… ë¶„í¬: Fault={fault_count}, Good={good_count}")
    
    # ë¶„í¬ ë¹„ìœ¨ ê³„ì‚° ë° ê²½ê³ 
    if total_processed > 0:
        fault_ratio = fault_count / total_processed * 100
        good_ratio = good_count / total_processed * 100
        print(f"      ğŸ“ˆ ë¹„ìœ¨: Fault={fault_ratio:.1f}%, Good={good_ratio:.1f}%")
        
        # ë¶ˆê· í˜• ê²½ê³ 
        if fault_count == 0:
            print(f"      âš ï¸  ê²½ê³ : Fault ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤! AUROC ê³„ì‚°ì— ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        elif good_count == 0:
            print(f"      âš ï¸  ê²½ê³ : Good ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤! AUROC ê³„ì‚°ì— ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        elif abs(fault_count - good_count) > total_processed * 0.3:
            print(f"      âš ï¸  ê²½ê³ : ë¼ë²¨ ë¶„í¬ê°€ ë¶ˆê· í˜•í•©ë‹ˆë‹¤ (30% ì´ìƒ ì°¨ì´)")
        else:
            print(f"      âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¼ë²¨ ë¶„í¬ ì •ìƒ")
    
    return fault_count, good_count


def analyze_dataset_statistics(datamodule, train_size: int, test_size: int, val_size: int = None) -> dict:
    """í›ˆë ¨, í…ŒìŠ¤íŠ¸, ê²€ì¦ ë°ì´í„°ì˜ í”½ì…€ê°’ í†µê³„ëŸ‰ì„ ë¶„ì„í•©ë‹ˆë‹¤.
    
    Args:
        datamodule: ë°ì´í„°ë¥¼ ì œê³µí•˜ëŠ” ë°ì´í„°ëª¨ë“ˆ
        train_size: í›ˆë ¨ ë°ì´í„° í¬ê¸°
        test_size: í…ŒìŠ¤íŠ¸ ë°ì´í„° í¬ê¸°
        val_size: ê²€ì¦ ë°ì´í„° í¬ê¸° (ì„ íƒì‚¬í•­)
        
    Returns:
        dict: ê° ë°ì´í„°ì…‹ë³„ í†µê³„ëŸ‰ ì •ë³´
    """
    import torch
    import numpy as np
    from typing import List
    
    print(f"   ğŸ“Š ë°ì´í„°ì…‹ í”½ì…€ê°’ í†µê³„ ë¶„ì„ ì‹œì‘...")
    
    statistics = {
        "train": {"values": [], "labels": [], "count": 0},
        "test": {"values": [], "labels": [], "count": 0}
    }
    
    if val_size is not None and val_size > 0:
        statistics["val"] = {"values": [], "labels": [], "count": 0}
    
    # í›ˆë ¨ ë°ì´í„° ë¶„ì„
    print(f"      ğŸ” í›ˆë ¨ ë°ì´í„° ë¶„ì„ ì¤‘ (ì´ {train_size}ê°œ)...")
    train_loader = datamodule.train_dataloader()
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(train_loader):
            # ì´ë¯¸ì§€ ë°ì´í„° ìˆ˜ì§‘
            images = batch.image.numpy()  # (B, C, H, W)
            labels = batch.gt_label.numpy() if hasattr(batch, 'gt_label') else np.zeros(images.shape[0])
            
            # ê° ì´ë¯¸ì§€ë³„ë¡œ í”½ì…€ê°’ê³¼ ë¼ë²¨ì„ ë§¤í•‘
            for img, label in zip(images, labels):
                img_values = img.flatten()
                statistics["train"]["values"].extend(img_values.tolist())
                # ì´ë¯¸ì§€ì˜ ëª¨ë“  í”½ì…€ì— ëŒ€í•´ ë™ì¼í•œ ë¼ë²¨ ì ìš©
                statistics["train"]["labels"].extend([label] * len(img_values))
            
            statistics["train"]["count"] += len(labels)
            
            # ì§„í–‰ë¥  í‘œì‹œ
            if batch_idx % 50 == 0 and batch_idx > 0:
                print(f"         ğŸ“ˆ í›ˆë ¨ ë°ì´í„°: {batch_idx+1} ë°°ì¹˜, {statistics['train']['count']}ê°œ ì²˜ë¦¬ë¨")
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ì„
    print(f"      ğŸ” í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ì„ ì¤‘ (ì´ {test_size}ê°œ)...")
    test_loader = datamodule.test_dataloader()
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(test_loader):
            # ì´ë¯¸ì§€ ë°ì´í„° ìˆ˜ì§‘
            images = batch.image.numpy()  # (B, C, H, W)
            labels = batch.gt_label.numpy() if hasattr(batch, 'gt_label') else np.zeros(images.shape[0])
            
            # ê° ì´ë¯¸ì§€ë³„ë¡œ í”½ì…€ê°’ê³¼ ë¼ë²¨ì„ ë§¤í•‘
            for img, label in zip(images, labels):
                img_values = img.flatten()
                statistics["test"]["values"].extend(img_values.tolist())
                # ì´ë¯¸ì§€ì˜ ëª¨ë“  í”½ì…€ì— ëŒ€í•´ ë™ì¼í•œ ë¼ë²¨ ì ìš©
                statistics["test"]["labels"].extend([label] * len(img_values))
            
            statistics["test"]["count"] += len(labels)
            
            # ì§„í–‰ë¥  í‘œì‹œ
            if batch_idx % 50 == 0 and batch_idx > 0:
                print(f"         ğŸ“ˆ í…ŒìŠ¤íŠ¸ ë°ì´í„°: {batch_idx+1} ë°°ì¹˜, {statistics['test']['count']}ê°œ ì²˜ë¦¬ë¨")
    
    # ê²€ì¦ ë°ì´í„° ë¶„ì„ (ìˆëŠ” ê²½ìš°)
    if "val" in statistics:
        print(f"      ğŸ” ê²€ì¦ ë°ì´í„° ë¶„ì„ ì¤‘ (ì´ {val_size}ê°œ)...")
        val_loader = datamodule.val_dataloader()
        
        with torch.no_grad():
            for batch_idx, batch in enumerate(val_loader):
                # ì´ë¯¸ì§€ ë°ì´í„° ìˆ˜ì§‘
                images = batch.image.numpy()  # (B, C, H, W)
                labels = batch.gt_label.numpy() if hasattr(batch, 'gt_label') else np.zeros(images.shape[0])
                
                # ê° ì´ë¯¸ì§€ë³„ë¡œ í”½ì…€ê°’ê³¼ ë¼ë²¨ì„ ë§¤í•‘
                for img, label in zip(images, labels):
                    img_values = img.flatten()
                    statistics["val"]["values"].extend(img_values.tolist())
                    # ì´ë¯¸ì§€ì˜ ëª¨ë“  í”½ì…€ì— ëŒ€í•´ ë™ì¼í•œ ë¼ë²¨ ì ìš©
                    statistics["val"]["labels"].extend([label] * len(img_values))
                
                statistics["val"]["count"] += len(labels)
                
                # ì§„í–‰ë¥  í‘œì‹œ
                if batch_idx % 50 == 0 and batch_idx > 0:
                    print(f"         ğŸ“ˆ ê²€ì¦ ë°ì´í„°: {batch_idx+1} ë°°ì¹˜, {statistics['val']['count']}ê°œ ì²˜ë¦¬ë¨")
    
    # í†µê³„ëŸ‰ ê³„ì‚° ë° ì¶œë ¥
    print(f"   ğŸ“Š í”½ì…€ê°’ í†µê³„ëŸ‰ ê³„ì‚° ì¤‘...")
    
    results = {}
    for split_name, data in statistics.items():
        if len(data["values"]) == 0:
            continue
            
        values = np.array(data["values"])
        labels = np.array(data["labels"])
        
        # ì „ì²´ í†µê³„
        overall_stats = {
            "count": len(values),
            "min": float(np.min(values)),
            "max": float(np.max(values)),
            "mean": float(np.mean(values)),
            "std": float(np.std(values)),
            "median": float(np.median(values)),
            "q25": float(np.percentile(values, 25)),
            "q75": float(np.percentile(values, 75))
        }
        
        # ë¼ë²¨ë³„ í†µê³„ (ë¼ë²¨ì´ ìˆëŠ” ê²½ìš°)
        label_stats = {}
        unique_labels = np.unique(labels)
        
        for label in unique_labels:
            mask = labels == label
            label_values = values[mask] if np.any(mask) else np.array([])
            
            if len(label_values) > 0:
                label_name = "normal" if label == 0 else "fault"
                label_stats[label_name] = {
                    "count": len(label_values),
                    "min": float(np.min(label_values)),
                    "max": float(np.max(label_values)),
                    "mean": float(np.mean(label_values)),
                    "std": float(np.std(label_values)),
                    "median": float(np.median(label_values))
                }
        
        results[split_name] = {
            "overall": overall_stats,
            "by_label": label_stats
        }
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"   âœ… ë°ì´í„°ì…‹ í†µê³„ ë¶„ì„ ì™„ë£Œ!")
    print(f"   ğŸ“‹ === í”½ì…€ê°’ í†µê³„ ìš”ì•½ ===")
    
    for split_name, split_stats in results.items():
        split_display = {"train": "í›ˆë ¨", "test": "í…ŒìŠ¤íŠ¸", "val": "ê²€ì¦"}
        print(f"   ğŸ“Š {split_display.get(split_name, split_name).upper()} ë°ì´í„°:")
        
        overall = split_stats["overall"]
        print(f"      ğŸ”¢ ì „ì²´ í”½ì…€: {overall['count']:,}ê°œ")
        print(f"      ğŸ“ ë²”ìœ„: [{overall['min']:.4f}, {overall['max']:.4f}]")
        print(f"      ğŸ“Š í‰ê· Â±í‘œì¤€í¸ì°¨: {overall['mean']:.4f} Â± {overall['std']:.4f}")
        print(f"      ğŸ“ ì¤‘ìœ„ìˆ˜: {overall['median']:.4f}")
        print(f"      ğŸ“ˆ Q1/Q3: {overall['q25']:.4f} / {overall['q75']:.4f}")
        
        # ë¼ë²¨ë³„ í†µê³„
        if split_stats["by_label"]:
            for label_name, label_stat in split_stats["by_label"].items():
                label_emoji = "âœ…" if label_name == "normal" else "ğŸš¨"
                print(f"      {label_emoji} {label_name.upper()} ({label_stat['count']:,}ê°œ ìƒ˜í”Œ):")
                print(f"         ğŸ“ ë²”ìœ„: [{label_stat['min']:.4f}, {label_stat['max']:.4f}]")
                print(f"         ğŸ“Š í‰ê· Â±í‘œì¤€í¸ì°¨: {label_stat['mean']:.4f} Â± {label_stat['std']:.4f}")
        print()
    
    # ë°ì´í„°ì…‹ ê°„ ë¹„êµ
    if len(results) > 1:
        print(f"   ğŸ” === ë°ì´í„°ì…‹ ê°„ ë¹„êµ ===")
        
        # í‰ê· ê°’ ë¹„êµ
        means = {name: stats["overall"]["mean"] for name, stats in results.items()}
        stds = {name: stats["overall"]["std"] for name, stats in results.items()}
        ranges = {name: (stats["overall"]["max"] - stats["overall"]["min"]) 
                 for name, stats in results.items()}
        
        print(f"   ğŸ“Š í‰ê· ê°’ ë¹„êµ:")
        for name, mean_val in means.items():
            split_name = {"train": "í›ˆë ¨", "test": "í…ŒìŠ¤íŠ¸", "val": "ê²€ì¦"}.get(name, name)
            print(f"      {split_name}: {mean_val:.4f}")
        
        print(f"   ğŸ“ í‘œì¤€í¸ì°¨ ë¹„êµ:")
        for name, std_val in stds.items():
            split_name = {"train": "í›ˆë ¨", "test": "í…ŒìŠ¤íŠ¸", "val": "ê²€ì¦"}.get(name, name)
            print(f"      {split_name}: {std_val:.4f}")
        
        print(f"   ğŸ“ˆ ê°’ ë²”ìœ„ ë¹„êµ:")
        for name, range_val in ranges.items():
            split_name = {"train": "í›ˆë ¨", "test": "í…ŒìŠ¤íŠ¸", "val": "ê²€ì¦"}.get(name, name)
            print(f"      {split_name}: {range_val:.4f}")
        
        # ë¶„í¬ ì¼ê´€ì„± í™•ì¸
        train_mean = means.get("train", 0)
        test_mean = means.get("test", 0)
        
        if "train" in means and "test" in means:
            mean_diff = abs(train_mean - test_mean)
            if mean_diff > 0.1:  # ì„ê³„ê°’ ì„¤ì •
                print(f"   âš ï¸  ê²½ê³ : í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë°ì´í„° í‰ê· ê°’ ì°¨ì´ê°€ í½ë‹ˆë‹¤ (ì°¨ì´: {mean_diff:.4f})")
            else:
                print(f"   âœ… í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„í¬ê°€ ì¼ê´€ì„± ìˆìŠµë‹ˆë‹¤ (ì°¨ì´: {mean_diff:.4f})")
    
    return results


def unified_model_evaluation(model, datamodule, experiment_dir, experiment_name, model_type, logger):
    """í†µí•©ëœ ëª¨ë¸ í‰ê°€ í•¨ìˆ˜
    
    Args:
        model: Lightning ëª¨ë¸
        datamodule: ë°ì´í„° ëª¨ë“ˆ
        experiment_dir: ì‹¤í—˜ ë””ë ‰í„°ë¦¬ ê²½ë¡œ
        experiment_name: ì‹¤í—˜ ì´ë¦„
        model_type: ëª¨ë¸ íƒ€ì… (ì†Œë¬¸ì)
        logger: ë¡œê±° ê°ì²´
                
    Returns:
        dict: AUROC, threshold, precision, recall, f1 score, confusion matrix ë“± í‰ê°€ ë©”íŠ¸ë¦­
    """
    import numpy as np
    from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix
    
    print(f"   ğŸš€ í†µí•© ëª¨ë¸ í‰ê°€ ì‹œì‘...")
    
    # ëª¨ë¸ì„ evaluation ëª¨ë“œë¡œ ì„¤ì •
    model.eval()
    
    # PyTorch ëª¨ë¸ì— ì§ì ‘ ì ‘ê·¼
    torch_model = model.model
    torch_model.eval()
    
    # ëª¨ë¸ì„ GPUë¡œ ì´ë™ (CUDA ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    torch_model = torch_model.to(device)
    print(f"   ğŸ–¥ï¸ ëª¨ë¸ì„ {device}ë¡œ ì´ë™ ì™„ë£Œ")
    
    # ë°ì´í„° ìˆ˜ì§‘ì„ ìœ„í•œ ë¦¬ìŠ¤íŠ¸ë“¤
    all_image_paths = []
    all_ground_truth = []
    all_scores = []
    all_mask_scores = []
    all_severity_scores = []
    all_raw_severity_scores = []
    all_normalized_severity_scores = []
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œë” ìƒì„±
    test_dataloader = datamodule.test_dataloader()
    print(f"   âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œë” ìƒì„± ì™„ë£Œ")
    total_batches = len(test_dataloader)
    
    print(f"   ğŸ”„ {total_batches}ê°œ ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘...")
    
    # ë°°ì¹˜ë³„ë¡œ ì˜ˆì¸¡ ìˆ˜í–‰
    with torch.no_grad():
        for batch_idx, batch in enumerate(test_dataloader):
            print(f"   ğŸ“ ì²˜ë¦¬ ì¤‘: {batch_idx+1}/{total_batches} ë°°ì¹˜ (ì§„í–‰ë¥ : {100*(batch_idx+1)/total_batches:.1f}%)")
            
            # ì´ë¯¸ì§€ ê²½ë¡œ ì¶”ì¶œ (í•„ìˆ˜)
            image_paths = batch.image_path
            if not isinstance(image_paths, list):
                image_paths = [image_paths]
            
            # ì´ë¯¸ì§€ í…ì„œ ì¶”ì¶œ
            image_tensor = batch.image
            print(f"      ğŸ–¼ï¸  ì´ë¯¸ì§€ í…ì„œ í¬ê¸°: {image_tensor.shape}, ê²½ë¡œ ìˆ˜: {len(image_paths)}")
            
            # ì´ë¯¸ì§€ í…ì„œë¥¼ ëª¨ë¸ê³¼ ê°™ì€ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            image_tensor = image_tensor.to(device)
            
            # ëª¨ë¸ë¡œ ì§ì ‘ ì˜ˆì¸¡ ìˆ˜í–‰
            model_output = torch_model(image_tensor)
            print(f"      âœ… ëª¨ë¸ ì¶œë ¥ ì™„ë£Œ: {type(model_output)}")
            
            # DRAEM ëª¨ë¸ì˜ ê²½ìš° NaN ë””ë²„ê¹…
            if model_type.lower() == "draem" and hasattr(model_output, 'pred_score'):
                pred_score_tensor = model_output.pred_score
                print(f"      ğŸ” DRAEM ë””ë²„ê¹…:")
                print(f"         pred_score shape: {pred_score_tensor.shape}")
                
                # pred_scoreê°€ ìœ íš¨í•œì§€ í™•ì¸
                if torch.isnan(pred_score_tensor).any():
                    print(f"         âŒ pred_scoreì— NaN ë°œê²¬! ê°œìˆ˜: {torch.isnan(pred_score_tensor).sum().item()}")
                else:
                    print(f"         âœ… pred_score ì •ìƒ, ë²”ìœ„: [{pred_score_tensor.min():.6f}, {pred_score_tensor.max():.6f}]")
                
                if hasattr(model_output, 'anomaly_map'):
                    anomaly_map_tensor = model_output.anomaly_map
                    print(f"         anomaly_map shape: {anomaly_map_tensor.shape}")
                    
                    if torch.isnan(anomaly_map_tensor).any():
                        print(f"         âŒ anomaly_mapì— NaN ë°œê²¬! ê°œìˆ˜: {torch.isnan(anomaly_map_tensor).sum().item()}")
                    else:
                        print(f"         âœ… anomaly_map ì •ìƒ, ë²”ìœ„: [{anomaly_map_tensor.min():.6f}, {anomaly_map_tensor.max():.6f}]")
            
            # ëª¨ë¸ë³„ ì¶œë ¥ì—ì„œ ì ìˆ˜ë“¤ ì¶”ì¶œ
            final_scores, mask_scores, severity_scores, raw_severity_scores, normalized_severity_scores = extract_scores_from_model_output(
                model_output, image_tensor.shape[0], batch_idx, model_type
            )
            
            # Ground truth ì¶”ì¶œ (ì´ë¯¸ì§€ ê²½ë¡œì—ì„œ)
            gt_labels = []
            for path in image_paths:
                if '/fault/' in path:
                    gt_labels.append(1)  # anomaly
                elif '/good/' in path:
                    gt_labels.append(0)  # normal
                else:
                    gt_labels.append(0)  # ê¸°ë³¸ê°’
            
            # ê²°ê³¼ ìˆ˜ì§‘
            all_image_paths.extend(image_paths)
            all_ground_truth.extend(gt_labels)
            all_scores.extend(final_scores.flatten() if hasattr(final_scores, 'flatten') else final_scores)
            all_mask_scores.extend(mask_scores.flatten() if hasattr(mask_scores, 'flatten') else mask_scores)
            all_severity_scores.extend(severity_scores.flatten() if hasattr(severity_scores, 'flatten') else severity_scores)
            all_raw_severity_scores.extend(raw_severity_scores.flatten() if hasattr(raw_severity_scores, 'flatten') else raw_severity_scores)
            all_normalized_severity_scores.extend(normalized_severity_scores.flatten() if hasattr(normalized_severity_scores, 'flatten') else normalized_severity_scores)
            
            print(f"      âœ… ë°°ì¹˜ {batch_idx+1} ì™„ë£Œ: {len(gt_labels)}ê°œ ìƒ˜í”Œ ì¶”ê°€")
    
    print(f"   âœ… ì´ {len(all_image_paths)}ê°œ ìƒ˜í”Œ ì²˜ë¦¬ ì™„ë£Œ")
    
    # ê¸¸ì´ ë§ì¶”ê¸°
    min_len = min(len(all_ground_truth), len(all_scores))
    all_ground_truth = all_ground_truth[:min_len]
    all_scores = all_scores[:min_len]
    
    print(f"   âœ… í†µí•© í‰ê°€: {len(all_ground_truth)}ê°œ ìƒ˜í”Œë¡œ ë©”íŠ¸ë¦­ ê³„ì‚°")
    
    # NaN ê°’ í™•ì¸ ë° í•„í„°ë§
    import numpy as np
    scores_array = np.array(all_scores)
    gt_array = np.array(all_ground_truth)
    
    nan_mask = np.isnan(scores_array)
    if nan_mask.any():
        nan_count = nan_mask.sum()
        print(f"   âš ï¸  NaN ì ìˆ˜ {nan_count}ê°œ ë°œê²¬, ì œê±° í›„ ê³„ì†")
        
        # NaNì´ ì•„ë‹Œ ê°’ë“¤ë§Œ í•„í„°ë§
        valid_mask = ~nan_mask
        scores_array = scores_array[valid_mask]
        gt_array = gt_array[valid_mask]
        all_scores = scores_array.tolist()
        all_ground_truth = gt_array.tolist()
        
        print(f"   âœ… ìœ íš¨í•œ ìƒ˜í”Œ: {len(all_scores)}ê°œ")
    
    # AUROC ê³„ì‚° ë° ROC curve ìƒì„±
    try:
        auroc = roc_auc_score(all_ground_truth, all_scores)
    except ValueError as e:
        print(f"   âŒ í‰ê°€ ì‹¤íŒ¨: {e}")
        return None
    
    # ì„ê³„ê°’ ê³„ì‚° (Youden's J statistic)
    fpr, tpr, thresholds = roc_curve(all_ground_truth, all_scores)
    optimal_idx = np.argmax(tpr - fpr)
    optimal_threshold = thresholds[optimal_idx]
    
    print(f"   ğŸ“ˆ AUROC: {auroc:.4f}, ìµœì  ì„ê³„ê°’: {optimal_threshold:.4f}")
    
    # ì˜ˆì¸¡ ë¼ë²¨ ìƒì„±
    predictions = (np.array(all_scores) > optimal_threshold).astype(int)
    
    # Confusion Matrix ê³„ì‚°
    cm = confusion_matrix(all_ground_truth, predictions)
    
    # ë©”íŠ¸ë¦­ ê³„ì‚°
    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, len(all_ground_truth), 0)
    
    accuracy = (tp + tn) / len(all_ground_truth) if len(all_ground_truth) > 0 else 0
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"   ğŸ§® í†µí•© Confusion Matrix:")
    print(f"       ì‹¤ì œ\\ì˜ˆì¸¡    Normal  Anomaly")
    print(f"       Normal     {cm[0,0]:6d}  {cm[0,1]:6d}")
    print(f"       Anomaly    {cm[1,0]:6d}  {cm[1,1]:6d}")
    
    print(f"   ğŸ“ˆ í†µí•© ë©”íŠ¸ë¦­:")
    print(f"      AUROC: {auroc:.4f}")
    print(f"      Accuracy: {accuracy:.4f}")
    print(f"      Precision: {precision:.4f}")
    print(f"      Recall: {recall:.4f}")
    print(f"      F1-Score: {f1:.4f}")
    print(f"      Threshold: {optimal_threshold:.4f}")
    
    # ê¸°ë³¸ ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬
    unified_metrics = {
        "auroc": float(auroc),
        "accuracy": float(accuracy), 
        "precision": float(precision),
        "recall": float(recall),
        "f1_score": float(f1),
        "confusion_matrix": cm.tolist(),
        "optimal_threshold": float(optimal_threshold),
        "total_samples": len(all_ground_truth),
        "positive_samples": int(np.sum(all_ground_truth)),
        "negative_samples": int(len(all_ground_truth) - np.sum(all_ground_truth))
    }
    
    # analysis í´ë” ìƒì„± ë° ê²°ê³¼ ì €ì¥
    analysis_dir = Path(experiment_dir) / "analysis"
    analysis_dir.mkdir(exist_ok=True)
    print(f"   ğŸ’¾ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì¤‘: {analysis_dir}")
    
    # ìƒì„¸ í…ŒìŠ¤íŠ¸ ê²°ê³¼ CSV ì €ì¥
    predictions_dict = {
        "pred_scores": all_scores,
        "mask_scores": all_mask_scores,
        "severity_scores": all_severity_scores,
        "raw_severity_scores": all_raw_severity_scores,
        "normalized_severity_scores": all_normalized_severity_scores
    }
    ground_truth_dict = {
        "labels": all_ground_truth
    }
    save_detailed_test_results(
        predictions_dict, ground_truth_dict, all_image_paths, 
        analysis_dir, model_type
    )
    
    # ROC curve ìƒì„±
    plot_roc_curve(all_ground_truth, all_scores, analysis_dir, experiment_name)
    
    # ë©”íŠ¸ë¦­ ë³´ê³ ì„œ ì €ì¥
    save_metrics_report(all_ground_truth, predictions, all_scores, analysis_dir, auroc, optimal_threshold)
    
    # ì ìˆ˜ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨ ìƒì„±
    normal_scores = [score for gt, score in zip(all_ground_truth, all_scores) if gt == 0]
    anomaly_scores = [score for gt, score in zip(all_ground_truth, all_scores) if gt == 1]
    plot_score_distributions(normal_scores, anomaly_scores, analysis_dir, experiment_name)
    
    # ê·¹ë‹¨ì  ì‹ ë¢°ë„ ìƒ˜í”Œ ì €ì¥
    save_extreme_samples(all_image_paths, all_ground_truth, all_scores, predictions, analysis_dir)
    
    # ì‹¤í—˜ ìš”ì•½ ì €ì¥
    save_experiment_summary({}, {"auroc": auroc}, analysis_dir)
    
    logger.info(f"í†µí•© í‰ê°€ ì™„ë£Œ: AUROC={auroc:.4f}, F1={f1:.4f}, ìƒ˜í”Œìˆ˜={len(all_image_paths)}")
    
    return unified_metrics


def extract_scores_from_model_output(model_output, batch_size, batch_idx, model_type):
    """
    ëª¨ë¸ë³„ ì¶œë ¥ì—ì„œ ì ìˆ˜ë“¤ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    Args:
        model_output: ëª¨ë¸ ì¶œë ¥ ê°ì²´
        batch_size: ë°°ì¹˜ í¬ê¸°
        batch_idx: ë°°ì¹˜ ì¸ë±ìŠ¤
        model_type: ëª¨ë¸ íƒ€ì… (ì†Œë¬¸ì)
        
    Returns:
        tuple: (anomaly_scores, mask_scores, severity_scores, raw_severity_scores, normalized_severity_scores)
    """
    import numpy as np
    
    model_type = model_type.lower()
    
    if model_type == "draem":
        # DRAEM: pred_scoreë§Œ ìˆìŒ
        if hasattr(model_output, 'pred_score'):
            final_scores = model_output.pred_score.cpu().numpy()
            
            # NaN ê°’ í™•ì¸ ë° ì²˜ë¦¬
            if np.isnan(final_scores).any():
                print(f"      âš ï¸  DRAEM pred_scoreì— NaN ë°œê²¬, 0.0ìœ¼ë¡œ ëŒ€ì²´")
                final_scores = np.nan_to_num(final_scores, nan=0.0)
            
            mask_scores = [0.0] * batch_size  # DRAEMì—ëŠ” mask_score ì—†ìŒ
            severity_scores = [0.0] * batch_size  # DRAEMì—ëŠ” severity_score ì—†ìŒ
            raw_severity_scores = [0.0] * batch_size  # DRAEMì—ëŠ” raw_severity_score ì—†ìŒ
            normalized_severity_scores = [0.0] * batch_size  # DRAEMì—ëŠ” normalized_severity_score ì—†ìŒ
            print(f"      ğŸ“Š DRAEM ì ìˆ˜ ì¶”ì¶œ: pred_score={final_scores[0]:.4f}")
        elif hasattr(model_output, 'anomaly_map'):
            # anomaly_mapì—ì„œ ì ìˆ˜ ê³„ì‚°
            anomaly_map = model_output.anomaly_map.cpu().numpy()
            final_scores = [float(np.max(am)) if am.size > 0 else 0.0 for am in anomaly_map]
            mask_scores = [0.0] * batch_size
            severity_scores = [0.0] * batch_size
            raw_severity_scores = [0.0] * batch_size
            normalized_severity_scores = [0.0] * batch_size
            print(f"      ğŸ“Š DRAEM ì ìˆ˜ ì¶”ì¶œ (anomaly_map): max={final_scores[0]:.4f}")
        else:
            raise AttributeError("DRAEM ì¶œë ¥ ì†ì„± ì—†ìŒ")
            
    elif model_type == "patchcore":
        # PatchCore: pred_scoreë§Œ ìˆìŒ
        if hasattr(model_output, 'pred_score'):
            final_scores = model_output.pred_score.cpu().numpy()
            mask_scores = [0.0] * batch_size
            severity_scores = [0.0] * batch_size
            raw_severity_scores = [0.0] * batch_size
            normalized_severity_scores = [0.0] * batch_size
            print(f"      ğŸ“Š PatchCore ì ìˆ˜ ì¶”ì¶œ: pred_score={final_scores[0]:.4f}")
        elif hasattr(model_output, 'anomaly_map'):
            # anomaly_mapì—ì„œ ì ìˆ˜ ê³„ì‚°
            anomaly_map = model_output.anomaly_map.cpu().numpy()
            final_scores = [float(np.max(am)) if am.size > 0 else 0.0 for am in anomaly_map]
            mask_scores = [0.0] * batch_size
            severity_scores = [0.0] * batch_size
            raw_severity_scores = [0.0] * batch_size
            normalized_severity_scores = [0.0] * batch_size
            print(f"      ğŸ“Š PatchCore ì ìˆ˜ ì¶”ì¶œ (anomaly_map): max={final_scores[0]:.4f}")
        else:
            raise AttributeError("PatchCore ì¶œë ¥ ì†ì„± ì—†ìŒ")
            
    elif model_type == "dinomaly":
        # Dinomaly: pred_score ë˜ëŠ” anomaly_map
        if hasattr(model_output, 'pred_score'):
            final_scores = model_output.pred_score.cpu().numpy()
            mask_scores = [0.0] * batch_size
            severity_scores = [0.0] * batch_size
            raw_severity_scores = [0.0] * batch_size
            normalized_severity_scores = [0.0] * batch_size
            print(f"      ğŸ“Š Dinomaly ì ìˆ˜ ì¶”ì¶œ: pred_score={final_scores[0]:.4f}")
        elif hasattr(model_output, 'anomaly_map'):
            # anomaly_mapì—ì„œ ì ìˆ˜ ê³„ì‚°
            anomaly_map = model_output.anomaly_map.cpu().numpy()
            final_scores = [float(np.max(am)) if am.size > 0 else 0.0 for am in anomaly_map]
            mask_scores = [0.0] * batch_size
            severity_scores = [0.0] * batch_size
            raw_severity_scores = [0.0] * batch_size
            normalized_severity_scores = [0.0] * batch_size
            print(f"      ğŸ“Š Dinomaly ì ìˆ˜ ì¶”ì¶œ (anomaly_map): max={final_scores[0]:.4f}")
        else:
            raise AttributeError("Dinomaly ì¶œë ¥ ì†ì„± ì—†ìŒ")
            
    else:
        # ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸ íƒ€ì…: ì¼ë°˜ì ì¸ ì†ì„±ìœ¼ë¡œ ì‹œë„
        print(f"   âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸ íƒ€ì…: {model_type}, ì¼ë°˜ì ì¸ ì†ì„±ìœ¼ë¡œ ì‹œë„")
        if hasattr(model_output, 'pred_score'):
            final_scores = model_output.pred_score.cpu().numpy()
        elif hasattr(model_output, 'final_score'):
            final_scores = model_output.final_score.cpu().numpy()
        elif hasattr(model_output, 'anomaly_map'):
            anomaly_map = model_output.anomaly_map.cpu().numpy()
            final_scores = [float(np.max(am)) for am in anomaly_map]
        else:
            raise AttributeError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ëª¨ë¸ ì¶œë ¥ í˜•ì‹: {type(model_output)}")
            
        mask_scores = [0.0] * batch_size
        severity_scores = [0.0] * batch_size
        raw_severity_scores = [0.0] * batch_size
        normalized_severity_scores = [0.0] * batch_size
        print(f"      ğŸ“Š ì¼ë°˜ ëª¨ë¸ ì ìˆ˜ ì¶”ì¶œ: anomaly_score={final_scores[0]:.4f}")
        
    return final_scores, mask_scores, severity_scores, raw_severity_scores, normalized_severity_scores
