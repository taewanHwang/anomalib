#!/usr/bin/env python3
"""
ì‹¤ì œ í•™ìŠµì— ì‚¬ìš©ë˜ëŠ” CutPaste Augmentation ì‹œê°í™” ìŠ¤í¬ë¦½íŠ¸

exp_51-54_draem_cp.jsonì˜ exp-52 ì¡°ê±´ìœ¼ë¡œ ì‹¤ì œ í•™ìŠµ ë°ì´í„°ë¥¼ ìƒì„±í•˜ê³  ì €ì¥í•©ë‹ˆë‹¤.

ì‹¤í–‰ ë°©ë²•:
cd /mnt/ex-disk/taewan.hwang/study/anomalib
python examples/hdmap/visualize_training_augmentation.py
"""

import sys
import random
import numpy as np
import matplotlib.pyplot as plt
import torch
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parents[2]
sys.path.insert(0, str(project_root))

# Anomalib ëª¨ë“ˆ import
from anomalib.models.image.draem_cutpaste_clf.synthetic_generator_v2 import CutPasteSyntheticGenerator

# =============================================================================
# ğŸ”§ ì‹¤í—˜ ì„¤ì • (exp-52.C ì¡°ê±´)
# =============================================================================
DOMAIN = 'C'  # A, B, C, D ì¤‘ ì„ íƒ ê°€ëŠ¥
DATASET_ROOT = "datasets/HDMAP/100000_tiff_minmax"
TARGET_SIZE = [128, 128]
CUT_W_RANGE = [2, 127]
CUT_H_RANGE = [4, 8]
A_FAULT_START = 0.0
A_FAULT_RANGE_END = 0.3
AUGMENT_PROBABILITY = 0.5
SEED = 52
NUM_SAMPLES = 20  # ìƒì„±í•  ìƒ˜í”Œ ìˆ˜

# ì¶œë ¥ ë””ë ‰í† ë¦¬
OUTPUT_DIR = project_root / "examples" / "hdmap" / "single_domain" / "visualizations" / "training_augmentation"
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

def set_seed(seed):
    """ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ì„¤ì •"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

def load_sample_images(domain, num_samples=20):
    """ë°ì´í„°ì…‹ì—ì„œ ìƒ˜í”Œ ì´ë¯¸ì§€ ë¡œë“œ"""
    from PIL import Image

    data_path = project_root / DATASET_ROOT / f"domain_{domain}" / "train" / "good"

    # TIFF íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    tiff_files = list(data_path.glob("*.tiff"))
    if not tiff_files:
        tiff_files = list(data_path.glob("*.tif"))

    if not tiff_files:
        raise FileNotFoundError(f"No TIFF files found in {data_path}")

    # ëœë¤ ìƒ˜í”Œë§
    random.shuffle(tiff_files)
    selected_files = tiff_files[:num_samples]

    images = []
    filenames = []
    for file_path in selected_files:
        img = Image.open(file_path)
        img_array = np.array(img).astype(np.float32)

        # Resize to target size
        img_pil = Image.fromarray(img_array)
        img_resized = img_pil.resize((TARGET_SIZE[1], TARGET_SIZE[0]), Image.BILINEAR)
        img_array_resized = np.array(img_resized).astype(np.float32)

        # (H, W) -> (1, H, W) ë³€í™˜
        img_tensor = torch.from_numpy(img_array_resized).unsqueeze(0)

        # (1, H, W) -> (3, H, W) - RGB í˜•íƒœë¡œ ë³€í™˜ (CutPaste generatorê°€ 3ì±„ë„ ì…ë ¥ ê¸°ëŒ€)
        img_tensor_rgb = img_tensor.repeat(3, 1, 1)

        images.append(img_tensor_rgb)
        filenames.append(file_path.stem)  # í™•ì¥ì ì œì™¸í•œ íŒŒì¼ëª…

    print(f"âœ… ë¡œë“œëœ ì´ë¯¸ì§€: {len(images)}ê°œ")
    print(f"   - ì²« ë²ˆì§¸ ì´ë¯¸ì§€ shape: {images[0].shape}")
    print(f"   - ê°’ ë²”ìœ„: [{images[0].min():.4f}, {images[0].max():.4f}]")

    return images, filenames

def generate_augmented_samples(images, filenames):
    """CutPaste Augmentation ì ìš©"""

    # CutPasteSyntheticGenerator ì´ˆê¸°í™”
    generator = CutPasteSyntheticGenerator(
        cut_w_range=tuple(CUT_W_RANGE),
        cut_h_range=tuple(CUT_H_RANGE),
        a_fault_start=A_FAULT_START,
        a_fault_range_end=A_FAULT_RANGE_END,
        probability=AUGMENT_PROBABILITY,
        validation_enabled=False  # Training mode
    )

    print(f"ğŸ”§ CutPaste Generator ì„¤ì •:")
    print(f"   - cut_w_range: {CUT_W_RANGE}")
    print(f"   - cut_h_range: {CUT_H_RANGE}")
    print(f"   - a_fault_range: [{A_FAULT_START}, {A_FAULT_RANGE_END}]")
    print(f"   - probability: {AUGMENT_PROBABILITY}")

    samples = []
    for i, (img_tensor, filename) in enumerate(zip(images, filenames)):
        # ë°°ì¹˜ í˜•íƒœë¡œ ë³€í™˜: (3, H, W) -> (1, 3, H, W)
        img_batch = img_tensor.unsqueeze(0)

        # CutPaste augmentation ì ìš©
        augmented_batch, anomaly_mask, anomaly_label = generator(img_batch)

        # ê²°ê³¼ ì €ì¥
        samples.append({
            'image': img_tensor.cpu().numpy(),  # (3, H, W)
            'augmented': augmented_batch[0].cpu().numpy(),  # (3, H, W)
            'mask': anomaly_mask[0].cpu().numpy(),  # (1, H, W)
            'has_anomaly': anomaly_label[0].item(),
            'filename': filename  # ì›ë³¸ íŒŒì¼ëª… ì¶”ê°€
        })

    print(f"âœ… Augmentation ì™„ë£Œ: {len(samples)}ê°œ")

    # í†µê³„ ì¶œë ¥
    anomaly_count = sum(1 for s in samples if s['has_anomaly'])
    normal_count = len(samples) - anomaly_count
    print(f"   - Anomaly ìƒ˜í”Œ: {anomaly_count}ê°œ")
    print(f"   - Normal ìƒ˜í”Œ: {normal_count}ê°œ")

    return samples

def save_individual_images(samples, output_dir):
    """ê° ìƒ˜í”Œì„ ê°œë³„ ì´ë¯¸ì§€ íŒŒì¼ë¡œ ì €ì¥"""
    from PIL import Image

    # Anomalyì™€ Normal ë¶„ë¦¬
    anomaly_samples = [s for s in samples if s['has_anomaly']]
    normal_samples = [s for s in samples if not s['has_anomaly']]

    # Anomaly ì´ë¯¸ì§€ ì €ì¥
    anomaly_dir = output_dir / "anomaly_samples"
    anomaly_dir.mkdir(parents=True, exist_ok=True)

    for i, sample in enumerate(anomaly_samples):
        filename = sample['filename']

        # Original
        orig_img = sample['image'][0]  # ì²« ë²ˆì§¸ ì±„ë„
        orig_img_normalized = ((orig_img - orig_img.min()) / (orig_img.max() - orig_img.min()) * 255).astype(np.uint8)
        Image.fromarray(orig_img_normalized).save(anomaly_dir / f"anomaly_{i+1:02d}_{filename}_original.png")

        # Augmented
        aug_img = sample['augmented'][0]  # ì²« ë²ˆì§¸ ì±„ë„
        aug_img_normalized = ((aug_img - aug_img.min()) / (aug_img.max() - aug_img.min()) * 255).astype(np.uint8)
        Image.fromarray(aug_img_normalized).save(anomaly_dir / f"anomaly_{i+1:02d}_{filename}_augmented.png")

        # Mask
        mask_img = sample['mask'][0]
        mask_img_normalized = (mask_img * 255).astype(np.uint8)
        Image.fromarray(mask_img_normalized).save(anomaly_dir / f"anomaly_{i+1:02d}_{filename}_mask.png")

    print(f"âœ… Anomaly ìƒ˜í”Œ ì €ì¥: {len(anomaly_samples)}ê°œ â†’ {anomaly_dir}")

    # Normal ì´ë¯¸ì§€ ì €ì¥
    normal_dir = output_dir / "normal_samples"
    normal_dir.mkdir(parents=True, exist_ok=True)

    for i, sample in enumerate(normal_samples):
        filename = sample['filename']

        # Original
        orig_img = sample['image'][0]
        orig_img_normalized = ((orig_img - orig_img.min()) / (orig_img.max() - orig_img.min()) * 255).astype(np.uint8)
        Image.fromarray(orig_img_normalized).save(normal_dir / f"normal_{i+1:02d}_{filename}_original.png")

        # Augmented (should be same as original)
        aug_img = sample['augmented'][0]
        aug_img_normalized = ((aug_img - aug_img.min()) / (aug_img.max() - aug_img.min()) * 255).astype(np.uint8)
        Image.fromarray(aug_img_normalized).save(normal_dir / f"normal_{i+1:02d}_{filename}_augmented.png")

    print(f"âœ… Normal ìƒ˜í”Œ ì €ì¥: {len(normal_samples)}ê°œ â†’ {normal_dir}")

def plot_statistics(samples, save_path):
    """í”½ì…€ ê°’ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨"""
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle(f'Training Augmentation Statistics - Domain {DOMAIN}', fontsize=16, fontweight='bold')

    # ë°ì´í„° ìˆ˜ì§‘ (ì²« ë²ˆì§¸ ì±„ë„ë§Œ ì‚¬ìš©)
    all_original = []
    all_augmented = []
    anomaly_augmented = []
    normal_augmented = []

    for sample in samples:
        all_original.extend(sample['image'][0].flatten())
        all_augmented.extend(sample['augmented'][0].flatten())

        if sample['has_anomaly']:
            anomaly_augmented.extend(sample['augmented'][0].flatten())
        else:
            normal_augmented.extend(sample['augmented'][0].flatten())

    # Original íˆìŠ¤í† ê·¸ë¨
    axes[0, 0].hist(all_original, bins=50, alpha=0.7, color='blue', density=True)
    axes[0, 0].set_title('Original Images')
    axes[0, 0].set_xlabel('Pixel Value')
    axes[0, 0].set_ylabel('Density')
    axes[0, 0].grid(True, alpha=0.3)
    stats_text = f'Mean: {np.mean(all_original):.4f}\nStd: {np.std(all_original):.4f}'
    axes[0, 0].text(0.02, 0.98, stats_text, transform=axes[0, 0].transAxes,
                    verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))

    # Augmented ì „ì²´ íˆìŠ¤í† ê·¸ë¨
    axes[0, 1].hist(all_augmented, bins=50, alpha=0.7, color='red', density=True)
    axes[0, 1].set_title('Augmented Images (All)')
    axes[0, 1].set_xlabel('Pixel Value')
    axes[0, 1].set_ylabel('Density')
    axes[0, 1].grid(True, alpha=0.3)
    stats_text = f'Mean: {np.mean(all_augmented):.4f}\nStd: {np.std(all_augmented):.4f}'
    axes[0, 1].text(0.02, 0.98, stats_text, transform=axes[0, 1].transAxes,
                    verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.8))

    # Anomaly augmented íˆìŠ¤í† ê·¸ë¨
    if anomaly_augmented:
        axes[1, 0].hist(anomaly_augmented, bins=50, alpha=0.7, color='orange', density=True)
        axes[1, 0].set_title('Augmented Images (Anomaly Only)')
        axes[1, 0].set_xlabel('Pixel Value')
        axes[1, 0].set_ylabel('Density')
        axes[1, 0].grid(True, alpha=0.3)
        stats_text = f'Mean: {np.mean(anomaly_augmented):.4f}\nStd: {np.std(anomaly_augmented):.4f}\nCount: {len([s for s in samples if s["has_anomaly"]])}'
        axes[1, 0].text(0.02, 0.98, stats_text, transform=axes[1, 0].transAxes,
                        verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))

    # Normal augmented íˆìŠ¤í† ê·¸ë¨
    if normal_augmented:
        axes[1, 1].hist(normal_augmented, bins=50, alpha=0.7, color='green', density=True)
        axes[1, 1].set_title('Augmented Images (Normal Only)')
        axes[1, 1].set_xlabel('Pixel Value')
        axes[1, 1].set_ylabel('Density')
        axes[1, 1].grid(True, alpha=0.3)
        stats_text = f'Mean: {np.mean(normal_augmented):.4f}\nStd: {np.std(normal_augmented):.4f}\nCount: {len([s for s in samples if not s["has_anomaly"]])}'
        axes[1, 1].text(0.02, 0.98, stats_text, transform=axes[1, 1].transAxes,
                        verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))

    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"ğŸ“ˆ í†µê³„ ê·¸ë˜í”„ ì €ì¥: {save_path}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("=" * 80)
    print("ğŸ§ª ì‹¤ì œ í•™ìŠµ ë°ì´í„° Augmentation ì‹œê°í™” (exp-52.C ì¡°ê±´)")
    print("=" * 80)
    print(f"ğŸ”§ ì„¤ì •:")
    print(f"   - DOMAIN: {DOMAIN}")
    print(f"   - DATASET_ROOT: {DATASET_ROOT}")
    print(f"   - TARGET_SIZE: {TARGET_SIZE}")
    print(f"   - CUT_W_RANGE: {CUT_W_RANGE}")
    print(f"   - CUT_H_RANGE: {CUT_H_RANGE}")
    print(f"   - A_FAULT_START: {A_FAULT_START}")
    print(f"   - A_FAULT_RANGE_END: {A_FAULT_RANGE_END}")
    print(f"   - AUGMENT_PROBABILITY: {AUGMENT_PROBABILITY}")
    print(f"   - SEED: {SEED}")
    print()

    # ì‹œë“œ ì„¤ì •
    set_seed(SEED)

    # ìƒ˜í”Œ ì´ë¯¸ì§€ ë¡œë“œ
    print("ğŸ“ ìƒ˜í”Œ ì´ë¯¸ì§€ ë¡œë“œ ì¤‘...")
    images, filenames = load_sample_images(DOMAIN, NUM_SAMPLES)

    # Augmentation ì ìš©
    print("\nğŸ¨ CutPaste Augmentation ì ìš© ì¤‘...")
    samples = generate_augmented_samples(images, filenames)

    # ê°œë³„ ì´ë¯¸ì§€ ì €ì¥
    print("\nğŸ’¾ ê°œë³„ ì´ë¯¸ì§€ ì €ì¥ ì¤‘...")
    save_individual_images(samples, OUTPUT_DIR / f"domain_{DOMAIN}")

    # í†µê³„ ì‹œê°í™”
    print("\nğŸ“Š í†µê³„ ì‹œê°í™” ìƒì„± ì¤‘...")
    plot_statistics(samples, OUTPUT_DIR / f"domain_{DOMAIN}" / f"augmentation_statistics.png")

    # ìš”ì•½ ë¦¬í¬íŠ¸ ì €ì¥
    report_path = OUTPUT_DIR / f"domain_{DOMAIN}" / "augmentation_report.txt"
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("ì‹¤ì œ í•™ìŠµ ë°ì´í„° Augmentation ë¦¬í¬íŠ¸\n")
        f.write("=" * 60 + "\n\n")
        f.write(f"ì‹¤í—˜ ì¡°ê±´: exp-52.{DOMAIN}\n")
        f.write(f"ë„ë©”ì¸: {DOMAIN}\n")
        f.write(f"ë°ì´í„°ì…‹: {DATASET_ROOT}\n")
        f.write(f"ì´ë¯¸ì§€ í¬ê¸°: {TARGET_SIZE}\n")
        f.write(f"íŒ¨ì¹˜ ë„ˆë¹„ ë²”ìœ„: {CUT_W_RANGE}\n")
        f.write(f"íŒ¨ì¹˜ ë†’ì´ ë²”ìœ„: {CUT_H_RANGE}\n")
        f.write(f"Fault ê°•ë„ ë²”ìœ„: [{A_FAULT_START}, {A_FAULT_RANGE_END}]\n")
        f.write(f"Augmentation í™•ë¥ : {AUGMENT_PROBABILITY}\n")
        f.write(f"ì‹œë“œ: {SEED}\n\n")

        f.write(f"ìƒì„±ëœ ìƒ˜í”Œ í†µê³„:\n")
        f.write(f"- ì „ì²´ ìƒ˜í”Œ: {len(samples)}ê°œ\n")
        anomaly_count = sum(1 for s in samples if s['has_anomaly'])
        f.write(f"- Anomaly ìƒ˜í”Œ: {anomaly_count}ê°œ\n")
        f.write(f"- Normal ìƒ˜í”Œ: {len(samples) - anomaly_count}ê°œ\n\n")

        # í”½ì…€ ê°’ í†µê³„ (ì²« ë²ˆì§¸ ì±„ë„ë§Œ)
        all_original = np.concatenate([s['image'][0].flatten() for s in samples])
        all_augmented = np.concatenate([s['augmented'][0].flatten() for s in samples])

        f.write(f"í”½ì…€ ê°’ í†µê³„:\n")
        f.write(f"ì›ë³¸ ì´ë¯¸ì§€:\n")
        f.write(f"  - í‰ê· : {np.mean(all_original):.6f}\n")
        f.write(f"  - í‘œì¤€í¸ì°¨: {np.std(all_original):.6f}\n")
        f.write(f"  - ë²”ìœ„: [{np.min(all_original):.6f}, {np.max(all_original):.6f}]\n\n")

        f.write(f"Augmented ì´ë¯¸ì§€:\n")
        f.write(f"  - í‰ê· : {np.mean(all_augmented):.6f}\n")
        f.write(f"  - í‘œì¤€í¸ì°¨: {np.std(all_augmented):.6f}\n")
        f.write(f"  - ë²”ìœ„: [{np.min(all_augmented):.6f}, {np.max(all_augmented):.6f}]\n")

    print(f"ğŸ“„ ë¦¬í¬íŠ¸ ì €ì¥: {report_path}")
    print()
    print("âœ… ì™„ë£Œ!")
    print(f"ğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {OUTPUT_DIR / f'domain_{DOMAIN}'}")
    print(f"   - anomaly_samples/: Anomaly ì´ë¯¸ì§€ (original, augmented, mask)")
    print(f"   - normal_samples/: Normal ì´ë¯¸ì§€ (original, augmented)")
    print(f"   - augmentation_statistics.png: í†µê³„ ê·¸ë˜í”„")
    print(f"   - augmentation_report.txt: ìƒì„¸ ë¦¬í¬íŠ¸")
    print("=" * 80)

if __name__ == "__main__":
    main()
